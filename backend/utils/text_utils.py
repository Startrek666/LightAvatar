"""
æ–‡æœ¬å¤„ç†å·¥å…·
"""
import re


def clean_markdown_for_tts(text: str) -> str:
    """
    æ¸…ç†Markdownæ ¼å¼ç¬¦å·ï¼Œå‡†å¤‡ç”¨äºTTS
    
    ä¿ç•™æ–‡å­—å†…å®¹ï¼Œå»é™¤æ ¼å¼ç¬¦å·ï¼š
    - åŠ ç²—ï¼š**text** -> text
    - æ–œä½“ï¼š*text* -> text
    - ä»£ç å—ï¼š```code``` -> code
    - è¡Œå†…ä»£ç ï¼š`code` -> code
    - æ ‡é¢˜ï¼š# Title -> Title
    - åˆ—è¡¨ï¼š- item -> item
    - Markdowné“¾æ¥ï¼š[text](url) -> text
    - çº¯URLé“¾æ¥ï¼šhttps://example.com -> ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    - Emojiè¡¨æƒ…ç¬¦å·
    - å¼•ç”¨æ ‡è®°ï¼š[citation:X] -> ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    - å‚è€ƒæ¥æºéƒ¨åˆ†ï¼šå®Œæ•´ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    
    Args:
        text: åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«Markdownï¼‰
        
    Returns:
        æ¸…ç†åçš„çº¯æ–‡æœ¬
    """
    if not text:
        return text
    
    original_text = text  # ä¿å­˜åŸæ–‡ç”¨äºè°ƒè¯•
    
    # 0. ç§»é™¤"å‚è€ƒæ¥æº"éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ ‡é¢˜å’Œæ‰€æœ‰å¼•ç”¨åˆ—è¡¨ï¼‰
    # åŒ¹é…æ ¼å¼ï¼š**ğŸ“š å‚è€ƒæ¥æºï¼š**\n1. [æ ‡é¢˜](URL)\n2. [æ ‡é¢˜](URL)...
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…æ•´ä¸ªå‚è€ƒæ¥æºå—
    text = re.sub(
        r'\*\*ğŸ“š\s*å‚è€ƒæ¥æº[ï¼š:]\*\*\s*\n(?:\d+\.\s*\[.*?\]\(.*?\)\s*)+',
        '',
        text,
        flags=re.DOTALL
    )
    
    # å¦‚æœæ–‡æœ¬è¢«ä¿®æ”¹ï¼Œè¯´æ˜æˆåŠŸç§»é™¤äº†å‚è€ƒæ¥æºéƒ¨åˆ†
    if len(text) != len(original_text):
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤å‚è€ƒæ¥æºéƒ¨åˆ† (ä» {len(original_text)} å­—ç¬¦å‡å°‘åˆ° {len(text)} å­—ç¬¦)")
    
    # ç§»é™¤å¼•ç”¨æ ‡è®° [citation:X]ï¼ˆä¸è¯»å‡ºï¼‰
    before_citation_remove = text
    text = re.sub(r'\[citation:\d+\]', '', text)
    
    if len(text) != len(before_citation_remove):
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤å¼•ç”¨æ ‡è®° (ä» {len(before_citation_remove)} å­—ç¬¦å‡å°‘åˆ° {len(text)} å­—ç¬¦)")
    
    # 0. å»é™¤emojiè¡¨æƒ…ç¬¦å·ï¼ˆåœ¨å…¶ä»–å¤„ç†ä¹‹å‰ï¼‰
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„emoji UnicodeèŒƒå›´ï¼ˆé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦ï¼‰
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # æ——å¸œ
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•A
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•B
        "\u2600-\u27BF"          # æ‚é¡¹ç¬¦å·ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
        "\u2702-\u27B0"          # è£…é¥°ç¬¦å·ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)
    
    # 1. ä»£ç å—ï¼ˆå¤šè¡Œï¼‰
    text = re.sub(r'```[\s\S]*?```', '', text)
    
    # 2. è¡Œå†…ä»£ç 
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # 3. åŠ ç²—å’Œæ–œä½“ï¼ˆ**text** æˆ– __text__ï¼‰
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    
    # 4. æ–œä½“ï¼ˆ*text* æˆ– _text_ï¼‰
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    
    # 5. åˆ é™¤çº¿ï¼ˆ~~text~~ï¼‰
    text = re.sub(r'~~([^~]+)~~', r'\1', text)
    
    # 6. æ ‡é¢˜ï¼ˆ# ã€## ç­‰ï¼‰
    text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
    
    # 7. åˆ—è¡¨æ ‡è®°ï¼ˆ- ã€* ã€+ ã€1. ç­‰ï¼‰
    text = re.sub(r'^[\s]*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^[\s]*\d+\.\s+', '', text, flags=re.MULTILINE)
    
    # 8. é“¾æ¥ï¼ˆ[text](url)ï¼‰
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    
    # 8.5. çº¯URLé“¾æ¥ï¼ˆhttp://ã€https://ã€www. å¼€å¤´çš„é“¾æ¥ï¼‰
    # åŒ¹é…å„ç§URLæ ¼å¼ï¼šhttp://ã€https://ã€www.ã€ftp:// ç­‰
    # åŒ¹é…åˆ°å¥æœ«æˆ–ç©ºæ ¼ï¼Œæˆ–è€…è¢«æ‹¬å·åŒ…å›´çš„URL
    url_pattern = r'(?:https?://|www\.|ftp://)[^\s\)\]\}\"\'<>]+'
    removed_urls = re.findall(url_pattern, text)
    text = re.sub(url_pattern, '', text)
    
    # è®°å½•ç§»é™¤çš„URLæ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if removed_urls:
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤ {len(removed_urls)} ä¸ªçº¯URLé“¾æ¥: {removed_urls[:3]}...")
    
    # 9. å›¾ç‰‡ï¼ˆ![alt](url)ï¼‰
    text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', r'\1', text)
    
    # 10. å¼•ç”¨ï¼ˆ> textï¼‰
    text = re.sub(r'^>\s+', '', text, flags=re.MULTILINE)
    
    # 11. æ°´å¹³çº¿ï¼ˆ--- ã€*** ç­‰ï¼‰
    text = re.sub(r'^[\s]*[-*_]{3,}[\s]*$', '', text, flags=re.MULTILINE)
    
    # 12. HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # 13. æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()


def has_speakable_content(text: str) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å¯å‘éŸ³çš„å†…å®¹
    
    è¿‡æ»¤æ‰åªåŒ…å«æ ‡ç‚¹ç¬¦å·ã€ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬ï¼Œè¿™äº›æ— æ³•è¢«TTSå¤„ç†ã€‚
    
    Args:
        text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
        
    Returns:
        True: åŒ…å«å¯å‘éŸ³å†…å®¹
        False: åªåŒ…å«æ ‡ç‚¹ç¬¦å·/ç©ºç™½å­—ç¬¦
    """
    if not text:
        return False
    
    # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œå¸¸è§æ ‡ç‚¹ç¬¦å·
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ç­‰å¯å‘éŸ³å­—ç¬¦
    speakable = re.sub(r'[\s\.,;:!?ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿâ€¦\.]+', '', text)
    
    # å¦‚æœæ¸…ç†åè¿˜æœ‰å†…å®¹ï¼Œè¯´æ˜åŒ…å«å¯å‘éŸ³å­—ç¬¦
    return len(speakable) > 0


def split_long_text(text: str, max_length: int = 200) -> list:
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œä¾¿äºTTSå¤„ç†
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§é•¿åº¦
        
    Returns:
        æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    current_chunk = ""
    
    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?\n])', text)
    
    for i in range(0, len(sentences), 2):
        sentence = sentences[i]
        delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
        full_sentence = sentence + delimiter
        
        if len(current_chunk) + len(full_sentence) <= max_length:
            current_chunk += full_sentence
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = full_sentence
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
