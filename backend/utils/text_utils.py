"""
æ–‡æœ¬å¤„ç†å·¥å…·
"""
import re


def clean_markdown_for_tts(text: str) -> str:
    """
    æ¸…ç†Markdownæ ¼å¼ç¬¦å·ï¼Œå‡†å¤‡ç”¨äºTTS
    
    ä¿ç•™æ–‡å­—å†…å®¹ï¼Œå»é™¤æ ¼å¼ç¬¦å·ï¼š
    - åŠ ç²—ï¼š**text** -> text
    - æ–œä½“ï¼š*text* -> text
    - ä»£ç å—ï¼š```code``` -> code
    - è¡Œå†…ä»£ç ï¼š`code` -> code
    - æ ‡é¢˜ï¼š# Title -> Title
    - åˆ—è¡¨ï¼š- item -> item
    - Markdowné“¾æ¥ï¼š[text](url) -> text
    - çº¯URLé“¾æ¥ï¼šhttps://example.com -> ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    - Emojiè¡¨æƒ…ç¬¦å·
    - å¼•ç”¨æ ‡è®°ï¼š[citation:X] -> ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    - å‚è€ƒæ¥æºéƒ¨åˆ†ï¼šå®Œæ•´ç§»é™¤ï¼ˆä¸è¯»å‡ºï¼‰
    
    Args:
        text: åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«Markdownï¼‰
        
    Returns:
        æ¸…ç†åçš„çº¯æ–‡æœ¬
    """
    if not text:
        return text
    
    original_text = text  # ä¿å­˜åŸæ–‡ç”¨äºè°ƒè¯•
    
    # 0. ç§»é™¤"å‚è€ƒæ¥æº"éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ ‡é¢˜å’Œæ‰€æœ‰å¼•ç”¨åˆ—è¡¨ï¼‰
    # åŒ¹é…æ ¼å¼ï¼š
    # - **ğŸ“š å‚è€ƒæ¥æºï¼š**\n1. [æ ‡é¢˜](URL)\n2. [æ ‡é¢˜](URL)...
    # - ğŸ“š å‚è€ƒæ¥æºï¼š\n1. [æ ‡é¢˜](URL)\n2. [æ ‡é¢˜](URL)...
    # - å‚è€ƒæ¥æºï¼š\n1. [æ ‡é¢˜](URL)\n2. [æ ‡é¢˜](URL)...
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…æ•´ä¸ªå‚è€ƒæ¥æºå—
    text = re.sub(
        r'(?:\*\*)?(?:ğŸ“š\s*)?å‚è€ƒæ¥æº[ï¼š:]\s*(?:\*\*)?\s*\n(?:\d+\.\s*\[.*?\]\(.*?\)\s*)+',
        '',
        text,
        flags=re.DOTALL
    )
    
    # 0.1. ç§»é™¤å•ç‹¬çš„å‚è€ƒæ¥æºåˆ—è¡¨é¡¹ï¼ˆæ•°å­—å¼€å¤´çš„é“¾æ¥æ ¼å¼ï¼Œå¯èƒ½æ˜¯å‚è€ƒæ¥æºçš„ä¸€éƒ¨åˆ†ï¼‰
    # åŒ¹é…æ ¼å¼ï¼š17. [æ ‡é¢˜](URL) æˆ– 17. [æ ‡é¢˜](URL)\n
    # è¿™äº›å¯èƒ½æ˜¯å‚è€ƒæ¥æºè¢«åˆ†å‰²åçš„ç‰‡æ®µ
    text = re.sub(
        r'^\d+\.\s*\[.*?\]\(.*?\)\s*$',
        '',
        text,
        flags=re.MULTILINE
    )
    
    # 0.2. ç§»é™¤åªåŒ…å«é“¾æ¥æ ¼å¼çš„æ–‡æœ¬è¡Œï¼ˆå¯èƒ½æ˜¯å‚è€ƒæ¥æºé“¾æ¥è¢«åˆ†å‰²åçš„ç‰‡æ®µï¼‰
    # åŒ¹é…æ ¼å¼ï¼š
    # - )\nMeta: [æ ‡é¢˜](URL) æˆ–ç±»ä¼¼çš„æ ¼å¼
    # - åªåŒ…å«é“¾æ¥æ–‡æœ¬ï¼Œæ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰çš„å†…å®¹
    # æ£€æŸ¥æ¯ä¸€è¡Œï¼Œå¦‚æœåªåŒ…å«é“¾æ¥æ ¼å¼æˆ–é“¾æ¥æ–‡æœ¬ï¼Œåˆ™ç§»é™¤
    lines = text.split('\n')
    filtered_lines = []
    for line in lines:
        line_stripped = line.strip()
        # è·³è¿‡ç©ºè¡Œ
        if not line_stripped:
            filtered_lines.append(line)
            continue
        
        # è·³è¿‡åªåŒ…å«æ ‡ç‚¹ç¬¦å·çš„è¡Œï¼ˆå¦‚å•ç‹¬çš„å³æ‹¬å·ï¼‰
        if re.match(r'^[\)\]\}\.\s]+$', line_stripped):
            continue
        
        # è·³è¿‡åªåŒ…å«Markdowné“¾æ¥æ ¼å¼çš„è¡Œï¼ˆå¦‚ [æ ‡é¢˜](URL)ï¼‰
        if re.match(r'^\[.*?\]\(.*?\)\s*$', line_stripped):
            continue
        
        # è·³è¿‡åªåŒ…å«URLçš„è¡Œ
        if re.match(r'^(?:https?://|www\.|ftp://)[^\s]+$', line_stripped):
            continue
        
        # è·³è¿‡çœ‹èµ·æ¥åƒæ˜¯å‚è€ƒæ¥æºé“¾æ¥æ ‡é¢˜çš„è¡Œï¼ˆå¦‚ "Meta: Llama 4 Maverick Free Chat Online - skywork ai"ï¼‰
        # è¿™äº›é€šå¸¸æ˜¯é“¾æ¥çš„æ ‡é¢˜éƒ¨åˆ†ï¼Œåœ¨æµå¼è¾“å‡ºæ—¶å¯èƒ½è¢«å•ç‹¬å‘é€
        # ç‰¹å¾ï¼šä»¥ç½‘ç«™/å…¬å¸åå¼€å¤´ï¼Œåé¢è·Ÿç€å†’å·å’Œæ ‡é¢˜ï¼Œå¯èƒ½ä»¥ " - " åˆ†éš”ç½‘ç«™å
        # ä¾‹å¦‚ï¼š"Meta: Llama 4 Maverick Free Chat Online - skywork ai"
        # æˆ–è€…ï¼š")\nMeta: Llama 4 Maverick Free Chat Online - skywork ai"ï¼ˆåœ¨åŒä¸€è¡Œï¼‰
        
        # å…ˆç§»é™¤å¼€å¤´çš„æ ‡ç‚¹ç¬¦å·ï¼Œå†æ£€æŸ¥
        line_cleaned = re.sub(r'^[\)\]\}\.\s]+', '', line_stripped)
        if line_cleaned and re.match(r'^[A-Z][a-zA-Z0-9\s]+:\s+[^-]+(?:\s+-\s+[a-zA-Z0-9\s]+)?$', line_cleaned):
            # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå¦‚æœè¿™è¡ŒåªåŒ…å«æ ‡é¢˜æ ¼å¼ï¼Œæ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œåˆ™è·³è¿‡
            # é¿å…è¯¯åˆ æ­£å¸¸å†…å®¹ï¼ˆå¦‚ "Meta: è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µ"ï¼‰
            # å¦‚æœåŒ…å«å¸¸è§çš„ç½‘ç«™åç§°æ¨¡å¼ï¼ˆå¦‚ "ai"ã€"blog"ã€"online" ç­‰ï¼‰ï¼Œæ›´å¯èƒ½æ˜¯é“¾æ¥æ ‡é¢˜
            if re.search(r'\b(?:ai|blog|online|website|site|com|org|net)\b', line_cleaned, re.IGNORECASE):
                continue
        
        # ä¿ç•™å…¶ä»–è¡Œ
        filtered_lines.append(line)
    
    text = '\n'.join(filtered_lines)
    
    # å¦‚æœæ–‡æœ¬è¢«ä¿®æ”¹ï¼Œè¯´æ˜æˆåŠŸç§»é™¤äº†å‚è€ƒæ¥æºéƒ¨åˆ†
    if len(text) != len(original_text):
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤å‚è€ƒæ¥æºéƒ¨åˆ† (ä» {len(original_text)} å­—ç¬¦å‡å°‘åˆ° {len(text)} å­—ç¬¦)")
    
    # ç§»é™¤å¼•ç”¨æ ‡è®° [citation:X] æˆ– [citation:X, Y] æˆ– [citation: X]ï¼ˆä¸è¯»å‡ºï¼‰
    # åŒ¹é…æ ¼å¼ï¼š[citation:1], [citation:1, 9], [citation: 12], [citation:1, 12, 40] ç­‰
    before_citation_remove = text
    # æ›´å…¨é¢çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…æ‰€æœ‰å¯èƒ½çš„citationæ ¼å¼ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€å¤šä¸ªæ•°å­—ç­‰ï¼‰
    text = re.sub(r'\[citation\s*:\s*[\d\s,]+\]', '', text, flags=re.IGNORECASE)
    
    if len(text) != len(before_citation_remove):
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤å¼•ç”¨æ ‡è®° (ä» {len(before_citation_remove)} å­—ç¬¦å‡å°‘åˆ° {len(text)} å­—ç¬¦)")
    
    # 0. å»é™¤emojiè¡¨æƒ…ç¬¦å·ï¼ˆåœ¨å…¶ä»–å¤„ç†ä¹‹å‰ï¼‰
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„emoji UnicodeèŒƒå›´ï¼ˆé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦ï¼‰
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # æ——å¸œ
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•A
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•B
        "\u2600-\u27BF"          # æ‚é¡¹ç¬¦å·ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
        "\u2702-\u27B0"          # è£…é¥°ç¬¦å·ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)
    
    # 1. ä»£ç å—ï¼ˆå¤šè¡Œï¼‰
    text = re.sub(r'```[\s\S]*?```', '', text)
    
    # 2. è¡Œå†…ä»£ç 
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # 3. åŠ ç²—å’Œæ–œä½“ï¼ˆ**text** æˆ– __text__ï¼‰
    # å…ˆç§»é™¤æˆå¯¹çš„åŠ ç²—æ ‡è®°ï¼ˆä¿ç•™æ–‡æœ¬å†…å®¹ï¼‰
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    
    # ç§»é™¤æ‰€æœ‰å‰©ä½™çš„ ** å’Œ __ï¼ˆé˜²æ­¢æœ‰æœªåŒ¹é…æˆ–å•ç‹¬çš„æ ‡è®°ï¼‰
    text = re.sub(r'\*\*+', '', text)  # ç§»é™¤è¿ç»­çš„æ˜Ÿå·ï¼ˆ**ã€***ç­‰ï¼‰
    text = re.sub(r'__+', '', text)    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿ï¼ˆ__ã€___ç­‰ï¼‰
    
    # 4. æ–œä½“ï¼ˆ*text* æˆ– _text_ï¼‰
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    
    # ç§»é™¤æ‰€æœ‰å‰©ä½™çš„å•ç‹¬çš„ * å’Œ _ï¼ˆé˜²æ­¢æœ‰æœªåŒ¹é…çš„æ ‡è®°ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œè¦å°å¿ƒï¼Œä¸è¦ç§»é™¤ä¸­æ–‡ä¹¦åå·ä¸­çš„ä¸‹åˆ’çº¿
    text = re.sub(r'(?<!\*)\*(?!\*)', '', text)  # ç§»é™¤å•ç‹¬çš„æ˜Ÿå·
    text = re.sub(r'(?<!_)_(?!_)', '', text)    # ç§»é™¤å•ç‹¬çš„ä¸‹åˆ’çº¿
    
    # 5. åˆ é™¤çº¿ï¼ˆ~~text~~ï¼‰
    text = re.sub(r'~~([^~]+)~~', r'\1', text)
    
    # 6. æ ‡é¢˜ï¼ˆ# ã€## ç­‰ï¼‰
    text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
    
    # 7. åˆ—è¡¨æ ‡è®°ï¼ˆ- ã€* ã€+ ã€1. ç­‰ï¼‰
    text = re.sub(r'^[\s]*[-*+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^[\s]*\d+\.\s+', '', text, flags=re.MULTILINE)
    
    # 8. é“¾æ¥ï¼ˆ[text](url)ï¼‰
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    
    # 8.5. çº¯URLé“¾æ¥ï¼ˆhttp://ã€https://ã€www. å¼€å¤´çš„é“¾æ¥ï¼‰
    # åŒ¹é…å„ç§URLæ ¼å¼ï¼šhttp://ã€https://ã€www.ã€ftp:// ç­‰
    # åŒ¹é…åˆ°å¥æœ«æˆ–ç©ºæ ¼ï¼Œæˆ–è€…è¢«æ‹¬å·åŒ…å›´çš„URL
    url_pattern = r'(?:https?://|www\.|ftp://)[^\s\)\]\}\"\'<>]+'
    removed_urls = re.findall(url_pattern, text)
    text = re.sub(url_pattern, '', text)
    
    # è®°å½•ç§»é™¤çš„URLæ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if removed_urls:
        from loguru import logger
        logger.debug(f"âœ‚ï¸ å·²ç§»é™¤ {len(removed_urls)} ä¸ªçº¯URLé“¾æ¥: {removed_urls[:3]}...")
    
    # 9. å›¾ç‰‡ï¼ˆ![alt](url)ï¼‰
    text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', r'\1', text)
    
    # 10. å¼•ç”¨ï¼ˆ> textï¼‰
    text = re.sub(r'^>\s+', '', text, flags=re.MULTILINE)
    
    # 11. æ°´å¹³çº¿ï¼ˆ--- ã€*** ç­‰ï¼‰
    text = re.sub(r'^[\s]*[-*_]{3,}[\s]*$', '', text, flags=re.MULTILINE)
    
    # 12. HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # 13. æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()


def has_speakable_content(text: str) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å¯å‘éŸ³çš„å†…å®¹
    
    è¿‡æ»¤æ‰åªåŒ…å«æ ‡ç‚¹ç¬¦å·ã€ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬ï¼Œè¿™äº›æ— æ³•è¢«TTSå¤„ç†ã€‚
    
    Args:
        text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
        
    Returns:
        True: åŒ…å«å¯å‘éŸ³å†…å®¹
        False: åªåŒ…å«æ ‡ç‚¹ç¬¦å·/ç©ºç™½å­—ç¬¦
    """
    if not text:
        return False
    
    # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œå¸¸è§æ ‡ç‚¹ç¬¦å·
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ç­‰å¯å‘éŸ³å­—ç¬¦
    speakable = re.sub(r'[\s\.,;:!?ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿâ€¦\.]+', '', text)
    
    # å¦‚æœæ¸…ç†åè¿˜æœ‰å†…å®¹ï¼Œè¯´æ˜åŒ…å«å¯å‘éŸ³å­—ç¬¦
    return len(speakable) > 0


def split_long_text(text: str, max_length: int = 200) -> list:
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µï¼Œä¾¿äºTTSå¤„ç†
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§é•¿åº¦
        
    Returns:
        æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    current_chunk = ""
    
    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?\n])', text)
    
    for i in range(0, len(sentences), 2):
        sentence = sentences[i]
        delimiter = sentences[i + 1] if i + 1 < len(sentences) else ""
        full_sentence = sentence + delimiter
        
        if len(current_chunk) + len(full_sentence) <= max_length:
            current_chunk += full_sentence
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = full_sentence
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
