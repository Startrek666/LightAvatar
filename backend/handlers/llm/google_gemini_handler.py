"""
Google Gemini API Handler for LLM responses
ä½¿ç”¨è°·æ­Œäº‘åŸç”Ÿ Gemini APIï¼Œæ”¯æŒæµå¼å¯¹è¯å’Œå†å²è®°å½•
"""
from typing import AsyncGenerator, List, Dict, Optional, Any
from loguru import logger
from google import genai

from backend.handlers.base import BaseHandler


class GoogleGeminiHandler(BaseHandler):
    """Google Gemini API handler with streaming support"""
    
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp", config: Optional[dict] = None):
        """
        åˆå§‹åŒ– Google Gemini handler
        
        Args:
            api_key: Google API Key
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ gemini-2.0-flash-exp
            config: é¢å¤–é…ç½®
        """
        super().__init__(config)
        self.api_key = api_key
        self.model = model
        self.client = None
        self.chat = None
    
    async def _setup(self):
        """
        è®¾ç½® Google Gemini å®¢æˆ·ç«¯ï¼ˆBaseHandler æ¥å£ï¼‰
        """
        try:
            self.client = genai.Client(api_key=self.api_key)
            logger.info(f"âœ… Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model})")
            # åˆ›å»ºèŠå¤©ä¼šè¯
            self.create_chat()
        except Exception as e:
            logger.error(f"âŒ Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process(self, data: Any) -> Any:
        """
        å¤„ç†æ•°æ®ï¼ˆBaseHandler æ¥å£ï¼‰
        è¿™é‡Œä¸å®ç°ï¼Œä½¿ç”¨ stream_response ç³»åˆ—æ–¹æ³•
        """
        raise NotImplementedError("Use stream_response() or stream_response_with_search() instead")
    
    def create_chat(self):
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        try:
            self.chat = self.client.chats.create(model=self.model)
            logger.info(f"âœ… åˆ›å»ºæ–°çš„ Gemini èŠå¤©ä¼šè¯ (æ¨¡å‹: {self.model})")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º Gemini èŠå¤©ä¼šè¯å¤±è´¥: {e}")
            raise
    
    def get_history(self) -> List[Dict]:
        """
        è·å–èŠå¤©å†å²è®°å½•
        
        Returns:
            List of message dicts with 'role' and 'content'
        """
        if not self.chat:
            return []
        
        try:
            history = []
            for message in self.chat.get_history():
                history.append({
                    "role": message.role,
                    "content": message.parts[0].text if message.parts else ""
                })
            return history
        except Exception as e:
            logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []
    
    async def stream_response(
        self, 
        text: str,
        conversation_history: List[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            conversation_history: å¯¹è¯å†å²è®°å½• [{"role": "user/assistant", "content": "..."}]
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        if not self.chat:
            self.create_chat()
        
        # Google Gemini API é€šè¿‡ chat ç®¡ç†å†å²ï¼Œåªéœ€å‘é€æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_message = text
        
        if not user_message:
            logger.warning("âš ï¸ ç”¨æˆ·æ¶ˆæ¯ä¸ºç©º")
            return
        
        try:
            history = self.get_history()
            logger.info(f"ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ Gemini å“åº”")
            logger.info(f"  - æ¨¡å‹: {self.model}")
            logger.info(f"  - æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
            logger.info(f"  - å†å²æ¶ˆæ¯æ•°: {len(history)}")
            logger.info(f"  - å½“å‰å¯¹è¯å†å²æ•°: {len(conversation_history) if conversation_history else 0}")
            
            response = self.chat.send_message_stream(user_message)
            
            chunk_count = 0
            total_text = ""
            
            # Google Gemini çš„ send_message_stream è¿”å›åŒæ­¥è¿­ä»£å™¨
            # éœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­é€å—å¤„ç†ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            import asyncio
            
            # ä½¿ç”¨ run_in_executor åœ¨çº¿ç¨‹æ± ä¸­å¤„ç†åŒæ­¥è¿­ä»£å™¨
            loop = asyncio.get_event_loop()
            
            def get_next_chunk(iterator, sentinel=object()):
                """è·å–è¿­ä»£å™¨çš„ä¸‹ä¸€ä¸ªå…ƒç´ """
                try:
                    return next(iterator, sentinel)
                except StopIteration:
                    return sentinel
            
            sentinel = object()
            while True:
                # åœ¨çº¿ç¨‹æ± ä¸­è·å–ä¸‹ä¸€ä¸ª chunk
                chunk = await loop.run_in_executor(None, get_next_chunk, response, sentinel)
                
                if chunk is sentinel:
                    break
                
                if hasattr(chunk, 'text') and chunk.text:
                    chunk_count += 1
                    total_text += chunk.text
                    yield chunk.text
            
            logger.info(f"âœ… Gemini æµå¼å“åº”å®Œæˆ")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {len(total_text)}")
            logger.info(f"  - å“åº”é¢„è§ˆ: {total_text[:100]}...")
            
        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            yield error_msg
    
    async def stream_response_with_search(
        self,
        text: str,
        conversation_history: List[Dict] = None,
        search_handler=None,
        use_search: bool = True,
        search_mode: str = "advanced",
        momo_search_handler=None,
        momo_search_quality: str = "speed",
        progress_callback=None,
        search_results_callback=None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦æœç´¢åŠŸèƒ½çš„æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            search_handler: ç®€å•æœç´¢å¤„ç†å™¨ï¼ˆæœªä½¿ç”¨ï¼‰
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢
            search_mode: æœç´¢æ¨¡å¼ (simple/advanced)
            momo_search_handler: é«˜çº§æœç´¢å¤„ç†å™¨
            momo_search_quality: æœç´¢è´¨é‡ (speed/quality)
            progress_callback: æœç´¢è¿›åº¦å›è°ƒ
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        user_query = text
        
        if not user_query:
            logger.warning("âš ï¸ ç”¨æˆ·æŸ¥è¯¢ä¸ºç©º")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        # æ‰§è¡Œæœç´¢ï¼ˆä»…æ”¯æŒé«˜çº§æœç´¢ï¼‰
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šAgentæ¨¡å¼
        use_multi_agent = getattr(momo_search_handler, 'use_multi_agent', False) if momo_search_handler else False
        if use_multi_agent:
            logger.info(f"ğŸ¤– [å¤šAgentæ¨¡å¼] å¼€å§‹ Momo é«˜çº§æœç´¢ (è´¨é‡: {momo_search_quality})")
        else:
            logger.info(f"âš™ï¸ [ç®¡é“æ¨¡å¼] å¼€å§‹ Momo é«˜çº§æœç´¢ (è´¨é‡: {momo_search_quality})")
        
        if not momo_search_handler:
            logger.warning("âš ï¸ Momo æœç´¢å¤„ç†å™¨æœªæä¾›ï¼Œè·³è¿‡æœç´¢")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        # ä¿å­˜å¼•ç”¨ä¿¡æ¯
        citations_text = ""
        
        try:
            # Momo æœç´¢è¿”å› (relevant_docs, citations) å…ƒç»„
            search_results, citations, thinking_results = await momo_search_handler.search_with_progress(
                user_query,
                mode=momo_search_quality,
                progress_callback=progress_callback
            )
            
            # ä¿å­˜å¼•ç”¨ä¿¡æ¯ç”¨äºæœ€åæ·»åŠ 
            citations_text = citations
            
            # å‘é€æœç´¢ç»“æœåˆ°å‰ç«¯ï¼ˆç”¨äºå¼¹çª—æ˜¾ç¤ºï¼‰
            if search_results and search_results_callback:
                await search_results_callback(search_results)
            
            if search_results and len(search_results) > 0:
                logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                logger.info(f"ğŸ“š å¼•ç”¨ä¿¡æ¯é•¿åº¦: {len(citations)}")
                
                # ä½¿ç”¨æ€è€ƒé“¾æ„å»ºæ·±åº¦æ€è€ƒçš„ Prompt
                from datetime import datetime
                from backend.handlers.llm.thinking_chain import build_enhanced_search_prompt
                
                today = datetime.now().strftime("%Y-%m-%d")
                
                # æ ¹æ®æœç´¢è´¨é‡æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨æ€è€ƒé“¾
                # qualityï¼ˆæ·±åº¦ï¼‰æ¨¡å¼ï¼šä½¿ç”¨æ€è€ƒé“¾ï¼Œè¿›è¡Œæ·±åº¦æ€è€ƒ
                # speedï¼ˆå¿«é€Ÿï¼‰æ¨¡å¼ï¼šä½¿ç”¨ç®€å•æ¨¡å¼ï¼Œå¿«é€Ÿå›ç­”
                use_thinking_chain = (momo_search_quality == "quality")
                
                if use_thinking_chain:
                    logger.info(f"ğŸ§  [æ·±åº¦æ¨¡å¼] ä½¿ç”¨æ·±åº¦æ€è€ƒé“¾ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                    enhanced_text = build_enhanced_search_prompt(
                        user_query=user_query,
                        search_results=search_results,
                        current_date=today,
                        use_thinking_chain=True,
                        thinking_results=thinking_results
                    )
                else:
                    # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç®€å• Prompt
                    logger.info(f"âš¡ [å¿«é€Ÿæ¨¡å¼] ä½¿ç”¨ç®€å•æ¨¡å¼ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                    enhanced_text = build_enhanced_search_prompt(
                        user_query=user_query,
                        search_results=search_results,
                        current_date=today,
                        use_thinking_chain=False
                    )
                
                logger.info(f"ğŸ“ æœç´¢ä¸Šä¸‹æ–‡å·²æ„å»º (é•¿åº¦: {len(enhanced_text)}, æ€è€ƒé“¾: {use_thinking_chain})")
                
                logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€å¢å¼ºæ¶ˆæ¯ (æ€»é•¿åº¦: {len(enhanced_text)})")
                
                # ä½¿ç”¨å¢å¼ºåçš„æ¶ˆæ¯è¿›è¡Œç”Ÿæˆ
                async for chunk in self.stream_response(enhanced_text, conversation_history):
                    yield chunk
                
                # åœ¨å“åº”ç»“æŸåæ·»åŠ å¼•ç”¨ä¿¡æ¯
                if citations_text:
                    logger.info(f"ğŸ“š æ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°å“åº”æœ«å°¾")
                    yield f"\n\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n{citations_text}"
            else:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
                async for chunk in self.stream_response(text, conversation_history):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}", exc_info=True)
            logger.info("âš ï¸ æœç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
    
    def update_config(self, config: dict):
        """
        æ›´æ–°é…ç½®
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å« model, api_key ç­‰
        """
        updated = False
        
        if "model" in config and config["model"] != self.model:
            self.model = config["model"]
            logger.info(f"âœ… æ›´æ–° Gemini æ¨¡å‹: {self.model}")
            # æ¨¡å‹å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»º chat
            if self.chat:
                self.create_chat()
            updated = True
        
        if "api_key" in config and config["api_key"] != self.api_key:
            self.api_key = config["api_key"]
            logger.info("âœ… æ›´æ–° Gemini API Key")
            # API Key å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
            try:
                self.client = genai.Client(api_key=self.api_key)
                if self.chat:
                    self.create_chat()
                updated = True
            except Exception as e:
                logger.error(f"âŒ æ›´æ–° Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        if updated:
            logger.info("âœ… Gemini é…ç½®æ›´æ–°å®Œæˆ")

