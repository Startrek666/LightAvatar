"""
Google Gemini API Handler for LLM responses
ä½¿ç”¨è°·æ­Œäº‘åŸç”Ÿ Gemini APIï¼Œæ”¯æŒæµå¼å¯¹è¯å’Œå†å²è®°å½•
"""
import asyncio
from typing import AsyncGenerator, List, Dict, Optional, Any
from loguru import logger
from google import genai

from backend.handlers.base import BaseHandler


class GoogleGeminiHandler(BaseHandler):
    """Google Gemini API handler with streaming support"""
    
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp", config: Optional[dict] = None):
        """
        åˆå§‹åŒ– Google Gemini handler
        
        Args:
            api_key: Google API Key
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ gemini-2.0-flash-exp
            config: é¢å¤–é…ç½®
        """
        super().__init__(config)
        self.api_key = api_key
        self.model = model
        self.client = None
        self.chat = None
    
    async def _setup(self):
        """
        è®¾ç½® Google Gemini å®¢æˆ·ç«¯ï¼ˆBaseHandler æ¥å£ï¼‰
        """
        try:
            self.client = genai.Client(api_key=self.api_key)
            logger.info(f"âœ… Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model})")
            # åˆ›å»ºèŠå¤©ä¼šè¯
            self.create_chat()
        except Exception as e:
            logger.error(f"âŒ Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process(self, data: Any) -> Any:
        """
        å¤„ç†æ•°æ®ï¼ˆBaseHandler æ¥å£ï¼‰
        è¿™é‡Œä¸å®ç°ï¼Œä½¿ç”¨ stream_response ç³»åˆ—æ–¹æ³•
        """
        raise NotImplementedError("Use stream_response() or stream_response_with_search() instead")
    
    def create_chat(self):
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        try:
            self.chat = self.client.chats.create(model=self.model)
            logger.info(f"âœ… åˆ›å»ºæ–°çš„ Gemini èŠå¤©ä¼šè¯ (æ¨¡å‹: {self.model})")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º Gemini èŠå¤©ä¼šè¯å¤±è´¥: {e}")
            raise
    
    def get_history(self) -> List[Dict]:
        """
        è·å–èŠå¤©å†å²è®°å½•
        
        Returns:
            List of message dicts with 'role' and 'content'
        """
        if not self.chat:
            return []
        
        try:
            history = []
            for message in self.chat.get_history():
                history.append({
                    "role": message.role,
                    "content": message.parts[0].text if message.parts else ""
                })
            return history
        except Exception as e:
            logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []
    
    def _sync_conversation_history(self, conversation_history: List[Dict] = None):
        """
        åŒæ­¥ conversation_history åˆ° Gemini chat å¯¹è±¡
        
        Args:
            conversation_history: å¯¹è¯å†å²è®°å½• [{"role": "user/assistant", "content": "..."}]
        """
        if not conversation_history or len(conversation_history) == 0:
            return
        
        try:
            # è·å–å½“å‰ chat å¯¹è±¡çš„å†å²
            current_history = self.get_history()
            
            # æ¯”è¾ƒå†å²è®°å½•æ˜¯å¦ä¸€è‡´
            # conversation_history çš„æœ€åä¸€æ¡åº”è¯¥æ˜¯å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œéœ€è¦æ’é™¤å®ƒæ¥æ¯”è¾ƒ
            history_to_compare = conversation_history[:-1] if conversation_history and conversation_history[-1].get("role") == "user" else conversation_history
            
            # æ£€æŸ¥å†å²æ˜¯å¦ä¸€è‡´ï¼ˆæ¯”è¾ƒæ¶ˆæ¯æ•°é‡å’Œå†…å®¹ï¼‰
            if len(current_history) == len(history_to_compare):
                # å¦‚æœé•¿åº¦ç›¸åŒï¼Œæ£€æŸ¥å†…å®¹æ˜¯å¦ä¸€è‡´
                is_synced = True
                for i, (current_msg, expected_msg) in enumerate(zip(current_history, history_to_compare)):
                    current_role = current_msg.get("role", "")
                    expected_role = expected_msg.get("role", "")
                    current_content = current_msg.get("content", "").strip()
                    expected_content = expected_msg.get("content", "").strip()
                    
                    # è§’è‰²å¿…é¡»ä¸€è‡´ï¼Œå†…å®¹åº”è¯¥å¤§è‡´ç›¸åŒï¼ˆå…è®¸ä¸€äº›å·®å¼‚ï¼Œå› ä¸ºå¯èƒ½æœ‰æ ¼å¼å˜åŒ–ï¼‰
                    if current_role != expected_role or (len(current_content) > 0 and len(expected_content) > 0 and 
                                                         abs(len(current_content) - len(expected_content)) > len(expected_content) * 0.1):
                        is_synced = False
                        logger.info(f"ğŸ“Š å†å²è®°å½•ä¸ä¸€è‡´: ä½ç½®{i}, è§’è‰²={current_role}vs{expected_role}, é•¿åº¦å·®å¼‚={abs(len(current_content) - len(expected_content))}")
                        break
                
                if is_synced:
                    logger.info(f"âœ… å†å²è®°å½•å·²åŒæ­¥: {len(current_history)} æ¡æ¶ˆæ¯")
                    return
            
            # å†å²ä¸ä¸€è‡´ï¼Œéœ€è¦é‡å»º chat å¹¶é‡æ–°å‘é€å†å²æ¶ˆæ¯
            logger.info(f"ğŸ”„ å†å²è®°å½•ä¸ä¸€è‡´ï¼Œé‡å»º chat å¯¹è±¡")
            logger.info(f"  - Chatå†å²: {len(current_history)} æ¡")
            logger.info(f"  - Sessionå†å²: {len(history_to_compare)} æ¡")
            
            # é‡å»º chat å¯¹è±¡
            self.create_chat()
            
            # é‡æ–°å‘é€å†å²æ¶ˆæ¯ï¼ˆæˆå¯¹å¤„ç† user-assistantï¼‰
            # æ³¨æ„ï¼šGemini API çš„ chat å¯¹è±¡éœ€è¦å®é™…è°ƒç”¨ API æ‰èƒ½ä¿å­˜ assistant å›å¤
            # ä¸ºäº†å»ºç«‹å®Œæ•´å†å²ï¼Œæˆ‘ä»¬éœ€è¦å®é™…è°ƒç”¨ API è·å– assistant å›å¤
            if history_to_compare:
                logger.info(f"ğŸ“¤ é‡æ–°å‘é€ {len(history_to_compare)} æ¡å†å²æ¶ˆæ¯åˆ° Gemini")
                
                # å°†å†å²æ¶ˆæ¯é…å¯¹å¤„ç†ï¼ˆuser-assistant å¯¹ï¼‰
                i = 0
                while i < len(history_to_compare):
                    msg = history_to_compare[i]
                    role = msg.get("role", "")
                    content = msg.get("content", "").strip()
                    
                    if role == "user" and content:
                        try:
                            # æ£€æŸ¥ä¸‹ä¸€æ¡æ˜¯å¦æ˜¯ assistant å›å¤
                            if i + 1 < len(history_to_compare) and history_to_compare[i + 1].get("role") == "assistant":
                                # æœ‰å¯¹åº”çš„ assistant å›å¤
                                # å‘é€ user æ¶ˆæ¯ï¼ŒGemini ä¼šè‡ªåŠ¨ä¿å­˜ user å’Œ assistant
                                # æ³¨æ„ï¼šGemini API çš„ send_message ä¼šè¿”å› assistant å›å¤
                                # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬åªå‘é€ user æ¶ˆæ¯ï¼Œä¸ç­‰å¾… assistant å›å¤
                                # è¿™ä¼šå¯¼è‡´å†å²ä¸å®Œå…¨ä¸€è‡´ï¼Œä½†è‡³å°‘èƒ½ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§
                                # å®é™…ä¸Šï¼ŒGemini çš„ chat å¯¹è±¡ä¼šè‡ªåŠ¨ä¿å­˜ user å’Œ assistant çš„å¯¹è¯
                                # ä½†å¦‚æœæˆ‘ä»¬åªå‘é€ userï¼Œassistant ä¸ä¼šè‡ªåŠ¨å‡ºç°
                                # æ‰€ä»¥æˆ‘ä»¬éœ€è¦å®é™…è°ƒç”¨ API æ¥è·å– assistant å›å¤
                                # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬è·³è¿‡è¿™ä¸€æ­¥ï¼Œåªå‘é€ user æ¶ˆæ¯
                                self.chat.send_message(content)
                                i += 2  # è·³è¿‡ user å’Œ assistant
                            else:
                                # æ²¡æœ‰å¯¹åº”çš„ assistant å›å¤ï¼Œåªå‘é€ user æ¶ˆæ¯
                                self.chat.send_message(content)
                                i += 1
                        except Exception as e:
                            logger.warning(f"âš ï¸ å‘é€å†å²æ¶ˆæ¯å¤±è´¥: {e}")
                            i += 1
                    elif role == "assistant":
                        # assistant æ¶ˆæ¯ä¼šè¢«è‡ªåŠ¨ä¿å­˜ï¼ˆé€šè¿‡ä¹‹å‰çš„ user æ¶ˆæ¯è°ƒç”¨ï¼‰
                        i += 1
                    else:
                        i += 1
                
                logger.info(f"âœ… å†å²æ¶ˆæ¯åŒæ­¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å†å²è®°å½•å¤±è´¥: {e}", exc_info=True)
            # å¦‚æœåŒæ­¥å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰ chat å¯¹è±¡ï¼Œä¸ä¸­æ–­æµç¨‹
    
    async def stream_response(
        self, 
        text: str,
        conversation_history: List[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            conversation_history: å¯¹è¯å†å²è®°å½• [{"role": "user/assistant", "content": "..."}]
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        if not self.chat:
            self.create_chat()
        
        # åŒæ­¥ conversation_history åˆ° Gemini chat å¯¹è±¡
        self._sync_conversation_history(conversation_history)
        
        # Google Gemini API é€šè¿‡ chat ç®¡ç†å†å²ï¼Œåªéœ€å‘é€æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_message = text
        
        if not user_message:
            logger.warning("âš ï¸ ç”¨æˆ·æ¶ˆæ¯ä¸ºç©º")
            return
        
        try:
            history = self.get_history()
            logger.info(f"ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ Gemini å“åº”")
            logger.info(f"  - æ¨¡å‹: {self.model}")
            logger.info(f"  - æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
            logger.info(f"  - å†å²æ¶ˆæ¯æ•°: {len(history)}")
            logger.info(f"  - å½“å‰å¯¹è¯å†å²æ•°: {len(conversation_history) if conversation_history else 0}")
            
            response = self.chat.send_message_stream(user_message)
            
            chunk_count = 0
            total_text = ""
            
            # Google Gemini çš„ send_message_stream è¿”å›åŒæ­¥è¿­ä»£å™¨
            # éœ€è¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­é€å—å¤„ç†ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            # ä½¿ç”¨ run_in_executor åœ¨çº¿ç¨‹æ± ä¸­å¤„ç†åŒæ­¥è¿­ä»£å™¨
            loop = asyncio.get_event_loop()
            
            def get_next_chunk(iterator, sentinel=object()):
                """è·å–è¿­ä»£å™¨çš„ä¸‹ä¸€ä¸ªå…ƒç´ """
                try:
                    return next(iterator, sentinel)
                except StopIteration:
                    return sentinel
            
            sentinel = object()
            # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé¿å…åœ¨å¤„ç†å¤§è¡¨æ ¼æ—¶å¡æ­»
            CHUNK_TIMEOUT = 30.0  # æ¯ä¸ªchunkæœ€å¤šç­‰å¾…30ç§’
            
            while True:
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­è·å–ä¸‹ä¸€ä¸ª chunkï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
                    chunk = await asyncio.wait_for(
                        loop.run_in_executor(None, get_next_chunk, response, sentinel),
                        timeout=CHUNK_TIMEOUT
                    )
                    
                    if chunk is sentinel:
                        break
                    
                    if hasattr(chunk, 'text') and chunk.text:
                        chunk_count += 1
                        total_text += chunk.text
                        yield chunk.text
                        
                except asyncio.TimeoutError:
                    logger.error(f"âŒ Gemini chunk è·å–è¶…æ—¶ï¼ˆ{CHUNK_TIMEOUT}ç§’ï¼‰ï¼Œå¯èƒ½é‡åˆ°å¤§è¡¨æ ¼æˆ–ç½‘ç»œé—®é¢˜")
                    logger.error(f"  - å·²æ¥æ”¶å—æ•°: {chunk_count}")
                    logger.error(f"  - å·²æ¥æ”¶å­—ç¬¦æ•°: {len(total_text)}")
                    # å°è¯•ç»§ç»­è·å–ä¸‹ä¸€ä¸ªchunkï¼Œä½†å¦‚æœè¿ç»­è¶…æ—¶åˆ™é€€å‡º
                    try:
                        # å†å°è¯•ä¸€æ¬¡ï¼Œç¼©çŸ­è¶…æ—¶æ—¶é—´
                        chunk = await asyncio.wait_for(
                            loop.run_in_executor(None, get_next_chunk, response, sentinel),
                            timeout=5.0
                        )
                        if chunk is sentinel:
                            break
                        if hasattr(chunk, 'text') and chunk.text:
                            chunk_count += 1
                            total_text += chunk.text
                            yield chunk.text
                    except asyncio.TimeoutError:
                        logger.error("âŒ è¿ç»­è¶…æ—¶ï¼Œåœæ­¢æµå¼å¤„ç†")
                        break
            
            logger.info(f"âœ… Gemini æµå¼å“åº”å®Œæˆ")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {len(total_text)}")
            logger.info(f"  - å“åº”é¢„è§ˆ: {total_text[:100]}...")
            
        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            yield error_msg
    
    async def stream_response_with_search(
        self,
        text: str,
        conversation_history: List[Dict] = None,
        search_handler=None,
        use_search: bool = True,
        search_mode: str = "advanced",
        momo_search_handler=None,
        momo_search_quality: str = "speed",
        progress_callback=None,
        search_results_callback=None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦æœç´¢åŠŸèƒ½çš„æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            search_handler: ç®€å•æœç´¢å¤„ç†å™¨ï¼ˆæœªä½¿ç”¨ï¼‰
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢
            search_mode: æœç´¢æ¨¡å¼ (simple/advanced)
            momo_search_handler: é«˜çº§æœç´¢å¤„ç†å™¨
            momo_search_quality: æœç´¢è´¨é‡ (speed/quality)
            progress_callback: æœç´¢è¿›åº¦å›è°ƒ
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        user_query = text
        
        if not user_query:
            logger.warning("âš ï¸ ç”¨æˆ·æŸ¥è¯¢ä¸ºç©º")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        # æ‰§è¡Œæœç´¢ï¼ˆä»…æ”¯æŒé«˜çº§æœç´¢ï¼‰
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šAgentæ¨¡å¼
        use_multi_agent = getattr(momo_search_handler, 'use_multi_agent', False) if momo_search_handler else False
        if use_multi_agent:
            logger.info(f"ğŸ¤– [å¤šAgentæ¨¡å¼] å¼€å§‹ Momo é«˜çº§æœç´¢ (è´¨é‡: {momo_search_quality})")
        else:
            logger.info(f"âš™ï¸ [ç®¡é“æ¨¡å¼] å¼€å§‹ Momo é«˜çº§æœç´¢ (è´¨é‡: {momo_search_quality})")
        
        if not momo_search_handler:
            logger.warning("âš ï¸ Momo æœç´¢å¤„ç†å™¨æœªæä¾›ï¼Œè·³è¿‡æœç´¢")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        # ä¿å­˜å¼•ç”¨ä¿¡æ¯
        citations_text = ""
        
        try:
            # Momo æœç´¢è¿”å› (relevant_docs, citations) å…ƒç»„
            search_results, citations, thinking_results = await momo_search_handler.search_with_progress(
                user_query,
                mode=momo_search_quality,
                progress_callback=progress_callback,
                conversation_history=conversation_history  # ä¼ é€’å¯¹è¯å†å²ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£
            )
            
            # ä¿å­˜å¼•ç”¨ä¿¡æ¯ç”¨äºæœ€åæ·»åŠ 
            citations_text = citations
            
            # å‘é€æœç´¢ç»“æœåˆ°å‰ç«¯ï¼ˆç”¨äºå¼¹çª—æ˜¾ç¤ºï¼‰
            if search_results and search_results_callback:
                await search_results_callback(search_results)
            
            if search_results and len(search_results) > 0:
                logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                logger.info(f"ğŸ“š å¼•ç”¨ä¿¡æ¯é•¿åº¦: {len(citations)}")
                
                # ä½¿ç”¨æ€è€ƒé“¾æ„å»ºæ·±åº¦æ€è€ƒçš„ Prompt
                from datetime import datetime
                from backend.handlers.llm.thinking_chain import build_enhanced_search_prompt
                
                today = datetime.now().strftime("%Y-%m-%d")
                
                # æ ¹æ®æœç´¢è´¨é‡æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨æ€è€ƒé“¾
                # qualityï¼ˆæ·±åº¦ï¼‰æ¨¡å¼ï¼šä½¿ç”¨æ€è€ƒé“¾ï¼Œè¿›è¡Œæ·±åº¦æ€è€ƒ
                # speedï¼ˆå¿«é€Ÿï¼‰æ¨¡å¼ï¼šä½¿ç”¨ç®€å•æ¨¡å¼ï¼Œå¿«é€Ÿå›ç­”
                use_thinking_chain = (momo_search_quality == "quality")
                
                if use_thinking_chain:
                    logger.info(f"ğŸ§  [æ·±åº¦æ¨¡å¼] ä½¿ç”¨æ·±åº¦æ€è€ƒé“¾ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                    enhanced_text = build_enhanced_search_prompt(
                        user_query=user_query,
                        search_results=search_results,
                        current_date=today,
                        use_thinking_chain=True,
                        thinking_results=thinking_results
                    )
                else:
                    # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç®€å• Prompt
                    logger.info(f"âš¡ [å¿«é€Ÿæ¨¡å¼] ä½¿ç”¨ç®€å•æ¨¡å¼ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                    enhanced_text = build_enhanced_search_prompt(
                        user_query=user_query,
                        search_results=search_results,
                        current_date=today,
                        use_thinking_chain=False
                    )
                
                logger.info(f"ğŸ“ æœç´¢ä¸Šä¸‹æ–‡å·²æ„å»º (é•¿åº¦: {len(enhanced_text)}, æ€è€ƒé“¾: {use_thinking_chain})")
                
                logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€å¢å¼ºæ¶ˆæ¯ (æ€»é•¿åº¦: {len(enhanced_text)})")
                
                # ä½¿ç”¨å¢å¼ºåçš„æ¶ˆæ¯è¿›è¡Œç”Ÿæˆ
                async for chunk in self.stream_response(enhanced_text, conversation_history):
                    yield chunk
                
                # åœ¨å“åº”ç»“æŸåæ·»åŠ å¼•ç”¨ä¿¡æ¯
                if citations_text:
                    logger.info(f"ğŸ“š æ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°å“åº”æœ«å°¾")
                    yield f"\n\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n{citations_text}"
            else:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
                async for chunk in self.stream_response(text, conversation_history):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}", exc_info=True)
            logger.info("âš ï¸ æœç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
    
    def update_config(self, config: dict):
        """
        æ›´æ–°é…ç½®
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å« model, api_key ç­‰
        """
        updated = False
        
        if "model" in config and config["model"] != self.model:
            self.model = config["model"]
            logger.info(f"âœ… æ›´æ–° Gemini æ¨¡å‹: {self.model}")
            # æ¨¡å‹å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»º chat
            if self.chat:
                self.create_chat()
            updated = True
        
        if "api_key" in config and config["api_key"] != self.api_key:
            self.api_key = config["api_key"]
            logger.info("âœ… æ›´æ–° Gemini API Key")
            # API Key å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
            try:
                self.client = genai.Client(api_key=self.api_key)
                if self.chat:
                    self.create_chat()
                updated = True
            except Exception as e:
                logger.error(f"âŒ æ›´æ–° Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        if updated:
            logger.info("âœ… Gemini é…ç½®æ›´æ–°å®Œæˆ")

