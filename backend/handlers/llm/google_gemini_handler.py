"""
Google Gemini API Handler for LLM responses
ä½¿ç”¨è°·æ­Œäº‘åŸç”Ÿ Gemini APIï¼Œæ”¯æŒæµå¼å¯¹è¯å’Œå†å²è®°å½•
"""
from typing import AsyncGenerator, List, Dict, Optional, Any
from loguru import logger
from google import genai

from backend.handlers.base import BaseHandler


class GoogleGeminiHandler(BaseHandler):
    """Google Gemini API handler with streaming support"""
    
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp", config: Optional[dict] = None):
        """
        åˆå§‹åŒ– Google Gemini handler
        
        Args:
            api_key: Google API Key
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ gemini-2.0-flash-exp
            config: é¢å¤–é…ç½®
        """
        super().__init__(config)
        self.api_key = api_key
        self.model = model
        self.client = None
        self.chat = None
    
    async def _setup(self):
        """
        è®¾ç½® Google Gemini å®¢æˆ·ç«¯ï¼ˆBaseHandler æ¥å£ï¼‰
        """
        try:
            self.client = genai.Client(api_key=self.api_key)
            logger.info(f"âœ… Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model})")
            # åˆ›å»ºèŠå¤©ä¼šè¯
            self.create_chat()
        except Exception as e:
            logger.error(f"âŒ Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process(self, data: Any) -> Any:
        """
        å¤„ç†æ•°æ®ï¼ˆBaseHandler æ¥å£ï¼‰
        è¿™é‡Œä¸å®ç°ï¼Œä½¿ç”¨ stream_response ç³»åˆ—æ–¹æ³•
        """
        raise NotImplementedError("Use stream_response() or stream_response_with_search() instead")
    
    def create_chat(self):
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        try:
            self.chat = self.client.chats.create(model=self.model)
            logger.info(f"âœ… åˆ›å»ºæ–°çš„ Gemini èŠå¤©ä¼šè¯ (æ¨¡å‹: {self.model})")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º Gemini èŠå¤©ä¼šè¯å¤±è´¥: {e}")
            raise
    
    def get_history(self) -> List[Dict]:
        """
        è·å–èŠå¤©å†å²è®°å½•
        
        Returns:
            List of message dicts with 'role' and 'content'
        """
        if not self.chat:
            return []
        
        try:
            history = []
            for message in self.chat.get_history():
                history.append({
                    "role": message.role,
                    "content": message.parts[0].text if message.parts else ""
                })
            return history
        except Exception as e:
            logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []
    
    async def stream_response(
        self, 
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå“åº”
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant/system", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•° (æœªä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´)
            max_tokens: æœ€å¤§tokenæ•° (æœªä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´)
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        if not self.chat:
            self.create_chat()
        
        # Google Gemini API é€šè¿‡ chat ç®¡ç†å†å²ï¼Œåªéœ€å‘é€æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        # ç³»ç»Ÿæç¤ºè¯åœ¨é¦–æ¬¡æ¶ˆæ¯æ—¶å‘é€
        user_message = None
        system_prompt = None
        
        # æå–ç³»ç»Ÿæç¤ºå’Œæœ€æ–°ç”¨æˆ·æ¶ˆæ¯
        for msg in messages:
            if msg["role"] == "system":
                system_prompt = msg["content"]
            elif msg["role"] == "user":
                user_message = msg["content"]
        
        if not user_message:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
            return
        
        # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ä¸”æ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œéœ€è¦åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
        # (Google Gemini ä¸ç›´æ¥æ”¯æŒ system roleï¼Œéœ€è¦åœ¨ç”¨æˆ·æ¶ˆæ¯ä¸­åŒ…å«)
        history = self.get_history()
        if system_prompt and len(history) == 0:
            # é¦–æ¬¡å¯¹è¯ï¼Œå°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯å‰
            user_message = f"{system_prompt}\n\n{user_message}"
            logger.info(f"ğŸ“ é¦–æ¬¡å¯¹è¯ï¼Œåˆå¹¶ç³»ç»Ÿæç¤ºè¯ (é•¿åº¦: {len(system_prompt)})")
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ Gemini å“åº”")
            logger.info(f"  - æ¨¡å‹: {self.model}")
            logger.info(f"  - æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
            logger.info(f"  - å†å²æ¶ˆæ¯æ•°: {len(history)}")
            
            response = self.chat.send_message_stream(user_message)
            
            chunk_count = 0
            total_text = ""
            
            for chunk in response:
                if chunk.text:
                    chunk_count += 1
                    total_text += chunk.text
                    yield chunk.text
            
            logger.info(f"âœ… Gemini æµå¼å“åº”å®Œæˆ")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {len(total_text)}")
            logger.info(f"  - å“åº”é¢„è§ˆ: {total_text[:100]}...")
            
        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            yield error_msg
    
    async def stream_response_with_search(
        self,
        messages: List[Dict[str, str]],
        search_handler,
        search_mode: str = "simple",
        search_quality: str = "speed",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        search_progress_callback=None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦æœç´¢åŠŸèƒ½çš„æµå¼ç”Ÿæˆå“åº”
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            search_handler: æœç´¢å¤„ç†å™¨
            search_mode: æœç´¢æ¨¡å¼ (simple/advanced)
            search_quality: æœç´¢è´¨é‡ (speed/quality)
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            search_progress_callback: æœç´¢è¿›åº¦å›è°ƒ
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        # æå–ç”¨æˆ·æŸ¥è¯¢
        user_query = None
        for msg in reversed(messages):
            if msg["role"] == "user":
                user_query = msg["content"]
                break
        
        if not user_query:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æŸ¥è¯¢")
            async for chunk in self.stream_response(messages, temperature, max_tokens):
                yield chunk
            return
        
        # æ‰§è¡Œæœç´¢
        logger.info(f"ğŸ” å¼€å§‹æœç´¢ (æ¨¡å¼: {search_mode}, è´¨é‡: {search_quality})")
        
        try:
            if search_mode == "advanced":
                search_results = await search_handler.search_with_progress(
                    user_query,
                    mode=search_quality,
                    progress_callback=search_progress_callback
                )
            else:
                search_results = await search_handler.search(user_query)
            
            if search_results and len(search_results) > 0:
                logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                
                # æ„å»ºæœç´¢ä¸Šä¸‹æ–‡
                from datetime import datetime
                today = datetime.now().strftime("%Y-%m-%d")
                
                search_context = f"# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœï¼ˆä»Šå¤©æ˜¯{today}ï¼‰:\n\n"
                
                for idx, doc in enumerate(search_results[:15], 1):  # é™åˆ¶15ä¸ªç»“æœ
                    search_context += f"[ç½‘é¡µ {idx} å¼€å§‹]\n\n"
                    search_context += f"æ ‡é¢˜: {doc.get('title', 'N/A')}\n\n"
                    search_context += f"é“¾æ¥: {doc.get('url', 'N/A')}\n\n"
                    
                    content = doc.get('content', '')
                    if content:
                        # é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹é•¿åº¦
                        content = content[:1000] if len(content) > 1000 else content
                        search_context += f"å†…å®¹æ‘˜è¦:\n{content}\n\n"
                    
                    search_context += f"[ç½‘é¡µ {idx} ç»“æŸ]\n\n"
                
                search_context += "# è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ä¸”å¼•ç”¨æ¥æºã€‚\n\n"
                
                logger.info(f"ğŸ“ æœç´¢ä¸Šä¸‹æ–‡å·²æ„å»º (é•¿åº¦: {len(search_context)})")
                
                # å°†æœç´¢ç»“æœæ³¨å…¥åˆ°æ¶ˆæ¯ä¸­
                enhanced_messages = []
                for msg in messages:
                    if msg["role"] == "system":
                        enhanced_messages.append(msg)
                
                # æ·»åŠ æœç´¢ä¸Šä¸‹æ–‡ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
                enhanced_messages.append({
                    "role": "system",
                    "content": search_context
                })
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                for msg in messages:
                    if msg["role"] == "user":
                        enhanced_messages.append(msg)
                
                logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€å¢å¼ºæ¶ˆæ¯ (å…± {len(enhanced_messages)} æ¡)")
                
                # ä½¿ç”¨å¢å¼ºåçš„æ¶ˆæ¯è¿›è¡Œç”Ÿæˆ
                async for chunk in self.stream_response(enhanced_messages, temperature, max_tokens):
                    yield chunk
            else:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
                async for chunk in self.stream_response(messages, temperature, max_tokens):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}", exc_info=True)
            logger.info("âš ï¸ æœç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
            async for chunk in self.stream_response(messages, temperature, max_tokens):
                yield chunk
    
    def update_config(self, config: dict):
        """
        æ›´æ–°é…ç½®
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å« model, api_key ç­‰
        """
        updated = False
        
        if "model" in config and config["model"] != self.model:
            self.model = config["model"]
            logger.info(f"âœ… æ›´æ–° Gemini æ¨¡å‹: {self.model}")
            # æ¨¡å‹å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»º chat
            if self.chat:
                self.create_chat()
            updated = True
        
        if "api_key" in config and config["api_key"] != self.api_key:
            self.api_key = config["api_key"]
            logger.info("âœ… æ›´æ–° Gemini API Key")
            # API Key å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
            try:
                self.client = genai.Client(api_key=self.api_key)
                if self.chat:
                    self.create_chat()
                updated = True
            except Exception as e:
                logger.error(f"âŒ æ›´æ–° Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        if updated:
            logger.info("âœ… Gemini é…ç½®æ›´æ–°å®Œæˆ")

