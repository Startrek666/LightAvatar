"""
Google Gemini API Handler for LLM responses
ä½¿ç”¨è°·æ­Œäº‘åŸç”Ÿ Gemini APIï¼Œæ”¯æŒæµå¼å¯¹è¯å’Œå†å²è®°å½•
"""
from typing import AsyncGenerator, List, Dict, Optional, Any
from loguru import logger
from google import genai

from backend.handlers.base import BaseHandler


class GoogleGeminiHandler(BaseHandler):
    """Google Gemini API handler with streaming support"""
    
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp", config: Optional[dict] = None):
        """
        åˆå§‹åŒ– Google Gemini handler
        
        Args:
            api_key: Google API Key
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ gemini-2.0-flash-exp
            config: é¢å¤–é…ç½®
        """
        super().__init__(config)
        self.api_key = api_key
        self.model = model
        self.client = None
        self.chat = None
    
    async def _setup(self):
        """
        è®¾ç½® Google Gemini å®¢æˆ·ç«¯ï¼ˆBaseHandler æ¥å£ï¼‰
        """
        try:
            self.client = genai.Client(api_key=self.api_key)
            logger.info(f"âœ… Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model})")
            # åˆ›å»ºèŠå¤©ä¼šè¯
            self.create_chat()
        except Exception as e:
            logger.error(f"âŒ Google Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process(self, data: Any) -> Any:
        """
        å¤„ç†æ•°æ®ï¼ˆBaseHandler æ¥å£ï¼‰
        è¿™é‡Œä¸å®ç°ï¼Œä½¿ç”¨ stream_response ç³»åˆ—æ–¹æ³•
        """
        raise NotImplementedError("Use stream_response() or stream_response_with_search() instead")
    
    def create_chat(self):
        """åˆ›å»ºæ–°çš„èŠå¤©ä¼šè¯"""
        try:
            self.chat = self.client.chats.create(model=self.model)
            logger.info(f"âœ… åˆ›å»ºæ–°çš„ Gemini èŠå¤©ä¼šè¯ (æ¨¡å‹: {self.model})")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º Gemini èŠå¤©ä¼šè¯å¤±è´¥: {e}")
            raise
    
    def get_history(self) -> List[Dict]:
        """
        è·å–èŠå¤©å†å²è®°å½•
        
        Returns:
            List of message dicts with 'role' and 'content'
        """
        if not self.chat:
            return []
        
        try:
            history = []
            for message in self.chat.get_history():
                history.append({
                    "role": message.role,
                    "content": message.parts[0].text if message.parts else ""
                })
            return history
        except Exception as e:
            logger.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {e}")
            return []
    
    async def stream_response(
        self, 
        text: str,
        conversation_history: List[Dict] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            conversation_history: å¯¹è¯å†å²è®°å½• [{"role": "user/assistant", "content": "..."}]
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        if not self.chat:
            self.create_chat()
        
        # Google Gemini API é€šè¿‡ chat ç®¡ç†å†å²ï¼Œåªéœ€å‘é€æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_message = text
        
        if not user_message:
            logger.warning("âš ï¸ ç”¨æˆ·æ¶ˆæ¯ä¸ºç©º")
            return
        
        try:
            history = self.get_history()
            logger.info(f"ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ Gemini å“åº”")
            logger.info(f"  - æ¨¡å‹: {self.model}")
            logger.info(f"  - æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
            logger.info(f"  - å†å²æ¶ˆæ¯æ•°: {len(history)}")
            logger.info(f"  - å½“å‰å¯¹è¯å†å²æ•°: {len(conversation_history) if conversation_history else 0}")
            
            response = self.chat.send_message_stream(user_message)
            
            chunk_count = 0
            total_text = ""
            
            for chunk in response:
                if chunk.text:
                    chunk_count += 1
                    total_text += chunk.text
                    yield chunk.text
            
            logger.info(f"âœ… Gemini æµå¼å“åº”å®Œæˆ")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {len(total_text)}")
            logger.info(f"  - å“åº”é¢„è§ˆ: {total_text[:100]}...")
            
        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            yield error_msg
    
    async def stream_response_with_search(
        self,
        text: str,
        conversation_history: List[Dict] = None,
        search_handler=None,
        use_search: bool = True,
        search_mode: str = "advanced",
        momo_search_handler=None,
        momo_search_quality: str = "speed",
        progress_callback=None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦æœç´¢åŠŸèƒ½çš„æµå¼ç”Ÿæˆå“åº”ï¼ˆä¸ OpenAIHandler æ¥å£ä¸€è‡´ï¼‰
        
        Args:
            text: å½“å‰ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²
            search_handler: ç®€å•æœç´¢å¤„ç†å™¨ï¼ˆæœªä½¿ç”¨ï¼‰
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢
            search_mode: æœç´¢æ¨¡å¼ (simple/advanced)
            momo_search_handler: é«˜çº§æœç´¢å¤„ç†å™¨
            momo_search_quality: æœç´¢è´¨é‡ (speed/quality)
            progress_callback: æœç´¢è¿›åº¦å›è°ƒ
            
        Yields:
            str: å“åº”æ–‡æœ¬å—
        """
        user_query = text
        
        if not user_query:
            logger.warning("âš ï¸ ç”¨æˆ·æŸ¥è¯¢ä¸ºç©º")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        # æ‰§è¡Œæœç´¢ï¼ˆä»…æ”¯æŒé«˜çº§æœç´¢ï¼‰
        logger.info(f"ğŸ” å¼€å§‹ Momo é«˜çº§æœç´¢ (è´¨é‡: {momo_search_quality})")
        
        if not momo_search_handler:
            logger.warning("âš ï¸ Momo æœç´¢å¤„ç†å™¨æœªæä¾›ï¼Œè·³è¿‡æœç´¢")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
            return
        
        try:
            search_results = await momo_search_handler.search_with_progress(
                user_query,
                mode=momo_search_quality,
                progress_callback=progress_callback
            )
            
            if search_results and len(search_results) > 0:
                logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ")
                
                # æ„å»ºæœç´¢ä¸Šä¸‹æ–‡
                from datetime import datetime
                today = datetime.now().strftime("%Y-%m-%d")
                
                search_context = f"# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœï¼ˆä»Šå¤©æ˜¯{today}ï¼‰:\n\n"
                
                for idx, doc in enumerate(search_results[:15], 1):  # é™åˆ¶15ä¸ªç»“æœ
                    search_context += f"[ç½‘é¡µ {idx} å¼€å§‹]\n\n"
                    search_context += f"æ ‡é¢˜: {doc.get('title', 'N/A')}\n\n"
                    search_context += f"é“¾æ¥: {doc.get('url', 'N/A')}\n\n"
                    
                    content = doc.get('content', '')
                    if content:
                        # é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹é•¿åº¦
                        content = content[:1000] if len(content) > 1000 else content
                        search_context += f"å†…å®¹æ‘˜è¦:\n{content}\n\n"
                    
                    search_context += f"[ç½‘é¡µ {idx} ç»“æŸ]\n\n"
                
                search_context += "# è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ä¸”å¼•ç”¨æ¥æºã€‚\n\n"
                
                logger.info(f"ğŸ“ æœç´¢ä¸Šä¸‹æ–‡å·²æ„å»º (é•¿åº¦: {len(search_context)})")
                
                # å°†æœç´¢ä¸Šä¸‹æ–‡åˆå¹¶åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
                enhanced_text = f"{search_context}\n\nç”¨æˆ·é—®é¢˜: {user_query}"
                
                logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€å¢å¼ºæ¶ˆæ¯ (æ€»é•¿åº¦: {len(enhanced_text)})")
                
                # ä½¿ç”¨å¢å¼ºåçš„æ¶ˆæ¯è¿›è¡Œç”Ÿæˆ
                async for chunk in self.stream_response(enhanced_text, conversation_history):
                    yield chunk
            else:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
                async for chunk in self.stream_response(text, conversation_history):
                    yield chunk
                    
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}", exc_info=True)
            logger.info("âš ï¸ æœç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯")
            async for chunk in self.stream_response(text, conversation_history):
                yield chunk
    
    def update_config(self, config: dict):
        """
        æ›´æ–°é…ç½®
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å« model, api_key ç­‰
        """
        updated = False
        
        if "model" in config and config["model"] != self.model:
            self.model = config["model"]
            logger.info(f"âœ… æ›´æ–° Gemini æ¨¡å‹: {self.model}")
            # æ¨¡å‹å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»º chat
            if self.chat:
                self.create_chat()
            updated = True
        
        if "api_key" in config and config["api_key"] != self.api_key:
            self.api_key = config["api_key"]
            logger.info("âœ… æ›´æ–° Gemini API Key")
            # API Key å˜æ›´ï¼Œéœ€è¦é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
            try:
                self.client = genai.Client(api_key=self.api_key)
                if self.chat:
                    self.create_chat()
                updated = True
            except Exception as e:
                logger.error(f"âŒ æ›´æ–° Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        if updated:
            logger.info("âœ… Gemini é…ç½®æ›´æ–°å®Œæˆ")

