"""
OpenAI-compatible API Handler for language models
"""
import json
from typing import List, Dict, Optional, AsyncGenerator
from openai import AsyncOpenAI
from loguru import logger

from backend.handlers.base import BaseHandler
from backend.core.health_monitor import timer, llm_processing_time


class OpenAIHandler(BaseHandler):
    """Handler for OpenAI-compatible API endpoints"""
    
    def __init__(self, 
                 api_url: str,
                 api_key: str,
                 model: str = "gpt-3.5-turbo",
                 config: Optional[dict] = None):
        super().__init__(config)
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
        self.client = None
        
        # Generation parameters
        self.temperature = self.config.get("temperature", 0.7)
        self.max_tokens = self.config.get("max_tokens", 20000)  # æé«˜é»˜è®¤å€¼ä»¥æ”¯æŒé•¿å›å¤
        self.top_p = self.config.get("top_p", 0.9)
        self.presence_penalty = self.config.get("presence_penalty", 0)
        self.frequency_penalty = self.config.get("frequency_penalty", 0)
        self.system_prompt = self.config.get(
            "system_prompt",
            "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
        )
        
    async def _setup(self):
        """Setup OpenAI client"""
        try:
            # Initialize async OpenAI client
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_url
            )
            
            # Test connection
            await self._test_connection()
            
            logger.info(f"OpenAI client initialized with model '{self.model}'")
            
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise
    
    async def _test_connection(self):
        """Test API connection"""
        try:
            # Try to list models or make a simple completion
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            logger.info("API connection test successful")
        except Exception as e:
            logger.error(f"API connection test failed: {e}")
            raise
    
    async def process(self, text: str, conversation_history: List[Dict] = None) -> str:
        """
        Generate response for given text
        
        Args:
            text: User input text
            conversation_history: Previous conversation messages
            
        Returns:
            Generated response text
        """
        with timer(llm_processing_time):
            return await self._generate_response(text, conversation_history)
    
    async def _generate_response(self, text: str, conversation_history: List[Dict] = None) -> str:
        """Generate response using the API"""
        try:
            # Prepare messages
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Add conversation history
            if conversation_history:
                # Keep last N messages to avoid token limit
                max_history = self.config.get("max_history", 10)
                recent_history = conversation_history[-max_history:]
                
                # æ³¨æ„ï¼šrecent_history å·²ç»åŒ…å«å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆåœ¨ session_manager ä¸­æ·»åŠ ï¼‰ï¼Œç›´æ¥å…¨éƒ¨æ·»åŠ å³å¯
                for msg in recent_history:
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
            else:
                # æ²¡æœ‰å†å²è®°å½•ï¼Œç›´æ¥æ·»åŠ æ–°æ¶ˆæ¯
                messages.append({"role": "user", "content": text})
            
            # Generate response
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                top_p=self.top_p,
                presence_penalty=self.presence_penalty,
                frequency_penalty=self.frequency_penalty
            )
            
            # Extract response text
            response_text = response.choices[0].message.content
            
            logger.info(f"Generated response: {response_text[:100]}...")
            
            return response_text
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def generate_response(self, text: str, conversation_history: List[Dict] = None) -> str:
        """Public method to generate response"""
        if not self._initialized:
            await self.initialize()
        
        return await self.process(text, conversation_history)
    
    async def stream_response_with_search(
        self, 
        text: str, 
        conversation_history: List[Dict] = None,
        search_handler = None,
        use_search: bool = False,
        search_mode: str = "simple",  # "simple" æˆ– "advanced"
        momo_search_handler = None,
        momo_search_quality: str = "speed",  # "speed" æˆ– "quality"
        progress_callback = None,
        search_results_callback = None
    ) -> AsyncGenerator[str, None]:
        """
        Generate streaming response with optional web search
        
        Args:
            text: User input text
            conversation_history: Previous conversation
            search_handler: WebSearchHandler instance (ç®€å•æœç´¢)
            use_search: Whether to perform web search
            search_mode: æœç´¢æ¨¡å¼ ("simple" æˆ– "advanced")
            momo_search_handler: MomoSearchHandler instance (é«˜çº§æœç´¢)
            momo_search_quality: Momoæœç´¢è´¨é‡ ("speed" æˆ– "quality")
            progress_callback: Callback for search progress (step, total, message)
        """
        try:
            # Prepare messages
            messages = [{"role": "system", "content": self.system_prompt}]
            
            if conversation_history:
                # Gemma æ¨¡å‹ç‰¹æ®Šå¤„ç†ï¼šæ¯5è½®å¯¹è¯é‡ç½®ä¸€æ¬¡ä¸Šä¸‹æ–‡
                # æ³¨æ„ï¼šå¿…é¡»å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®ï¼ˆåŸºäºå®Œæ•´å†å²ï¼‰ï¼Œå†åº”ç”¨ max_history æˆªæ–­
                if 'gemma' in self.model.lower():
                    # conversation_history ä¸­æœ€åä¸€æ¡æ˜¯å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆå·²ç»åœ¨ session_manager ä¸­æ·»åŠ ï¼‰
                    # æ‰€ä»¥è¦æ’é™¤å®ƒæ¥è®¡ç®—ä¹‹å‰çš„å¯¹è¯è½®æ•°
                    history_without_current = conversation_history[:-1] if conversation_history and conversation_history[-1].get("role") == "user" else conversation_history
                    user_messages = [m for m in history_without_current if m["role"] == "user"] if history_without_current else []
                    
                    logger.info(f"ğŸ“Š Gemmaæ¨¡å‹(æœç´¢æ¨¡å¼)å¯¹è¯ç»Ÿè®¡: å®Œæ•´å†å²={len(conversation_history) if conversation_history else 0}æ¡, ç”¨æˆ·æ¶ˆæ¯={len(user_messages)}æ¡")
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®ï¼ˆç¬¬5è½®ä¹‹åï¼Œå³ç¬¬6ã€11ã€16è½®...ï¼‰
                    if len(user_messages) >= 5 and len(user_messages) % 5 == 0:
                        logger.info(f"ğŸ”„ Gemmaæ¨¡å‹(æœç´¢æ¨¡å¼)æ£€æµ‹åˆ°ç¬¬ {len(user_messages)+1} è½®å¯¹è¯ï¼Œæ‰§è¡Œä¸Šä¸‹æ–‡é‡ç½®")
                        
                        # ä» history_without_current ä¸­å–æœ€å2æ¡æ¶ˆæ¯ï¼ˆä¸Šä¸€è½®å¯¹è¯ï¼‰
                        if history_without_current and len(history_without_current) >= 2:
                            last_user_msg = history_without_current[-2]  # ä¸Šä¸€è½®çš„useræ¶ˆæ¯
                            last_assistant_msg = history_without_current[-1]  # ä¸Šä¸€è½®çš„assistantå›å¤
                            
                            # éªŒè¯æ ¼å¼æ­£ç¡®
                            if last_user_msg["role"] == "user" and last_assistant_msg["role"] == "assistant":
                                # åˆå¹¶ä¸Šä¸€è½®å¯¹è¯å’Œæ–°æ¶ˆæ¯ä½œä¸ºæ–°çš„ç¬¬ä¸€è½®
                                merged_content = f"ä¹‹å‰çš„å¯¹è¯ï¼š\nç”¨æˆ·: {last_user_msg['content']}\nåŠ©æ‰‹: {last_assistant_msg['content']}\n\nå½“å‰é—®é¢˜: {text}"
                                messages.append({"role": "user", "content": merged_content})
                                logger.info(f"âœ… é‡ç½®ä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™ä¸Šä¸€è½®(user:{len(last_user_msg['content'])}å­— + assistant:{len(last_assistant_msg['content'])}å­—) + æ–°æ¶ˆæ¯({len(text) if text else 0}å­—)")
                            else:
                                # æ ¼å¼ä¸å¯¹ï¼Œè¯´æ˜å†å²è®°å½•å¼‚å¸¸ï¼ŒæŒ‰æ­£å¸¸æµç¨‹å¤„ç†ï¼ˆä½†æ’é™¤å½“å‰è¾“å…¥ï¼Œå› ä¸ºåé¢ä¼šç»Ÿä¸€æ·»åŠ ï¼‰
                                logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡é‡ç½®å¤±è´¥ï¼šå†å²è®°å½•æ ¼å¼ä¸æ­£ç¡® (last_user_msg role={last_user_msg.get('role')}, last_assistant_msg role={last_assistant_msg.get('role')})")
                                for msg in history_without_current:
                                    messages.append({
                                        "role": msg["role"],
                                        "content": msg["content"]
                                    })
                                # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
                                messages.append({"role": "user", "content": text})
                        else:
                            # å†å²è®°å½•ä¸è¶³ï¼ŒæŒ‰æ­£å¸¸æµç¨‹å¤„ç†
                            logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡é‡ç½®å¤±è´¥ï¼šå†å²è®°å½•ä¸è¶³ (len={len(history_without_current) if history_without_current else 0})")
                            if history_without_current:
                                for msg in history_without_current:
                                    messages.append({
                                        "role": msg["role"],
                                        "content": msg["content"]
                                    })
                            messages.append({"role": "user", "content": text})
                    else:
                        # ä¸åˆ°5è½®ï¼Œæ­£å¸¸æ·»åŠ å†å²ï¼ˆGemma æ¨¡å‹é™åˆ¶ä¸ºæœ€å¤š8æ¡ï¼Œå³4è½®å¯¹è¯ï¼‰
                        max_history = 8  # Gemma æ¨¡å‹æœ€å¤šä¿ç•™ 4 è½®å¯¹è¯ï¼ˆ8æ¡æ¶ˆæ¯ï¼‰
                        recent_history = conversation_history[-max_history:]
                        
                        # âœ… ç¡®ä¿ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯ userï¼ˆä¿æŒå¯¹è¯é…å¯¹å®Œæ•´æ€§ï¼‰
                        while recent_history and recent_history[0].get("role") != "user":
                            recent_history = recent_history[1:]
                            logger.warning(f"âš ï¸ Gemmaæ¨¡å‹(æœç´¢æ¨¡å¼)ï¼šè·³è¿‡å¼€å¤´çš„éuseræ¶ˆæ¯ï¼Œç¡®ä¿å¯¹è¯é…å¯¹å®Œæ•´")
                        
                        for msg in recent_history:
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        logger.info(f"ğŸ“ Gemmaæ¨¡å‹(æœç´¢æ¨¡å¼)æ­£å¸¸å¯¹è¯: ä½¿ç”¨æœ€è¿‘ {len(recent_history)} æ¡å†å²è®°å½•ï¼ˆæœ€å¤š4è½®ï¼‰")
                else:
                    # é Gemma æ¨¡å‹ï¼Œæ­£å¸¸å¤„ç†ï¼ˆåº”ç”¨ max_history é™åˆ¶ï¼‰
                    max_history = self.config.get("max_history", 10)
                    recent_history = conversation_history[-max_history:]
                    for msg in recent_history:
                        messages.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })
            else:
                # æ²¡æœ‰å†å²è®°å½•ï¼Œç›´æ¥æ·»åŠ æ–°æ¶ˆæ¯
                messages.append({"role": "user", "content": text})
            
            # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æœ€ç»ˆå‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæœç´¢å‰ï¼‰
            logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€ç»™ LLM çš„æ¶ˆæ¯æ•°é‡ï¼ˆæœç´¢å‰ï¼‰: {len(messages)}")
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content_preview = msg.get('content', '')[:50]
                logger.debug(f"  æ¶ˆæ¯ {i+1}: role={role}, content={content_preview}...")
            
            # å¦‚æœå¯ç”¨æœç´¢
            citations_text = ""  # ç”¨äºå­˜å‚¨å¼•ç”¨ä¿¡æ¯
            
            if use_search:
                # é«˜çº§æœç´¢æ¨¡å¼ (Momo Search)
                if search_mode == "advanced" and momo_search_handler:
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šAgentæ¨¡å¼
                    use_multi_agent = getattr(momo_search_handler, 'use_multi_agent', False) if momo_search_handler else False
                    if use_multi_agent:
                        logger.info(f"ğŸ¤– [å¤šAgentæ¨¡å¼] æ‰§è¡ŒMomoé«˜çº§æœç´¢: {text} (è´¨é‡: {momo_search_quality})")
                    else:
                        logger.info(f"âš™ï¸ [ç®¡é“æ¨¡å¼] æ‰§è¡ŒMomoé«˜çº§æœç´¢: {text} (è´¨é‡: {momo_search_quality})")
                    
                    # æ‰§è¡ŒMomoæœç´¢
                    from datetime import datetime
                    cur_date = datetime.today().strftime('%Y-%m-%d')
                    
                    relevant_docs, citations, thinking_results = await momo_search_handler.search_with_progress(
                        query=text,
                        mode=momo_search_quality,
                        progress_callback=progress_callback
                    )
                    
                    # å‘é€æœç´¢ç»“æœåˆ°å‰ç«¯ï¼ˆç”¨äºå¼¹çª—æ˜¾ç¤ºï¼‰
                    if relevant_docs and search_results_callback:
                        await search_results_callback(relevant_docs)
                    
                    if relevant_docs:
                        logger.info(f"\n{'*'*80}")
                        logger.info(f"ğŸ“š æ„å»ºMomoæœç´¢ä¸Šä¸‹æ–‡ (å…± {len(relevant_docs)} ä¸ªç»“æœ)")
                        logger.info(f"{'*'*80}\n")
                        
                        # ä½¿ç”¨æ€è€ƒé“¾æ„å»ºæ·±åº¦æ€è€ƒçš„ Prompt
                        from backend.handlers.llm.thinking_chain import build_enhanced_search_prompt
                        
                        # æ ¹æ®æœç´¢è´¨é‡æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨æ€è€ƒé“¾
                        # qualityï¼ˆæ·±åº¦ï¼‰æ¨¡å¼ï¼šä½¿ç”¨æ€è€ƒé“¾ï¼Œè¿›è¡Œæ·±åº¦æ€è€ƒ
                        # speedï¼ˆå¿«é€Ÿï¼‰æ¨¡å¼ï¼šä½¿ç”¨ç®€å•æ¨¡å¼ï¼Œå¿«é€Ÿå›ç­”
                        use_thinking_chain = (momo_search_quality == "quality")
                        
                        if use_thinking_chain:
                            logger.info(f"ğŸ§  [æ·±åº¦æ¨¡å¼] ä½¿ç”¨æ·±åº¦æ€è€ƒé“¾ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                            thinking_prompt = build_enhanced_search_prompt(
                                user_query=text,
                                search_results=relevant_docs,
                                current_date=cur_date,
                                use_thinking_chain=True,
                                thinking_results=thinking_results
                            )
                            context = thinking_prompt
                        else:
                            # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç®€å• Prompt
                            logger.info(f"âš¡ [å¿«é€Ÿæ¨¡å¼] ä½¿ç”¨ç®€å•æ¨¡å¼ç”Ÿæˆå›ç­” (è´¨é‡: {momo_search_quality})")
                            context = f"# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœï¼ˆä»Šå¤©æ˜¯{cur_date}ï¼‰:\n\n"
                        
                        for i, doc in enumerate(relevant_docs, 1):
                            context += f"[ç½‘é¡µ {i} å¼€å§‹]\n"
                            context += f"æ ‡é¢˜: {doc.title}\n"
                            context += f"é“¾æ¥: {doc.url}\n"
                            content_text = doc.content if doc.content else doc.snippet
                            context += f"å†…å®¹: {content_text}\n"
                            context += f"[ç½‘é¡µ {i} ç»“æŸ]\n\n"
                        
                        context += """åœ¨å›ç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ï¼ŒæŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡
- å¦‚æœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]
- å¹¶éæœç´¢ç»“æœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸ç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œç”„åˆ«ã€ç­›é€‰
- å¯¹äºåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼Œå°½é‡å°†ç­”æ¡ˆæ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…
- å¦‚æœå›ç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æ„åŒ–ã€åˆ†æ®µè½æ€»ç»“ï¼Œæ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…
- ä½ çš„å›ç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›ç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µ
- é™¤éç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›ç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´

# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š
{text}"""
                        
                        # ä¿å­˜å¼•ç”¨ä¿¡æ¯ï¼Œç¨åæ·»åŠ åˆ°å“åº”ä¸­
                        citations_text = citations
                        
                        # å°†æœç´¢ç»“æœæ’å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰
                        messages.insert(-1, {
                            'role': 'system',
                            'content': context
                        })
                        
                        logger.info(f"ğŸ“ æœç´¢ä¸Šä¸‹æ–‡å·²æ„å»º (é•¿åº¦: {len(context)}, æ€è€ƒé“¾: {use_thinking_chain})")
                        
                        # è¯¦ç»†è®°å½•
                        logger.info(f"ğŸ“ Momoæœç´¢ä¸Šä¸‹æ–‡å·²æ³¨å…¥ (é•¿åº¦: {len(context)}å­—ç¬¦)")
                        logger.info(f"ğŸ“Š ç›¸å…³æ–‡æ¡£æ•°: {len(relevant_docs)}")
                    else:
                        logger.warning(f"âš ï¸ Momoæœç´¢æœªè¿”å›ç»“æœ")
                
                # ç®€å•æœç´¢æ¨¡å¼ (WebSearchHandler)
                elif search_mode == "simple" and search_handler:
                    logger.info(f"ğŸ” æ‰§è¡Œç®€å•æœç´¢: {text}")
                    
                    # æ‰§è¡Œç®€å•æœç´¢
                    search_results = await search_handler.search_with_progress(
                        query=text,
                        max_results=3,
                        progress_callback=progress_callback
                    )
                    
                    if search_results:
                        logger.info(f"\n{'*'*80}")
                        logger.info(f"ğŸ“š æ„å»ºæœç´¢ä¸Šä¸‹æ–‡ (å…± {len(search_results)} ä¸ªç»“æœ)")
                        logger.info(f"{'*'*80}\n")
                        
                        # æ„å»ºæœç´¢ä¸Šä¸‹æ–‡
                        context = "æˆ‘ä¸ºä½ æœç´¢åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n"
                        for i, result in enumerate(search_results, 1):
                            context += f"{i}. **{result['title']}**\n"
                            context += f"   æ¥æº: {result['url']}\n"
                            if result.get('content'):
                                # æˆªå–éƒ¨åˆ†å†…å®¹
                                content_preview = result['content'][:300]
                                context += f"   å†…å®¹: {content_preview}...\n"
                            else:
                                context += f"   æ‘˜è¦: {result['snippet']}\n"
                            context += "\n"
                        
                        context += "è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                        
                        # å°†æœç´¢ç»“æœæ’å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰
                        messages.insert(-1, {
                            'role': 'system',
                            'content': context
                        })
                        
                        # è¯¦ç»†è®°å½•
                        logger.info(f"ğŸ“ ç®€å•æœç´¢ä¸Šä¸‹æ–‡å·²æ³¨å…¥")
                        logger.info(f"ğŸ“Š æœç´¢ç»“æœæ•°: {len(search_results)}")
                    else:
                        logger.warning(f"âš ï¸ æœç´¢æœªè¿”å›ä»»ä½•ç»“æœï¼Œå°†ä¸ä½¿ç”¨æœç´¢ä¸Šä¸‹æ–‡")
            
            # ç»§ç»­æ­£å¸¸çš„æµå¼å“åº”
            async for chunk in self._stream_response_internal(messages):
                yield chunk
            
            # å¦‚æœæœ‰å¼•ç”¨ä¿¡æ¯ï¼Œåœ¨å“åº”ç»“æŸåæ·»åŠ 
            if citations_text:
                yield f"\n\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n{citations_text}"
                
        except Exception as e:
            logger.error(f"Failed to stream response with search: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _stream_response_internal(self, messages: List[Dict]) -> AsyncGenerator[str, None]:
        """Internal method for streaming response"""
        
        # è®¡ç®—è¾“å…¥çš„ token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šä¸­æ–‡æŒ‰2å­—ç¬¦/tokenï¼Œè‹±æ–‡æŒ‰4å­—ç¬¦/tokenï¼‰
        total_chars = sum(len(msg.get('content', '')) for msg in messages)
        estimated_tokens = int(total_chars * 0.6)  # å¹³å‡ä¼°ç®—
        
        logger.info(f"{'='*60}")
        logger.info(f"Starting LLM stream request")
        logger.info(f"  Model: {self.model}")
        logger.info(f"  Base URL: {self.api_url}")
        logger.info(f"  Messages count: {len(messages)}")
        logger.info(f"  Total characters: {total_chars}")
        logger.info(f"  Estimated input tokens: ~{estimated_tokens}")
        logger.info(f"  Temperature: {self.temperature}")
        logger.info(f"  Max tokens: {self.max_tokens}")
        
        # è®°å½•æ¯æ¡æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
        for i, msg in enumerate(messages):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            content_preview = content[:100] + ('...' if len(content) > 100 else '')
            logger.info(f"  Message {i+1} [{role}]: {len(content)} chars - {content_preview}")
        
        logger.info(f"{'='*60}")
        
        # å¦‚æœè¾“å…¥è¿‡é•¿ï¼Œè®°å½•å®Œæ•´çš„ messagesï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if total_chars > 500:
            logger.warning(f"âš ï¸  Large input detected ({total_chars} chars), logging full messages for debugging:")
            import json
            for i, msg in enumerate(messages):
                logger.debug(f"Message {i+1} full content:\n{json.dumps(msg, ensure_ascii=False, indent=2)}")
        
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True,
                timeout=30.0  # æ·»åŠ è¶…æ—¶é˜²æ­¢æŒ‚èµ·
            )
            logger.info(f"âœ… Stream object created successfully: {type(stream)}")
        except Exception as e:
            logger.error(f"âŒ Failed to create stream: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

        chunk_count = 0
        content_chunks = 0
        total_content_length = 0
        logger.info("ğŸ”„ Entering async for loop to receive chunks...")
        
        try:
            async for chunk in stream:
                chunk_count += 1
                
                # è®°å½•ç¬¬ä¸€ä¸ª chunk çš„å®Œæ•´å†…å®¹ä»¥ä¾¿è°ƒè¯•
                if chunk_count == 1:
                    logger.info(f"ğŸ“¦ First chunk received")
                    logger.info(f"   Type: {type(chunk)}")
                    logger.info(f"   Has choices: {hasattr(chunk, 'choices')}")
                    if hasattr(chunk, 'choices'):
                        logger.info(f"   Choices count: {len(chunk.choices) if chunk.choices else 0}")
                    logger.info(f"   Raw chunk: {chunk}")
                
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ finish_reason
                    if hasattr(choice, 'finish_reason') and choice.finish_reason:
                        logger.info(f"ğŸ Stream finished with reason: {choice.finish_reason}")
                    
                    delta = choice.delta
                    if delta and delta.content:
                        content_chunks += 1
                        content_length = len(delta.content)
                        total_content_length += content_length
                        
                        if content_chunks == 1:
                            logger.info(f"âœ¨ First content chunk received (chunk #{chunk_count})")
                            logger.info(f"   Content preview: {delta.content[:50]}")
                        elif content_chunks % 10 == 0:  # æ¯10ä¸ªå†…å®¹chunkè®°å½•ä¸€æ¬¡
                            logger.info(f"ğŸ“ Received {content_chunks} content chunks, total {total_content_length} chars")
                        
                        yield delta.content
                else:
                    if chunk_count <= 3:  # åªè®°å½•å‰3ä¸ªç©º chunk
                        logger.warning(f"âš ï¸  Chunk #{chunk_count} has no choices or empty content")
                        logger.warning(f"   Chunk structure: {chunk}")
        except Exception as e:
            logger.error(f"Error during stream iteration: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
        
        logger.info(f"{'='*60}")
        logger.info(f"LLM stream completed")
        logger.info(f"  Total chunks received: {chunk_count}")
        logger.info(f"  Content chunks: {content_chunks}")
        logger.info(f"  Total content length: {total_content_length} chars")
        logger.info(f"  Estimated output tokens: ~{int(total_content_length * 0.6)}")

        if content_chunks == 0:
            logger.error(f"âŒ Stream returned 0 content chunks!")
            logger.error(f"   Total chunks received: {chunk_count}")
            logger.error(f"   API endpoint: {self.client.base_url}")
            logger.error(f"   This suggests:")
            logger.error(f"   1. LLM API returned empty response")
            logger.error(f"   2. API may have rate limits or errors")
            logger.error(f"   3. Input may have triggered content filter")
            logger.error(f"   4. Model may not exist or is unavailable")
        else:
            logger.info(f"âœ… Stream successful: {content_chunks} chunks, {total_content_length} chars")
        
        logger.info(f"{'='*60}")
    
    async def stream_response(self, text: str, conversation_history: List[Dict] = None) -> AsyncGenerator[str, None]:
        """Generate streaming response (without search)"""
        try:
            # Prepare messages
            messages = [{"role": "system", "content": self.system_prompt}]
            
            if conversation_history:
                # Gemma æ¨¡å‹ç‰¹æ®Šå¤„ç†ï¼šæ¯5è½®å¯¹è¯é‡ç½®ä¸€æ¬¡ä¸Šä¸‹æ–‡
                # æ³¨æ„ï¼šå¿…é¡»å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®ï¼ˆåŸºäºå®Œæ•´å†å²ï¼‰ï¼Œå†åº”ç”¨ max_history æˆªæ–­
                if 'gemma' in self.model.lower():
                    # conversation_history ä¸­æœ€åä¸€æ¡æ˜¯å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆå·²ç»åœ¨ session_manager ä¸­æ·»åŠ ï¼‰
                    # æ‰€ä»¥è¦æ’é™¤å®ƒæ¥è®¡ç®—ä¹‹å‰çš„å¯¹è¯è½®æ•°
                    history_without_current = conversation_history[:-1] if conversation_history and conversation_history[-1].get("role") == "user" else conversation_history
                    user_messages = [m for m in history_without_current if m["role"] == "user"] if history_without_current else []
                    
                    logger.info(f"ğŸ“Š Gemmaæ¨¡å‹å¯¹è¯ç»Ÿè®¡: å®Œæ•´å†å²={len(conversation_history) if conversation_history else 0}æ¡, ç”¨æˆ·æ¶ˆæ¯={len(user_messages)}æ¡")
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡ç½®ï¼ˆç¬¬5è½®ä¹‹åï¼Œå³ç¬¬6ã€11ã€16è½®...ï¼‰
                    if len(user_messages) >= 5 and len(user_messages) % 5 == 0:
                        logger.info(f"ğŸ”„ Gemmaæ¨¡å‹æ£€æµ‹åˆ°ç¬¬ {len(user_messages)+1} è½®å¯¹è¯ï¼Œæ‰§è¡Œä¸Šä¸‹æ–‡é‡ç½®")
                        
                        # ä» history_without_current ä¸­å–æœ€å2æ¡æ¶ˆæ¯ï¼ˆä¸Šä¸€è½®å¯¹è¯ï¼‰
                        if history_without_current and len(history_without_current) >= 2:
                            last_user_msg = history_without_current[-2]  # ä¸Šä¸€è½®çš„useræ¶ˆæ¯
                            last_assistant_msg = history_without_current[-1]  # ä¸Šä¸€è½®çš„assistantå›å¤
                            
                            # éªŒè¯æ ¼å¼æ­£ç¡®
                            if last_user_msg["role"] == "user" and last_assistant_msg["role"] == "assistant":
                                # åˆå¹¶ä¸Šä¸€è½®å¯¹è¯å’Œæ–°æ¶ˆæ¯ä½œä¸ºæ–°çš„ç¬¬ä¸€è½®
                                merged_content = f"ä¹‹å‰çš„å¯¹è¯ï¼š\nç”¨æˆ·: {last_user_msg['content']}\nåŠ©æ‰‹: {last_assistant_msg['content']}\n\nå½“å‰é—®é¢˜: {text}"
                                messages.append({"role": "user", "content": merged_content})
                                logger.info(f"âœ… é‡ç½®ä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™ä¸Šä¸€è½®(user:{len(last_user_msg['content'])}å­— + assistant:{len(last_assistant_msg['content'])}å­—) + æ–°æ¶ˆæ¯({len(text) if text else 0}å­—)")
                            else:
                                # æ ¼å¼ä¸å¯¹ï¼Œè¯´æ˜å†å²è®°å½•å¼‚å¸¸ï¼ŒæŒ‰æ­£å¸¸æµç¨‹å¤„ç†ï¼ˆä½†æ’é™¤å½“å‰è¾“å…¥ï¼Œå› ä¸ºåé¢ä¼šç»Ÿä¸€æ·»åŠ ï¼‰
                                logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡é‡ç½®å¤±è´¥ï¼šå†å²è®°å½•æ ¼å¼ä¸æ­£ç¡® (last_user_msg role={last_user_msg.get('role')}, last_assistant_msg role={last_assistant_msg.get('role')})")
                                for msg in history_without_current:
                                    messages.append({
                                        "role": msg["role"],
                                        "content": msg["content"]
                                    })
                                # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
                                messages.append({"role": "user", "content": text})
                        else:
                            # å†å²è®°å½•ä¸è¶³ï¼ŒæŒ‰æ­£å¸¸æµç¨‹å¤„ç†
                            logger.warning(f"âš ï¸ ä¸Šä¸‹æ–‡é‡ç½®å¤±è´¥ï¼šå†å²è®°å½•ä¸è¶³ (len={len(history_without_current) if history_without_current else 0})")
                            if history_without_current:
                                for msg in history_without_current:
                                    messages.append({
                                        "role": msg["role"],
                                        "content": msg["content"]
                                    })
                            messages.append({"role": "user", "content": text})
                    else:
                        # ä¸åˆ°5è½®ï¼Œæ­£å¸¸æ·»åŠ å†å²ï¼ˆGemma æ¨¡å‹é™åˆ¶ä¸ºæœ€å¤š8æ¡ï¼Œå³4è½®å¯¹è¯ï¼‰
                        max_history = 8  # Gemma æ¨¡å‹æœ€å¤šä¿ç•™ 4 è½®å¯¹è¯ï¼ˆ8æ¡æ¶ˆæ¯ï¼‰
                        recent_history = conversation_history[-max_history:]
                        
                        # âœ… ç¡®ä¿ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯ userï¼ˆä¿æŒå¯¹è¯é…å¯¹å®Œæ•´æ€§ï¼‰
                        while recent_history and recent_history[0].get("role") != "user":
                            recent_history = recent_history[1:]
                            logger.warning(f"âš ï¸ Gemmaæ¨¡å‹ï¼šè·³è¿‡å¼€å¤´çš„éuseræ¶ˆæ¯ï¼Œç¡®ä¿å¯¹è¯é…å¯¹å®Œæ•´")
                        
                        for msg in recent_history:
                            messages.append({
                                "role": msg["role"],
                                "content": msg["content"]
                            })
                        logger.info(f"ğŸ“ Gemmaæ¨¡å‹æ­£å¸¸å¯¹è¯: ä½¿ç”¨æœ€è¿‘ {len(recent_history)} æ¡å†å²è®°å½•ï¼ˆæœ€å¤š4è½®ï¼‰")
                else:
                    # é Gemma æ¨¡å‹ï¼Œæ­£å¸¸å¤„ç†ï¼ˆåº”ç”¨ max_history é™åˆ¶ï¼‰
                    max_history = self.config.get("max_history", 10)
                    recent_history = conversation_history[-max_history:]
                    for msg in recent_history:
                        messages.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })
            else:
                # æ²¡æœ‰å†å²è®°å½•ï¼Œç›´æ¥æ·»åŠ æ–°æ¶ˆæ¯
                messages.append({"role": "user", "content": text})
            
            # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æœ€ç»ˆå‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨
            logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€ç»™ LLM çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content_preview = msg.get('content', '')[:50]
                logger.debug(f"  æ¶ˆæ¯ {i+1}: role={role}, content={content_preview}...")
            
            # Stream response
            async for chunk in self._stream_response_internal(messages):
                yield chunk
                    
        except Exception as e:
            logger.error(f"Failed to stream response: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def update_config(self, config: Dict):
        """Update configuration"""
        super().update_config(config)
        
        # Update generation parameters
        self.temperature = self.config.get("temperature", self.temperature)
        self.max_tokens = self.config.get("max_tokens", self.max_tokens)
        self.top_p = self.config.get("top_p", self.top_p)
        self.system_prompt = self.config.get("system_prompt", self.system_prompt)
        
        # Update API settings if provided
        if "api_url" in config:
            self.api_url = config["api_url"]
            self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_url)
        
        if "api_key" in config:
            self.api_key = config["api_key"]
            self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_url)
        
        if "model" in config:
            self.model = config["model"]
