"""
æ·±åº¦æ€è€ƒé“¾ï¼ˆThinking Chainï¼‰- ä½¿ç”¨ LangChain å®ç°å¤šæ­¥éª¤æ¨ç†å’Œæ·±åº¦åˆ†æ
è§£å†³"å›ç­”ç¼ºä¹æ·±åº¦æ€è€ƒï¼Œåªæ˜¯æœç´¢ç»“æœæ‹¼æ¥"çš„é—®é¢˜
"""
from typing import List, Dict, Optional, Any
from loguru import logger

from backend.handlers.search.momo_utils import SearchDocument


class ThinkingChain:
    """
    æ€è€ƒé“¾ï¼šé€šè¿‡å¤šæ­¥éª¤æ¨ç†ç”Ÿæˆæ·±åº¦æ€è€ƒçš„å›ç­”
    
    æµç¨‹ï¼š
    1. ç†è§£é—®é¢˜ï¼šåˆ†æç”¨æˆ·æ„å›¾å’Œéœ€æ±‚
    2. åˆ†æèµ„æ–™ï¼šæ‰¹åˆ¤æ€§åˆ†ææœç´¢ç»“æœçš„å¯é æ€§å’Œç›¸å…³æ€§
    3. æ·±åº¦æ€è€ƒï¼šè¿›è¡Œæ¨ç†ã€å¯¹æ¯”ã€æ€»ç»“
    4. ç”Ÿæˆå›ç­”ï¼šç»¼åˆæ€è€ƒç»“æœç”Ÿæˆé«˜è´¨é‡å›ç­”
    5. è‡ªæˆ‘å®¡æŸ¥ï¼šæ£€æŸ¥å›ç­”çš„é€»è¾‘æ€§å’Œå®Œæ•´æ€§
    """
    
    def __init__(self):
        self.enabled = True
    
    def build_thinking_prompt(
        self,
        user_query: str,
        search_results: List[SearchDocument],
        current_date: str
    ) -> str:
        """
        æ„å»ºè¦æ±‚æ·±åº¦æ€è€ƒçš„ Prompt
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            search_results: æœç´¢ç»“æœ
            current_date: å½“å‰æ—¥æœŸ
            
        Returns:
            å®Œæ•´çš„æ€è€ƒé“¾ Prompt
        """
        # æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢çš„è¯­è¨€
        def detect_language(text: str) -> str:
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
            return "en" if english_chars > chinese_chars and english_chars > 0 else "zh"
        
        detected_lang = detect_language(user_query)
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€æ·»åŠ å¼ºåˆ¶æŒ‡ä»¤
        if detected_lang == "en":
            lang_instruction = """
ğŸ”´ğŸ”´ğŸ”´ CRITICAL MANDATORY INSTRUCTION ğŸ”´ğŸ”´ğŸ”´
The user's question is in ENGLISH. You MUST write your ENTIRE response in ENGLISH.
- All thinking steps: ENGLISH
- Final answer: ENGLISH
- Do NOT use any Chinese characters in your response.
This is a MANDATORY requirement that overrides all other instructions.
"""
        else:
            lang_instruction = """
ğŸ”´ğŸ”´ğŸ”´ é‡è¦å¼ºåˆ¶æŒ‡ä»¤ ğŸ”´ğŸ”´ğŸ”´
ç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ã€‚ä½ å¿…é¡»ç”¨ä¸­æ–‡æ’°å†™æ•´ä¸ªå›ç­”ã€‚
- æ‰€æœ‰æ€è€ƒæ­¥éª¤ï¼šä¸­æ–‡
- æœ€ç»ˆç­”æ¡ˆï¼šä¸­æ–‡
- ä¸è¦åœ¨å›ç­”ä¸­ä½¿ç”¨ä»»ä½•è‹±æ–‡å­—ç¬¦ã€‚
è¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¼˜å…ˆçº§é«˜äºæ‰€æœ‰å…¶ä»–æŒ‡ä»¤ã€‚
"""
        
        # æ„å»ºæœç´¢ç»“æœä¸Šä¸‹æ–‡
        search_context = self._build_search_context(search_results)
        
        # æ„å»ºæ€è€ƒé“¾ Prompt
        thinking_prompt = f"""{lang_instruction}

# è§’è‰²å®šä½
ä½ æ˜¯ä¸€ä½å…·æœ‰æ·±åº¦æ€è€ƒèƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡ä¸æ˜¯ç®€å•åœ°æ€»ç»“æœç´¢ç»“æœï¼Œè€Œæ˜¯è¦è¿›è¡Œæ·±åº¦çš„åˆ†æã€æ¨ç†å’Œæ€è€ƒï¼Œä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚

# å½“å‰æ—¥æœŸ
ä»Šå¤©æ˜¯ {current_date}

# æœç´¢ç»“æœ
{search_context}

# æ€è€ƒæµç¨‹ï¼ˆè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ€è€ƒï¼‰

## ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜ï¼ˆProblem Understandingï¼‰
è¯·å…ˆæ·±å…¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼š
- ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ
- é—®é¢˜çš„èƒŒæ™¯å’Œä¸Šä¸‹æ–‡æ˜¯ä»€ä¹ˆï¼Ÿ
- ç”¨æˆ·å¯èƒ½æƒ³è¦ä»€ä¹ˆæ ·çš„å›ç­”ï¼Ÿï¼ˆä¿¡æ¯ã€åˆ†æã€å»ºè®®ã€å¯¹æ¯”ç­‰ï¼‰
- è¿™ä¸ªé—®é¢˜æ¶‰åŠå“ªäº›å…³é”®æ¦‚å¿µå’Œé¢†åŸŸï¼Ÿ

**è¯·å†™å‡ºä½ çš„ç†è§£ï¼š**

## ç¬¬äºŒæ­¥ï¼šæ‰¹åˆ¤æ€§åˆ†æèµ„æ–™ï¼ˆCritical Analysisï¼‰
å¯¹æœç´¢ç»“æœè¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æï¼š
- å“ªäº›èµ„æ–™æœ€ç›¸å…³ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
- ä¸åŒèµ„æ–™ä¹‹é—´æœ‰ä»€ä¹ˆä¸€è‡´æ€§å’Œå·®å¼‚ï¼Ÿ
- èµ„æ–™çš„å¯é æ€§å’Œæƒå¨æ€§å¦‚ä½•ï¼Ÿ
- å“ªäº›ä¿¡æ¯å¯èƒ½è¿‡æ—¶æˆ–ä¸å‡†ç¡®ï¼Ÿ
- æ˜¯å¦å­˜åœ¨è§‚ç‚¹å†²çªï¼Ÿå¦‚ä½•ç†è§£è¿™äº›å†²çªï¼Ÿ

**è¯·å†™å‡ºä½ çš„åˆ†æï¼š**

## ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦æ€è€ƒä¸æ¨ç†ï¼ˆDeep Thinkingï¼‰
åŸºäºèµ„æ–™è¿›è¡Œæ·±åº¦æ€è€ƒï¼š
- è¿™äº›ä¿¡æ¯èƒŒååæ˜ äº†ä»€ä¹ˆè¶‹åŠ¿æˆ–è§„å¾‹ï¼Ÿ
- ä¸åŒè§‚ç‚¹æˆ–æ–¹æ¡ˆçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
- å¯ä»¥ä»å“ªäº›è§’åº¦æ¥åˆ†æè¿™ä¸ªé—®é¢˜ï¼Ÿ
- æœ‰ä»€ä¹ˆè¢«å¿½è§†çš„é‡è¦æ–¹é¢ï¼Ÿ
- å¦‚ä½•å°†è¿™äº›ä¿¡æ¯è”ç³»èµ·æ¥ï¼Œå½¢æˆæ›´æ·±å…¥çš„è§è§£ï¼Ÿ

**è¯·å†™å‡ºä½ çš„æ€è€ƒï¼š**

## ç¬¬å››æ­¥ï¼šç»¼åˆæ¨ç†ä¸ç»“è®ºï¼ˆSynthesisï¼‰
ç»¼åˆå‰é¢çš„åˆ†æï¼Œå½¢æˆè‡ªå·±çš„è§è§£ï¼š
- å¦‚ä½•æ•´åˆä¸åŒæ¥æºçš„ä¿¡æ¯ï¼Ÿ
- å¯ä»¥å¾—å‡ºä»€ä¹ˆæœ‰ä»·å€¼çš„ç»“è®ºï¼Ÿ
- æœ‰å“ªäº›é‡è¦çš„æ´å¯Ÿæˆ–å»ºè®®ï¼Ÿ
- æ˜¯å¦éœ€è¦å¯¹å¤šä¸ªè§’åº¦è¿›è¡Œå¯¹æ¯”åˆ†æï¼Ÿ

**è¯·å†™å‡ºä½ çš„ç»¼åˆç»“è®ºï¼š**

## ç¬¬äº”æ­¥ï¼šç”Ÿæˆé«˜è´¨é‡å›ç­”ï¼ˆResponse Generationï¼‰
åŸºäºä»¥ä¸Šæ€è€ƒï¼Œç”Ÿæˆå›ç­”ã€‚è¦æ±‚ï¼š
- **ä¸è¦ç®€å•ç½—åˆ—æœç´¢ç»“æœ**ï¼Œè€Œæ˜¯è¦åŸºäºæ€è€ƒå½¢æˆè‡ªå·±çš„è§‚ç‚¹
- **è¿›è¡Œå¤šè§’åº¦åˆ†æ**ï¼Œä¸åªæ˜¯å•ä¸€è§†è§’
- **æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿ**ï¼Œè€Œä¸ä»…ä»…æ˜¯äº‹å®é™ˆè¿°
- **é€»è¾‘æ¸…æ™°**ï¼Œç»“æ„åˆç†
- **å¼•ç”¨èµ„æ–™**ï¼šåœ¨é€‚å½“ä½ç½®ä½¿ç”¨ [citation:X] æ ¼å¼å¼•ç”¨æ¥æº
- **è¯­è¨€è‡ªç„¶**ï¼šå›ç­”åº”è¯¥æµç•…ã€ä¸“ä¸šï¼Œåƒä¸“å®¶åœ¨åˆ†äº«è§è§£
- **è¯­è¨€åŒ¹é…**ï¼š**å¿…é¡»ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜ç›¸åŒçš„è¯­è¨€å›ç­”**ã€‚å¦‚æœç”¨æˆ·ç”¨è‹±è¯­æé—®ï¼Œå¿…é¡»ç”¨è‹±è¯­å›ç­”ï¼›å¦‚æœç”¨æˆ·ç”¨ä¸­æ–‡æé—®ï¼Œå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ã€‚è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼

**å›ç­”æ ¼å¼è¦æ±‚ï¼š**
- å¦‚æœé—®é¢˜éœ€è¦å¯¹æ¯”åˆ†æï¼Œè¯·æä¾›æ¸…æ™°çš„å¯¹æ¯”æ¡†æ¶
- å¦‚æœé—®é¢˜éœ€è¦å»ºè®®ï¼Œè¯·æä¾›æœ‰ä¾æ®çš„å»ºè®®
- å¦‚æœé—®é¢˜éœ€è¦è§£é‡Šï¼Œè¯·æä¾›æ·±å…¥çš„è§£é‡Šå’ŒèƒŒæ™¯
- å§‹ç»ˆè®°ä½ï¼šä½ æ˜¯åœ¨åˆ†äº«**ç»è¿‡æ€è€ƒçš„è§è§£**ï¼Œè€Œä¸æ˜¯åœ¨**è½¬è¿°æœç´¢ç»“æœ**
- **é‡è¦ï¼šå›ç­”è¯­è¨€å¿…é¡»ä¸ç”¨æˆ·é—®é¢˜è¯­è¨€å®Œå…¨ä¸€è‡´**

# ç”¨æˆ·é—®é¢˜
{user_query}

---

**ç°åœ¨ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°äº”ä¸ªæ­¥éª¤è¿›è¡Œæ€è€ƒï¼Œç„¶åç”Ÿæˆé«˜è´¨é‡çš„å›ç­”ã€‚**"""
        
        return thinking_prompt
    
    def _build_search_context(self, search_results: List[SearchDocument]) -> str:
        """æ„å»ºæœç´¢ç»“æœä¸Šä¸‹æ–‡"""
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚"
        
        context_parts = []
        context_parts.append(f"å…±æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æœç´¢ç»“æœï¼š\n")
        
        for idx, doc in enumerate(search_results[:20], 1):  # é™åˆ¶å‰20ä¸ªç»“æœ
            context_parts.append(f"[å‚è€ƒèµ„æ–™ {idx}]")
            context_parts.append(f"æ ‡é¢˜: {doc.title if hasattr(doc, 'title') else 'N/A'}")
            
            # æ·»åŠ æ¥æºåŸŸå
            if hasattr(doc, 'url') and doc.url:
                from urllib.parse import urlparse
                try:
                    domain = urlparse(doc.url).netloc
                    context_parts.append(f"æ¥æº: {domain}")
                except:
                    context_parts.append(f"æ¥æº: ç½‘ç»œèµ„æ–™")
            
            # æ·»åŠ å†…å®¹
            content = doc.content if hasattr(doc, 'content') and doc.content else ''
            if not content and hasattr(doc, 'snippet'):
                content = doc.snippet
            
            if content:
                # é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹é•¿åº¦
                content = content[:1500] if len(content) > 1500 else content
                context_parts.append(f"å†…å®¹:\n{content}")
            
            context_parts.append("---\n")
        
        return "\n".join(context_parts)
    
    def build_synthesis_prompt(
        self,
        user_query: str,
        search_results: List[SearchDocument],
        current_date: str,
        thinking_results: dict
    ) -> str:
        """
        æ„å»ºç»¼åˆä¿¡æ¯ Promptï¼ˆä½¿ç”¨å‰é¢çš„æ€è€ƒç»“æœï¼‰
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            search_results: æœç´¢ç»“æœ
            current_date: å½“å‰æ—¥æœŸ
            thinking_results: æ€è€ƒç»“æœå­—å…¸ï¼ˆåŒ…å« understanding, analysis, thinkingï¼‰
            
        Returns:
            ç»¼åˆä¿¡æ¯ Prompt
        """
        # æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢çš„è¯­è¨€
        def detect_language(text: str) -> str:
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
            return "en" if english_chars > chinese_chars and english_chars > 0 else "zh"
        
        detected_lang = detect_language(user_query)
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€æ·»åŠ å¼ºåˆ¶æŒ‡ä»¤
        if detected_lang == "en":
            lang_instruction = """
ğŸ”´ğŸ”´ğŸ”´ CRITICAL MANDATORY INSTRUCTION ğŸ”´ğŸ”´ğŸ”´
The user's question is in ENGLISH. You MUST write your ENTIRE response in ENGLISH.
Do NOT use any Chinese characters in your response.
This is a MANDATORY requirement that overrides all other instructions.
"""
        else:
            lang_instruction = """
ğŸ”´ğŸ”´ğŸ”´ é‡è¦å¼ºåˆ¶æŒ‡ä»¤ ğŸ”´ğŸ”´ğŸ”´
ç”¨æˆ·çš„é—®é¢˜æ˜¯ä¸­æ–‡ã€‚ä½ å¿…é¡»ç”¨ä¸­æ–‡æ’°å†™æ•´ä¸ªå›ç­”ã€‚
ä¸è¦åœ¨å›ç­”ä¸­ä½¿ç”¨ä»»ä½•è‹±æ–‡å­—ç¬¦ã€‚
è¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¼˜å…ˆçº§é«˜äºæ‰€æœ‰å…¶ä»–æŒ‡ä»¤ã€‚
"""
        
        search_context = self._build_search_context(search_results)
        
        understanding = thinking_results.get("understanding", "")
        analysis = thinking_results.get("analysis", "")
        thinking = thinking_results.get("thinking", "")
        
        synthesis_prompt = f"""{lang_instruction}

# è§’è‰²å®šä½
ä½ æ˜¯ä¸€ä½å…·æœ‰æ·±åº¦æ€è€ƒèƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºå‰é¢çš„æ€è€ƒå’Œåˆ†æï¼Œç»¼åˆä¿¡æ¯å¹¶ç”Ÿæˆé«˜è´¨é‡çš„å›ç­”ã€‚

# å½“å‰æ—¥æœŸ
ä»Šå¤©æ˜¯ {current_date}

# å‰é¢çš„æ€è€ƒè¿‡ç¨‹

## é—®é¢˜ç†è§£
{understanding if understanding else "ï¼ˆæœªæä¾›ï¼‰"}

## èµ„æ–™åˆ†æ
{analysis if analysis else "ï¼ˆæœªæä¾›ï¼‰"}

## æ·±åº¦æ€è€ƒ
{thinking if thinking else "ï¼ˆæœªæä¾›ï¼‰"}

# æœç´¢ç»“æœ
{search_context}

# ä»»åŠ¡ï¼šç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡å›ç­”

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆå›ç­”ã€‚è¦æ±‚ï¼š

1. **ç»¼åˆå‰é¢çš„æ€è€ƒ**ï¼šæ•´åˆé—®é¢˜ç†è§£ã€èµ„æ–™åˆ†æå’Œæ·±åº¦æ€è€ƒçš„ç»“æœ
2. **æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿ**ï¼šä¸ä»…ä»…æ˜¯äº‹å®é™ˆè¿°ï¼Œè¦æä¾›ç»è¿‡æ€è€ƒçš„è§è§£
3. **é€»è¾‘æ¸…æ™°**ï¼šç»“æ„åˆç†ï¼Œæ¡ç†åˆ†æ˜
4. **å¼•ç”¨èµ„æ–™**ï¼šåœ¨é€‚å½“ä½ç½®ä½¿ç”¨ [citation:X] æ ¼å¼å¼•ç”¨æ¥æº
5. **è¯­è¨€è‡ªç„¶**ï¼šå›ç­”åº”è¯¥æµç•…ã€ä¸“ä¸šï¼Œåƒä¸“å®¶åœ¨åˆ†äº«è§è§£

**å›ç­”æ ¼å¼è¦æ±‚ï¼š**
- å¦‚æœé—®é¢˜éœ€è¦å¯¹æ¯”åˆ†æï¼Œè¯·æä¾›æ¸…æ™°çš„å¯¹æ¯”æ¡†æ¶
- å¦‚æœé—®é¢˜éœ€è¦å»ºè®®ï¼Œè¯·æä¾›æœ‰ä¾æ®çš„å»ºè®®
- å¦‚æœé—®é¢˜éœ€è¦è§£é‡Šï¼Œè¯·æä¾›æ·±å…¥çš„è§£é‡Šå’ŒèƒŒæ™¯
- å§‹ç»ˆè®°ä½ï¼šä½ æ˜¯åœ¨åˆ†äº«**ç»è¿‡æ·±åº¦æ€è€ƒçš„è§è§£**ï¼Œè€Œä¸æ˜¯åœ¨**è½¬è¿°æœç´¢ç»“æœ**

# ç”¨æˆ·é—®é¢˜
{user_query}

---

**ç°åœ¨ï¼Œè¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å›ç­”ã€‚**"""
        
        return synthesis_prompt
    
    def build_reflection_prompt(
        self,
        original_query: str,
        generated_response: str,
        search_results: List[SearchDocument]
    ) -> str:
        """
        æ„å»ºè‡ªæˆ‘å®¡æŸ¥ Promptï¼ˆå¯é€‰ï¼Œç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–å›ç­”ï¼‰
        
        Args:
            original_query: åŸå§‹æŸ¥è¯¢
            generated_response: ç”Ÿæˆçš„å›ç­”
            search_results: æœç´¢ç»“æœ
            
        Returns:
            åæ€ Prompt
        """
        reflection_prompt = f"""# è‡ªæˆ‘å®¡æŸ¥ä»»åŠ¡
è¯·å¯¹ä»¥ä¸‹å›ç­”è¿›è¡Œè‡ªæˆ‘å®¡æŸ¥ï¼Œç¡®ä¿è´¨é‡ï¼š

## åŸå§‹é—®é¢˜
{original_query}

## ç”Ÿæˆçš„å›ç­”
{generated_response}

## å®¡æŸ¥è¦ç‚¹
1. **é€»è¾‘æ€§**ï¼šå›ç­”çš„é€»è¾‘æ˜¯å¦æ¸…æ™°ï¼Ÿæ¨ç†æ˜¯å¦åˆç†ï¼Ÿ
2. **å®Œæ•´æ€§**ï¼šæ˜¯å¦å……åˆ†å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
3. **æ·±åº¦**ï¼šæ˜¯å¦æœ‰è¶³å¤Ÿçš„æ·±åº¦æ€è€ƒï¼Œè¿˜æ˜¯åªæ˜¯è¡¨é¢ä¿¡æ¯çš„æ‹¼æ¥ï¼Ÿ
4. **å‡†ç¡®æ€§**ï¼šå¼•ç”¨æ˜¯å¦æ­£ç¡®ï¼Ÿä¿¡æ¯æ˜¯å¦å‡†ç¡®ï¼Ÿ
5. **ä»·å€¼**ï¼šæ˜¯å¦æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œè€Œä¸ä»…ä»…æ˜¯äº‹å®é™ˆè¿°ï¼Ÿ

## æ”¹è¿›å»ºè®®
å¦‚æœå‘ç°ä»»ä½•é—®é¢˜ï¼Œè¯·æä¾›æ”¹è¿›åçš„å›ç­”ã€‚å¦‚æœå›ç­”å·²ç»å¾ˆå¥½ï¼Œè¯·è¯´æ˜ä¸ºä»€ä¹ˆã€‚"""
        
        return reflection_prompt
    
    def extract_thinking_steps(self, response: str) -> Dict[str, str]:
        """
        ä» LLM å›ç­”ä¸­æå–æ€è€ƒæ­¥éª¤ï¼ˆç”¨äºè°ƒè¯•å’Œä¼˜åŒ–ï¼‰
        
        Args:
            response: LLM çš„å®Œæ•´å›ç­”
            
        Returns:
            åŒ…å«å„æ­¥éª¤çš„å­—å…¸
        """
        steps = {
            "understanding": "",
            "analysis": "",
            "thinking": "",
            "synthesis": "",
            "final_response": ""
        }
        
        # å°è¯•æå–å„ä¸ªæ­¥éª¤ï¼ˆå¦‚æœ LLM æŒ‰ç…§æ ¼å¼å›ç­”ï¼‰
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™… LLM è¾“å‡ºæ ¼å¼è¿›è¡Œè°ƒæ•´
        sections = response.split("## ")
        for section in sections:
            if "ç¬¬ä¸€æ­¥" in section or "ç†è§£é—®é¢˜" in section:
                steps["understanding"] = section
            elif "ç¬¬äºŒæ­¥" in section or "æ‰¹åˆ¤æ€§åˆ†æ" in section:
                steps["analysis"] = section
            elif "ç¬¬ä¸‰æ­¥" in section or "æ·±åº¦æ€è€ƒ" in section:
                steps["thinking"] = section
            elif "ç¬¬å››æ­¥" in section or "ç»¼åˆæ¨ç†" in section:
                steps["synthesis"] = section
            elif "ç¬¬äº”æ­¥" in section or "ç”Ÿæˆé«˜è´¨é‡å›ç­”" in section:
                steps["final_response"] = section
        
        return steps


def build_enhanced_search_prompt(
    user_query: str,
    search_results: List[SearchDocument],
    current_date: str,
    use_thinking_chain: bool = True,
    thinking_results: Optional[dict] = None
) -> str:
    """
    æ„å»ºå¢å¼ºçš„æœç´¢ Promptï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        user_query: ç”¨æˆ·æŸ¥è¯¢
        search_results: æœç´¢ç»“æœ
        current_date: å½“å‰æ—¥æœŸ
        use_thinking_chain: æ˜¯å¦ä½¿ç”¨æ€è€ƒé“¾
        thinking_results: æ€è€ƒç»“æœå­—å…¸ï¼ˆåŒ…å« understanding, analysis, thinkingï¼‰
        
    Returns:
        å®Œæ•´çš„ Prompt
    """
    if use_thinking_chain:
        chain = ThinkingChain()
        # å¦‚æœæœ‰æ€è€ƒç»“æœï¼Œä½¿ç”¨ç»¼åˆä¿¡æ¯æ¨¡å¼
        if thinking_results and (thinking_results.get("understanding") or thinking_results.get("analysis") or thinking_results.get("thinking")):
            return chain.build_synthesis_prompt(user_query, search_results, current_date, thinking_results)
        else:
            # å¦åˆ™ä½¿ç”¨åŸæ¥çš„æ€è€ƒé“¾æ¨¡å¼
            return chain.build_thinking_prompt(user_query, search_results, current_date)
    else:
        # å›é€€åˆ°ç®€å•æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        return _build_simple_prompt(user_query, search_results, current_date)


def _build_simple_prompt(
    user_query: str,
    search_results: List[SearchDocument],
    current_date: str
) -> str:
    """æ„å»ºç®€å•çš„ Promptï¼ˆå‘åå…¼å®¹ï¼‰"""
    from urllib.parse import urlparse
    
    # æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢çš„è¯­è¨€
    def detect_language(text: str) -> str:
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
        return "en" if english_chars > chinese_chars and english_chars > 0 else "zh"
    
    detected_lang = detect_language(user_query)
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€æ·»åŠ å¼ºåˆ¶æŒ‡ä»¤
    if detected_lang == "en":
        lang_instruction = "ğŸ”´ CRITICAL: User's question is in ENGLISH. Answer in ENGLISH ONLY.\n\n"
    else:
        lang_instruction = "ğŸ”´ é‡è¦ï¼šç”¨æˆ·é—®é¢˜æ˜¯ä¸­æ–‡ã€‚åªç”¨ä¸­æ–‡å›ç­”ã€‚\n\n"
    
    context_parts = [lang_instruction + f"# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœï¼ˆä»Šå¤©æ˜¯{current_date}ï¼‰:\n"]
    
    for idx, doc in enumerate(search_results[:15], 1):
        context_parts.append(f"[å‚è€ƒèµ„æ–™ {idx}]")
        context_parts.append(f"æ ‡é¢˜: {doc.title if hasattr(doc, 'title') else 'N/A'}")
        
        if hasattr(doc, 'url') and doc.url:
            try:
                domain = urlparse(doc.url).netloc
                context_parts.append(f"æ¥æº: {domain}")
            except:
                pass
        
        content = doc.content if hasattr(doc, 'content') and doc.content else ''
        if not content and hasattr(doc, 'snippet'):
            content = doc.snippet
        
        if content:
            content = content[:1000] if len(content) > 1000 else content
            context_parts.append(f"å†…å®¹:\n{content}")
        
        context_parts.append("---\n")
    
    context_parts.append("\n# è¯·åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™ï¼Œç”¨è‡ªç„¶ä¸¥è°¨çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
    context_parts.append(f"\n# ç”¨æˆ·é—®é¢˜: {user_query}")
    
    return "\n".join(context_parts)

