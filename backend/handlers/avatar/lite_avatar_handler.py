"""
LiteAvatar Handler - é«˜æ€§èƒ½CPUå®æ—¶æ•°å­—äººé©±åŠ¨
åŸºäºå‚æ•°åŒ–2Dæ¸²æŸ“ï¼Œ30fpså®æ—¶æ€§èƒ½
"""
import cv2
import numpy as np
import torch
from typing import List, Tuple, Optional, Dict, Any
import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
import tempfile
from loguru import logger
import queue
import threading
import time
from io import BytesIO
import soundfile as sf
import wave

from backend.handlers.base import BaseHandler
from backend.core.health_monitor import timer, avatar_processing_time
from backend.utils.audio_utils import AudioProcessor


_TORCH_THREADS_CONFIGURED = False
_TORCH_THREADS_LOCK = threading.Lock()


def _configure_torch_threads(intra_threads: int = None, inter_threads: int = None) -> None:
    """
    ç¡®ä¿PyTorchçº¿ç¨‹æ•°åªé…ç½®ä¸€æ¬¡ï¼Œé¿å…é‡å¤è°ƒç”¨å¯¼è‡´çš„RuntimeError
    
    Args:
        intra_threads: å†…éƒ¨æ“ä½œå¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆå•ä¸ªæ“ä½œå†…çš„å¹¶è¡Œï¼‰ï¼Œé»˜è®¤ä»settingsè¯»å–
        inter_threads: æ“ä½œé—´å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆå¤šä¸ªæ“ä½œé—´çš„å¹¶è¡Œï¼‰ï¼Œé»˜è®¤ä»settingsè¯»å–
    """
    global _TORCH_THREADS_CONFIGURED
    if _TORCH_THREADS_CONFIGURED:
        return
    
    with _TORCH_THREADS_LOCK:
        if _TORCH_THREADS_CONFIGURED:
            return
        
        # ä»settingsè¯»å–é…ç½®
        from backend.app.config import settings
        if intra_threads is None:
            intra_threads = settings.PYTORCH_INTRA_THREADS
        if inter_threads is None:
            inter_threads = settings.PYTORCH_INTER_THREADS
        
        try:
            torch.set_num_threads(intra_threads)
            torch.set_num_interop_threads(inter_threads)
            logger.info(f"PyTorchçº¿ç¨‹é…ç½®: intra={intra_threads}, inter={inter_threads}")
        except RuntimeError as exc:
            logger.warning(f"PyTorchçº¿ç¨‹é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çº¿ç¨‹è®¾ç½®: {exc}")
        finally:
            _TORCH_THREADS_CONFIGURED = True


class LiteAvatarHandler(BaseHandler):
    """
    LiteAvataræ•°å­—äººé©±åŠ¨Handler
    
    ç‰¹æ€§ï¼š
    - 30fpså®æ—¶æ¸²æŸ“
    - CPUåŸç”Ÿä¼˜åŒ–
    - å‚æ•°åŒ–é©±åŠ¨ï¼Œæ— éœ€äººè„¸æ£€æµ‹
    - æµç•…çš„å£å‹åŠ¨ç”»
    """
    
    def __init__(self,
                 fps: int = 20,
                 resolution: Tuple[int, int] = (512, 512),
                 config: Optional[dict] = None):
        super().__init__(config)
        self.fps = fps
        self.resolution = resolution
        
        # LiteAvataræ ¸å¿ƒç»„ä»¶
        self.audio2mouth = None
        self.encoder = None
        self.generator = None
        
        # Avataræ•°æ®
        self.data_dir = None
        self.bg_data_list = []
        self.ref_img_list = []
        self.neutral_pose = None
        self.merge_mask = None
        
        # äººè„¸åŒºåŸŸ
        self.y1 = self.y2 = self.x1 = self.x2 = 0
        self.bg_video_frame_count = 0
        
        # âš¡ ä¼˜åŒ–ï¼šç§»é™¤å…¨å±€é˜Ÿåˆ—ï¼Œæ”¹ä¸ºåŠ¨æ€åˆ›å»ºç‹¬ç«‹é˜Ÿåˆ—ï¼ˆé¿å…ä»»åŠ¡é—´ç«äº‰ï¼‰
        self.threads = []
        
        # è®¾å¤‡
        self.use_gpu = self.config.get("use_gpu", False)
        self.device = "cuda" if self.use_gpu else "cpu"
        
        # å›¾åƒé¢„å¤„ç†
        from torchvision import transforms
        self.image_transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])
        
        # å‚æ•°åˆ—è¡¨ï¼ˆ32ä¸ªå£å‹å‚æ•°ï¼‰
        self.p_list = [str(ii) for ii in range(32)]
        
        # éŸ³é¢‘å¤„ç†å™¨
        self.audio_processor = AudioProcessor()
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=2)
        
    async def _setup(self):
        """åˆå§‹åŒ–LiteAvataræ¨¡å‹å’Œæ•°æ®"""
        try:
            # 1. æ£€æŸ¥avataræ•°æ®ç›®å½•
            avatar_name = self.config.get("avatar_name", "default")
            self.data_dir = Path("models") / "lite_avatar" / avatar_name
            
            if not self.data_dir.exists():
                raise FileNotFoundError(
                    f"Avataræ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}\n"
                    f"è¯·è¿è¡Œ: python scripts/prepare_lite_avatar_data.py --avatar {avatar_name}"
                )
            
            # é…ç½®PyTorchçº¿ç¨‹æ•°ï¼ˆç¡®ä¿åªæ‰§è¡Œä¸€æ¬¡ï¼‰
            _configure_torch_threads()
            
            # 2. åŠ è½½Audio2Mouthæ¨¡å‹
            logger.info("åŠ è½½Audio2Mouthæ¨¡å‹...")
            self._load_audio2mouth()
            
            # 3. åŠ è½½AvataråŠ¨æ€æ¨¡å‹
            logger.info("åŠ è½½AvataråŠ¨æ€æ¨¡å‹...")
            await self._load_avatar_model()
            
            # 4. åˆ›å»ºæ¸²æŸ“çº¿ç¨‹æ± ï¼ˆâš¡ ä¼˜åŒ–ï¼šæ¯ä¸ªä»»åŠ¡åŠ¨æ€åˆ†é…çº¿ç¨‹ï¼‰
            num_threads = self.config.get("render_threads", 4)
            self.render_executor = ThreadPoolExecutor(
                max_workers=num_threads,
                thread_name_prefix="LiteAvatar-Render"
            )
            logger.info(f"LiteAvataråˆå§‹åŒ–å®Œæˆ - Avatar: {avatar_name}, FPS: {self.fps}, æ¸²æŸ“çº¿ç¨‹æ± : {num_threads}")
            
        except Exception as e:
            logger.error(f"LiteAvataråˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _load_audio2mouth(self):
        """åŠ è½½Audio2Mouthæ¨¡å‹"""
        try:
            import onnxruntime
            
            # ä¼˜å…ˆä½¿ç”¨ INT8 é‡åŒ–æ¨¡å‹ï¼ˆåŠ é€Ÿ 2~3 å€ï¼‰
            int8_path = Path("models") / "lite_avatar" / "model_int8.onnx"
            fp32_path = Path("models") / "lite_avatar" / "model_1.onnx"

            if int8_path.exists():
                model_path = int8_path
                logger.info(f"æ£€æµ‹åˆ° INT8 é‡åŒ–æ¨¡å‹: {int8_path}")
            elif fp32_path.exists():
                model_path = fp32_path
                logger.warning("æœªæ‰¾åˆ° INT8 æ¨¡å‹ï¼Œé€€å› FP32ï¼šåŠ è½½é€Ÿåº¦ä¼šæ…¢ 2~3 å€")
            else:
                raise FileNotFoundError(
                    f"Audio2Mouth æ¨¡å‹ä¸å­˜åœ¨: {fp32_path} æˆ– {int8_path}\n"
                    f"è¯·è¿è¡Œ: bash scripts/download_lite_avatar_models.sh"
                )
            
            # é…ç½®ONNXæ¨ç†é€‰é¡¹ï¼ˆä»settingsè¯»å–çº¿ç¨‹é…ç½®ï¼‰
            from backend.app.config import settings
            sess_options = onnxruntime.SessionOptions()
            sess_options.intra_op_num_threads = settings.PYTORCH_INTRA_THREADS  # ç®—å­å†…éƒ¨å¹¶è¡Œçº¿ç¨‹æ•°
            sess_options.inter_op_num_threads = settings.PYTORCH_INTER_THREADS  # ç®—å­ä¹‹é—´å¹¶è¡Œçº¿ç¨‹æ•°
            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # åˆ›å»ºONNXæ¨ç†ä¼šè¯
            provider = "CUDAExecutionProvider" if self.use_gpu else "CPUExecutionProvider"
            self.audio2mouth = onnxruntime.InferenceSession(
                str(model_path),
                sess_options=sess_options,
                providers=[provider]
            )
            
            logger.info(
                f"Audio2Mouthæ¨¡å‹å·²åŠ è½½: {model_path.name} | provider={provider} | "
                f"threads={sess_options.intra_op_num_threads}")
            
        except ImportError:
            logger.error("ç¼ºå°‘ä¾èµ–: onnxruntimeï¼Œè¯·è¿è¡Œ: pip install onnxruntime")
            raise
    
    async def _load_avatar_model(self):
        """åŠ è½½Avatarç¼–ç å™¨å’Œç”Ÿæˆå™¨"""
        # åŠ è½½ç¼–è§£ç å™¨æ¨¡å‹
        encoder_path = self.data_dir / "net_encode.pt"
        decoder_path = self.data_dir / "net_decode.pt"
        
        if not encoder_path.exists() or not decoder_path.exists():
            raise FileNotFoundError(
                f"Avataræ¨¡å‹æ–‡ä»¶ç¼ºå¤±:\n"
                f"  - {encoder_path}\n"
                f"  - {decoder_path}"
            )
        
        self.encoder = torch.jit.load(str(encoder_path)).to(self.device)
        self.generator = torch.jit.load(str(decoder_path)).to(self.device)
        
        # åŠ è½½ä¸­æ€§è¡¨æƒ…å‚æ•°
        neutral_pose_path = self.data_dir / "neutral_pose.npy"
        if neutral_pose_path.exists():
            self.neutral_pose = np.load(str(neutral_pose_path))
        else:
            self.neutral_pose = np.zeros(32)
        
        # åŠ è½½èƒŒæ™¯è§†é¢‘
        await self._load_background_video()
        
        # åŠ è½½å¹¶ç¼–ç å‚è€ƒå¸§
        await self._load_reference_frames()
        
        # Warm-upæ¨ç†ï¼šé¿å…ç¬¬ä¸€æ¬¡æ¨ç†è¾“å‡ºNaN
        logger.info("æ‰§è¡Œæ¨¡å‹warm-upæ¨ç†...")
        await self._warmup_model()
        
        logger.info(f"Avataræ¨¡å‹å·²åŠ è½½ - èƒŒæ™¯å¸§æ•°: {self.bg_video_frame_count}, å‚è€ƒå¸§æ•°: {len(self.ref_img_list)}")
    
    async def _load_background_video(self):
        """åŠ è½½èƒŒæ™¯è§†é¢‘å¸§"""
        bg_video_path = self.data_dir / "bg_video.mp4"
        if not bg_video_path.exists():
            raise FileNotFoundError(f"èƒŒæ™¯è§†é¢‘ä¸å­˜åœ¨: {bg_video_path}")
        
        # è¯»å–èƒŒæ™¯è§†é¢‘
        bg_video = cv2.VideoCapture(str(bg_video_path))
        self.bg_data_list = []
        
        while True:
            ret, frame = bg_video.read()
            if not ret:
                break
            self.bg_data_list.append(frame)
        
        bg_video.release()
        
        bg_frame_cnt = self.config.get("bg_frame_count", 150)
        self.bg_video_frame_count = min(bg_frame_cnt, len(self.bg_data_list))
        
        # è¯»å–äººè„¸åŒºåŸŸ
        face_box_path = self.data_dir / "face_box.txt"
        if face_box_path.exists():
            with open(face_box_path, 'r') as f:
                y1, y2, x1, x2 = f.readline().strip().split()
                self.y1, self.y2, self.x1, self.x2 = int(y1), int(y2), int(x1), int(x2)
        
        # åˆ›å»ºèåˆmask
        self.merge_mask = (np.ones((self.y2 - self.y1, self.x2 - self.x1, 3)) * 255).astype(np.uint8)
        self.merge_mask[10:-10, 10:-10, :] *= 0
        self.merge_mask = cv2.GaussianBlur(self.merge_mask, (21, 21), 15)
        self.merge_mask = self.merge_mask / 255
        logger.info(
            f"è„¸éƒ¨ROI: y=({self.y1},{self.y2}), x=({self.x1},{self.x2}), maskå½¢çŠ¶={self.merge_mask.shape}"
        )
    
    async def _load_reference_frames(self):
        """åŠ è½½å¹¶ç¼–ç å‚è€ƒå¸§"""
        ref_frames_dir = self.data_dir / "ref_frames"
        if not ref_frames_dir.exists():
            raise FileNotFoundError(f"å‚è€ƒå¸§ç›®å½•ä¸å­˜åœ¨: {ref_frames_dir}")
        
        self.ref_img_list = []
        
        for ii in range(self.bg_video_frame_count):
            ref_path = ref_frames_dir / f"ref_{ii:05d}.jpg"
            if not ref_path.exists():
                logger.warning(f"å‚è€ƒå¸§ä¸å­˜åœ¨: {ref_path}")
                continue
            
            # è¯»å–å¹¶é¢„å¤„ç†
            image = cv2.cvtColor(cv2.imread(str(ref_path))[:, :, 0:3], cv2.COLOR_BGR2RGB)
            image = cv2.resize(image, (384, 384), interpolation=cv2.INTER_LINEAR)
            ref_img = self.image_transforms(np.uint8(image))
            
            # ç¼–ç 
            encoder_input = ref_img.unsqueeze(0).float().to(self.device)
            with torch.no_grad():
                x = self.encoder(encoder_input)
            # âš¡ ä¿æŒencoderè¾“å‡ºä¸ºlistæ ¼å¼ï¼ˆgeneratoræœŸæœ›List[Tensor]ï¼‰
            if not isinstance(x, (list, tuple)):
                x = [x]  # è½¬æ¢ä¸ºlist
            # å»æ‰æ¯ä¸ªtensorçš„batchç»´åº¦
            x = [t.squeeze(0) for t in x]
            self.ref_img_list.append(x)
    
    async def _warmup_model(self):
        """æ‰§è¡Œwarm-upæ¨ç†é¿å…NaN"""
        try:
            # ä½¿ç”¨ä¸­æ€§å‚æ•°æ‰§è¡Œä¸€æ¬¡æ¨ç†
            neutral_params = {key: 0.0 for key in self.p_list}
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‚è€ƒå¸§
            if self.ref_img_list:
                with torch.no_grad():
                    # ref_img_list[i]æ˜¯List[Tensor]ï¼Œéœ€è¦æ·»åŠ batchç»´åº¦
                    ref_img = [t.unsqueeze(0).to(self.device) for t in self.ref_img_list[0]]
                    test_output = self.generator(
                        ref_img,
                        torch.zeros(1, 32).float().to(self.device)
                    )
                    # æ£€æŸ¥è¾“å‡º
                    if torch.isnan(test_output).any():
                        logger.warning("Warm-upæ¨ç†ä»åŒ…å«NaNï¼Œå°†åœ¨è¿è¡Œæ—¶å¤„ç†")
                    else:
                        logger.info("Warm-upæ¨ç†æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Warm-upæ¨ç†å¤±è´¥: {e}ï¼Œç»§ç»­å¯åŠ¨")
    
    async def process(self, data: Dict[str, Any]) -> bytes:
        """
        å¤„ç†éŸ³é¢‘ç”Ÿæˆæ•°å­—äººè§†é¢‘
        
        Args:
            data: åŒ…å«audio_dataï¼ˆéŸ³é¢‘å­—èŠ‚ï¼‰çš„å­—å…¸
        
        Returns:
            è§†é¢‘å­—èŠ‚æµï¼ˆMP4æ ¼å¼ï¼‰
        """
        import time
        start_total = time.time()
        
        with timer(avatar_processing_time):
            try:
                # âš¡ ä¼˜åŒ–ï¼šç§»é™¤å…¨å±€é”ï¼Œå…è®¸å¹¶å‘æ¸²æŸ“å¤šä¸ªè§†é¢‘
                audio_data = data.get("audio_data")
                if not audio_data:
                    raise ValueError("ç¼ºå°‘audio_dataå‚æ•°")
                
                # 1. éŸ³é¢‘è½¬å‚æ•°ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                logger.info("æå–å£å‹å‚æ•°...")
                start = time.time()
                try:
                    param_res = await asyncio.wait_for(
                        self._audio_to_params(audio_data),
                        timeout=60.0  # 60ç§’è¶…æ—¶
                    )
                    logger.debug(f"å£å‹å‚æ•°æå–è€—æ—¶: {time.time() - start:.2f}ç§’")
                except asyncio.TimeoutError:
                    logger.error("âŒ å£å‹å‚æ•°æå–è¶…æ—¶ï¼ˆ60ç§’ï¼‰")
                    raise ValueError("å£å‹å‚æ•°æå–è¶…æ—¶")
                
                # 2. å‚æ•°è½¬è§†é¢‘å¸§ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                logger.info(f"æ¸²æŸ“{len(param_res)}å¸§...")
                start = time.time()
                try:
                    frames = await asyncio.wait_for(
                        self._params_to_frames(param_res),
                        timeout=120.0  # 120ç§’è¶…æ—¶ï¼ˆæ¸²æŸ“å¯èƒ½è¾ƒæ…¢ï¼‰
                    )
                    video_duration = len(frames) / self.fps if self.fps else 0
                    logger.info(f"è§†é¢‘å¸§æ•°: {len(frames)}, é¢„æœŸæ—¶é•¿: {video_duration:.2f}ç§’")
                    logger.debug(f"å¸§æ¸²æŸ“è€—æ—¶: {time.time() - start:.2f}ç§’")
                except asyncio.TimeoutError:
                    logger.error("âŒ è§†é¢‘å¸§æ¸²æŸ“è¶…æ—¶ï¼ˆ120ç§’ï¼‰")
                    raise ValueError("è§†é¢‘å¸§æ¸²æŸ“è¶…æ—¶")
                
                # 3. åˆæˆè§†é¢‘ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                logger.info("åˆæˆè§†é¢‘...")
                start = time.time()
                try:
                    video_data = await asyncio.wait_for(
                        self._frames_to_video(frames, audio_data),
                        timeout=60.0  # 60ç§’è¶…æ—¶
                    )
                    logger.debug(f"è§†é¢‘åˆæˆè€—æ—¶: {time.time() - start:.2f}ç§’")
                except asyncio.TimeoutError:
                    logger.error("âŒ è§†é¢‘åˆæˆè¶…æ—¶ï¼ˆ60ç§’ï¼‰")
                    raise ValueError("è§†é¢‘åˆæˆè¶…æ—¶")
                
                logger.info(f"æ€»è€—æ—¶: {time.time() - start_total:.2f}ç§’")
                
                return video_data
                
            except Exception as e:
                logger.error(f"LiteAvatarå¤„ç†å¤±è´¥: {e}")
                raise
    
    async def generate(self, audio_data: bytes, template_path: str = None) -> bytes:
        """
        ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå…¬å…±æ¥å£ï¼‰
        
        Args:
            audio_data: éŸ³é¢‘å­—èŠ‚æµï¼ˆæ¥è‡ªTTSï¼‰
            template_path: æ¨¡æ¿è·¯å¾„ï¼ˆLiteAvatarä¸ä½¿ç”¨æ­¤å‚æ•°ï¼‰
        
        Returns:
            MP4è§†é¢‘å­—èŠ‚æµ
        """
        if not self._initialized:
            await self.initialize()
        
        # è°ƒç”¨processæ–¹æ³•
        return await self.process({"audio_data": audio_data})
    
    async def _audio_to_params(self, audio_data: bytes) -> List[Dict[str, float]]:
        """éŸ³é¢‘è½¬å£å‹å‚æ•°"""
        try:
            # å°†TTSéŸ³é¢‘ï¼ˆé»˜è®¤MP3ï¼‰è½¬æ¢ä¸º16kå•å£°é“WAV
            wav_bytes = await self.audio_processor.convert_to_wav(audio_data, input_format="mp3")
            if not wav_bytes:
                raise ValueError("éŸ³é¢‘è½¬æ¢å¤±è´¥")

            audio_array, sr = sf.read(BytesIO(wav_bytes))
            if sr != 16000:
                # åŒé‡ä¿é™©ï¼šffmpegåº”å·²è½¬æ¢ä¸º16kï¼Œå¦‚æœä»ä¸ä¸€è‡´åˆ™å¼ºåˆ¶é‡é‡‡æ ·
                audio_array = self.audio_processor.resample_audio(audio_array, sr, 16000)
                sr = 16000
            audio_duration = len(audio_array) / sr if sr else 0
            logger.info(f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’, é‡‡æ ·ç‡: {sr}")
            
            # æå–Paraformerç‰¹å¾ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
            # å‘ä¸Šå–æ•´ç¡®ä¿è§†é¢‘æ—¶é•¿ >= éŸ³é¢‘æ—¶é•¿
            import math
            frame_cnt = math.ceil(len(audio_array) / 16000 * self.fps)
            logger.info(f"ğŸ¯ å¼€å§‹æå–éŸ³é¢‘ç‰¹å¾ (å¸§æ•°: {frame_cnt})...")
            try:
                au_data = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor,
                        self._extract_paraformer_feature,
                        audio_array,
                        frame_cnt
                    ),
                    timeout=30.0  # 30ç§’è¶…æ—¶
                )
                logger.info(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {au_data.shape}")
            except asyncio.TimeoutError:
                logger.error("âŒ éŸ³é¢‘ç‰¹å¾æå–è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                raise ValueError("éŸ³é¢‘ç‰¹å¾æå–è¶…æ—¶")
            
            # æ¸…ç†ç‰¹å¾ä¸­çš„NaN/Infï¼Œé¿å…åç»­æ¨ç†å¼‚å¸¸
            au_data = np.nan_to_num(au_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # é¢„æµ‹å£å‹å‚æ•°ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
            ph_data = np.zeros((au_data.shape[0], 2))
            logger.info(f"ğŸ¯ å¼€å§‹æ¨ç†å£å‹å‚æ•°...")
            try:
                param_res = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(
                        self.executor,
                        self._inference_mouth_params,
                        au_data,
                        ph_data
                    ),
                    timeout=30.0  # 30ç§’è¶…æ—¶
                )
                logger.info(f"âœ… å£å‹å‚æ•°æ¨ç†å®Œæˆ: {len(param_res)} ä¸ªå‚æ•°")
            except asyncio.TimeoutError:
                logger.error("âŒ å£å‹å‚æ•°æ¨ç†è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                raise ValueError("å£å‹å‚æ•°æ¨ç†è¶…æ—¶")
            
            # FPSè½¬æ¢
            if self.fps != 30:
                param_res = self._interpolate_params(param_res, self.fps)
            
            return param_res
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è½¬å‚æ•°å¤±è´¥: {e}")
            raise
    
    def _extract_paraformer_feature(self, audio_array: np.ndarray, frame_cnt: int) -> np.ndarray:
        """æå–Paraformerç‰¹å¾"""
        try:
            # å¯¼å…¥ç‰¹å¾æå–å‡½æ•°
            import sys
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_paths = [
                Path("/opt/lite-avatar"),
                Path("/opt/lightavatar/lite-avatar"),
                Path("d:/Aprojects/Light-avatar/lite-avatar-main")
            ]
            
            for lite_avatar_path in possible_paths:
                if lite_avatar_path.exists():
                    if str(lite_avatar_path) not in sys.path:
                        sys.path.insert(0, str(lite_avatar_path))
                    break
            
            from extract_paraformer_feature import extract_para_feature
            return extract_para_feature(audio_array, frame_cnt)
            
        except Exception as e:
            logger.warning(f"Paraformerç‰¹å¾æå–å¤±è´¥ï¼Œä½¿ç”¨MFCCæ›¿ä»£: {e}")
            # Fallback: ä½¿ç”¨librosaæå–MFCCç‰¹å¾
            return self._extract_mfcc_feature(audio_array, frame_cnt)
    
    def _extract_mfcc_feature(self, audio_array: np.ndarray, frame_cnt: int) -> np.ndarray:
        """ä½¿ç”¨MFCCä½œä¸ºéŸ³é¢‘ç‰¹å¾çš„fallbackï¼ŒåŒ¹é…Paraformeræ ¼å¼(frames, 50, 512)"""
        try:
            import librosa
            
            # æ¨¡å‹æœŸæœ›è¾“å…¥: (frames, 50, 512)
            # æ€»ç‰¹å¾ç»´åº¦: 50 * 512 = 25600
            target_seq_len = 50
            target_feat_dim = 512
            
            # æå–æ¢…å°”é¢‘è°±å›¾ä½œä¸ºç‰¹å¾
            n_mels = 80
            hop_length = len(audio_array) // frame_cnt if frame_cnt > 0 else 512
            
            # æå–Melé¢‘è°±
            mel = librosa.feature.melspectrogram(
                y=audio_array,
                sr=16000,
                n_mels=n_mels,
                hop_length=hop_length,
                n_fft=2048
            )
            
            # è½¬æ¢ä¸ºå¯¹æ•°åˆ»åº¦
            mel_db = librosa.power_to_db(mel, ref=np.max)
            
            # è½¬ç½®ä¸º (frames, n_mels)
            mel_db = mel_db.T
            
            # è°ƒæ•´åˆ°ç›®æ ‡å¸§æ•°
            if mel_db.shape[0] != frame_cnt:
                from scipy import interpolate
                x = np.linspace(0, frame_cnt - 1, mel_db.shape[0])
                x_new = np.arange(frame_cnt)
                f = interpolate.interp1d(x, mel_db, axis=0, kind='linear', fill_value='extrapolate')
                mel_db = f(x_new)
            
            # æ‰©å±•ç‰¹å¾ï¼šé‡å¤å¹¶paddingåˆ° (frames, 50*512)
            # å…ˆæ‰©å±•åˆ°25600ç»´
            total_dim = target_seq_len * target_feat_dim  # 25600
            if mel_db.shape[1] < total_dim:
                # é‡å¤å¹¶padding
                repeats = (total_dim // mel_db.shape[1]) + 1
                expanded = np.tile(mel_db, (1, repeats))
                expanded = expanded[:, :total_dim]
            else:
                expanded = mel_db[:, :total_dim]
            
            # Reshapeä¸º (frames, 50, 512)
            features = expanded.reshape(frame_cnt, target_seq_len, target_feat_dim)
            
            logger.info(f"ä½¿ç”¨MFCCç‰¹å¾æ›¿ä»£Paraformer: shape={features.shape}")
            return features.astype(np.float32)
            
        except Exception as e:
            logger.error(f"MFCCæå–å¤±è´¥: {e}")
            # æœ€åçš„fallbackï¼šé›¶ç‰¹å¾ï¼Œæ­£ç¡®çš„4Då½¢çŠ¶
            return np.zeros((frame_cnt, 50, 512), dtype=np.float32)
    
    def _inference_mouth_params(self, au_data: np.ndarray, ph_data: np.ndarray) -> List[Dict[str, float]]:
        """æ¨ç†å£å‹å‚æ•°ï¼ˆä½¿ç”¨å®˜æ–¹é€»è¾‘ï¼‰"""
        param_res = []
        # è®°å½•å®é™…å¸§æ•°ï¼ˆç”¨äºæˆªæ–­paddingçš„å¸§ï¼‰
        actual_frame_count = au_data.shape[0]
        audio_length = ph_data.shape[0] / 30
        interval = 1.0
        frag = int(interval * 30 / 5 + 0.5)
        
        w = np.array([1.0]).astype(np.float32)
        sp = np.array([2]).astype(np.int64)
        
        start_time = 0.0
        end_time = start_time + interval
        is_end = False
        
        while True:
            start_frame = int(start_time * 30)
            end_frame = start_frame + int(30 * interval)
            
            # å¤„ç†éŸ³é¢‘ç»“æŸæƒ…å†µ
            if end_time >= audio_length:
                is_end = True
                end_frame = au_data.shape[0]
                # å¦‚æœéŸ³é¢‘ä¸è¶³1ç§’ï¼Œä»æœ«å°¾å‘å‰å–30å¸§ï¼ˆpaddingï¼‰
                if end_frame < 30:
                    start_frame = 0
                    # Paddingåˆ°30å¸§
                    pad_size = 30 - end_frame
                    # au_dataæ˜¯3D (frames, 50, 512)ï¼Œéœ€è¦3ä¸ªç»´åº¦çš„padding
                    input_au = np.pad(au_data, ((0, pad_size), (0, 0), (0, 0)), mode='edge')
                    # ph_dataæ˜¯2D (frames, 2)ï¼Œéœ€è¦2ä¸ªç»´åº¦çš„padding
                    input_ph = np.pad(ph_data, ((0, pad_size), (0, 0)), mode='edge')
                else:
                    start_frame = end_frame - int(30 * interval)
                    input_au = au_data[start_frame:end_frame]
                    input_ph = ph_data[start_frame:end_frame]
                start_time = max(0, audio_length - interval)
                end_time = audio_length
            else:
                input_au = au_data[start_frame:end_frame]
                input_ph = ph_data[start_frame:end_frame]
            
            input_au = input_au[np.newaxis, :].astype(np.float32)
            input_ph = input_ph[np.newaxis, :].astype(np.float32)

            # è¿è¡Œå‰å†æ¬¡æ¸…ç†NaN/Inf
            input_au = np.nan_to_num(input_au, nan=0.0, posinf=0.0, neginf=0.0)
            input_ph = np.nan_to_num(input_ph, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Debug: æ‰“å°è¾“å…¥å½¢çŠ¶
            logger.debug(f"ONNXè¾“å…¥å½¢çŠ¶ - input_au: {input_au.shape}, input_ph: {input_ph.shape}")
            
            # æ¨ç†
            try:
                output, feat = self.audio2mouth.run(
                    ['output', 'feat'],
                    {'input_au': input_au, 'input_ph': input_ph, 'input_sp': sp, 'w': w}
                )
            except Exception as e:
                # æ‰“å°æ¨¡å‹æœŸæœ›çš„è¾“å…¥å½¢çŠ¶
                logger.error(f"ONNXæ¨ç†å¤±è´¥: {e}")
                logger.error(f"å½“å‰è¾“å…¥å½¢çŠ¶: input_au={input_au.shape}, input_ph={input_ph.shape}")
                # å°è¯•è·å–æ¨¡å‹è¾“å…¥è§„æ ¼
                try:
                    for inp in self.audio2mouth.get_inputs():
                        logger.error(f"æ¨¡å‹æœŸæœ›è¾“å…¥ '{inp.name}': shape={inp.shape}, type={inp.type}")
                except:
                    pass
                raise
            
            # æ¸…ç†æ¨ç†è¾“å‡ºä¸­çš„NaN/Infï¼Œé¿å…åç»­å£å‹ä¸ºä¸­æ€§
            output = np.nan_to_num(output, nan=0.0, posinf=0.0, neginf=0.0)

            # æå–å‚æ•°
            if start_time == 0.0:
                end_idx = int(30 * interval) if not is_end else int(30 * interval)
                for tt in range(end_idx - (0 if is_end else frag)):
                    param_frame = {}
                    for ii, key in enumerate(self.p_list):
                        param_frame[key] = round(float(output[0, tt, ii]), 3)
                    param_res.append(param_frame)
            elif not is_end:
                for tt in range(frag, int(30 * interval) - frag):
                    frame_id = start_frame + tt
                    if frame_id < len(param_res):
                        scale = min((len(param_res) - frame_id) / frag, 1.0)
                        for ii, key in enumerate(self.p_list):
                            value = float(output[0, tt, ii])
                            value = (1 - scale) * value + scale * param_res[frame_id][key]
                            param_res[frame_id][key] = round(value, 3)
                    else:
                        param_frame = {}
                        for ii, key in enumerate(self.p_list):
                            param_frame[key] = round(float(output[0, tt, ii]), 3)
                        param_res.append(param_frame)
            else:
                for tt in range(frag, int(30 * interval)):
                    frame_id = start_frame + tt
                    if frame_id < len(param_res):
                        scale = min((len(param_res) - frame_id) / frag, 1.0)
                        for ii, key in enumerate(self.p_list):
                            value = float(output[0, tt, ii])
                            value = (1 - scale) * value + scale * param_res[frame_id][key]
                            param_res[frame_id][key] = round(value, 3)
                    else:
                        param_frame = {}
                        for ii, key in enumerate(self.p_list):
                            param_frame[key] = round(float(output[0, tt, ii]), 3)
                        param_res.append(param_frame)
            
            start_time = end_time - (frag / 10)
            end_time = start_time + interval
            if is_end:
                break
        
        # æ¨ç†é€»è¾‘å·²ç»æ ¹æ®audio_lengthæ­£ç¡®ç”Ÿæˆäº†å¸§æ•°ï¼Œä¸éœ€è¦é¢å¤–æˆªæ–­
        logger.debug(f"æ¨ç†ç”Ÿæˆ {len(param_res)} å¸§å‚æ•°ï¼ˆéŸ³é¢‘ç‰¹å¾å¸§æ•°: {actual_frame_count}ï¼‰")
        
        # å¹³æ»‘å¤„ç†
        param_res = self._smooth_params(param_res)
        
        return param_res
    
    def _smooth_params(self, param_res: List[Dict[str, float]]) -> List[Dict[str, float]]:
        """å¹³æ»‘å£å‹å‚æ•°"""
        from scipy import signal
        
        for key in param_res[0]:
            val_list = [p[key] for p in param_res]
            val_array = np.array(val_list)
            
            # Butterworthä½é€šæ»¤æ³¢
            wn = 2 * 10 / 30  # cutoff=10, fs=30
            b, a = signal.butter(4, wn, 'lowpass', analog=False)
            smoothed = signal.filtfilt(b, a, val_array, padtype=None, axis=0)
            
            for ii, p in enumerate(param_res):
                p[key] = float(smoothed[ii])
        
        return param_res
    
    def _interpolate_params(self, param_res: List[Dict[str, float]], target_fps: int) -> List[Dict[str, float]]:
        """å‚æ•°æ’å€¼ä»¥é€‚é…ç›®æ ‡FPS"""
        from scipy.interpolate import interp1d
        
        old_len = len(param_res)
        new_len = int(old_len / 30 * target_fps + 0.5)
        
        interp_dict = {}
        for key in param_res[0]:
            val_list = [p[key] for p in param_res]
            val_array = np.array(val_list)
            
            x = np.linspace(0, old_len - 1, num=old_len, endpoint=True)
            newx = np.linspace(0, old_len - 1, num=new_len, endpoint=True)
            f = interp1d(x, val_array)
            interp_dict[key] = f(newx)
        
        new_param_res = []
        for ii in range(new_len):
            param_frame = {}
            for key in interp_dict:
                param_frame[key] = float(interp_dict[key][ii])
            new_param_res.append(param_frame)
        
        return new_param_res
    
    async def _params_to_frames(self, param_res: List[Dict[str, float]]) -> List[np.ndarray]:
        """âš¡ ä¼˜åŒ–ï¼šæ‰¹é‡æ¨ç†ï¼ˆä¸²è¡Œæ‰§è¡Œé¿å…CPUè¿‡è½½ï¼‰"""
        logger.debug(f"å¼€å§‹æ¸²æŸ“ {len(param_res)} ä¸ªå‚æ•°å¸§")
        
        # å‡†å¤‡èƒŒæ™¯å¸§ID
        bg_frame_ids = []
        for ii in range(len(param_res)):
            if int(ii / self.bg_video_frame_count) % 2 == 0:
                bg_frame_id = ii % self.bg_video_frame_count
            else:
                bg_frame_id = self.bg_video_frame_count - 1 - ii % self.bg_video_frame_count
            bg_frame_ids.append(bg_frame_id)
        
        # âš¡ ä¼˜åŒ–ï¼šé€‚ä¸­çš„batch_sizeï¼Œå¹³è¡¡é€Ÿåº¦ä¸èµ„æº
        # CPUç¯å¢ƒï¼šbatch_size=16æœ€ä¼˜ï¼ˆé¿å…å†…å­˜å¸¦å®½ç“¶é¢ˆï¼‰
        batch_size = 16
        
        # ä¸²è¡Œå¤„ç†batchï¼ˆé¿å…è¿‡åº¦å¹¶å‘å¯¼è‡´CPUè¿‡è½½ï¼‰
        loop = asyncio.get_event_loop()
        frames = []
        
        for start_idx in range(0, len(param_res), batch_size):
            end_idx = min(start_idx + batch_size, len(param_res))
            batch_params = param_res[start_idx:end_idx]
            batch_bg_ids = bg_frame_ids[start_idx:end_idx]
            
            # æ‰¹é‡æ¨ç†å½“å‰batch
            batch_frames = await loop.run_in_executor(
                self.render_executor,
                self._render_batch_frames,
                batch_params, batch_bg_ids, start_idx
            )
            frames.extend(batch_frames)
        
        num_batches = (len(param_res) + batch_size - 1) // batch_size
        logger.debug(f"æ‰€æœ‰ {len(frames)} å¸§æ¸²æŸ“å®Œæˆï¼ˆ{num_batches}ä¸ªbatchï¼Œbatch_size={batch_size}ï¼‰")
        
        # æ¸…ç†ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        import gc
        gc.collect()
        
        return frames
    
    def _render_batch_frames(self, batch_params: List[Dict[str, float]], 
                            batch_bg_ids: List[int], start_idx: int) -> List[np.ndarray]:
        """âš¡ æ‰¹é‡æ¸²æŸ“å¸§ï¼ˆåŠ é€Ÿå…³é”®ï¼‰"""
        try:
            batch_size = len(batch_params)
            
            # 1. æ‰¹é‡å‡†å¤‡å‚æ•°
            param_arrays = np.array([[p[key] for key in self.p_list] for p in batch_params])
            param_arrays = np.nan_to_num(param_arrays, nan=0.0)
            
            # 2. æ‰¹é‡æ¨ç†ï¼ˆå…³é”®ä¼˜åŒ–ï¼šä¸€æ¬¡æ¨ç†å¤šå¸§ï¼‰
            with torch.no_grad():
                param_tensor = torch.from_numpy(param_arrays).float().to(self.device)  # (batch, 32)
                
                # å‡†å¤‡æ‰¹é‡ref_imgsï¼ˆList[Tensor]æ ¼å¼ï¼‰
                # ref_img_list[i]æ˜¯List[Tensor]ï¼Œéœ€è¦å°†batchä¸­çš„å¤šä¸ªListåˆå¹¶
                # ä¾‹å¦‚ï¼šref_img_list[0]=[t0,t1], ref_img_list[1]=[t0',t1']
                # åˆå¹¶æˆï¼š[stack([t0,t0']), stack([t1,t1'])]
                ref_imgs_list = [self.ref_img_list[bg_id] for bg_id in batch_bg_ids]
                num_tensors = len(ref_imgs_list[0])  # Listä¸­Tensorçš„æ•°é‡
                ref_imgs_batch = [
                    torch.stack([ref_imgs_list[j][i] for j in range(len(ref_imgs_list))]).to(self.device)
                    for i in range(num_tensors)
                ]
                
                # æ‰¹é‡ç”Ÿæˆ
                mouth_imgs = self.generator(ref_imgs_batch, param_tensor)  # (batch, 3, H, W)
                
                # æ£€æµ‹NaN
                if torch.isnan(mouth_imgs).any():
                    logger.warning(f"æ‰¹é‡æ¨ç†è¾“å‡ºåŒ…å«NaNï¼Œä½¿ç”¨é›¶å¼ é‡æ›¿ä»£")
                    mouth_imgs = torch.nan_to_num(mouth_imgs, nan=0.0)
            
            # 3. æ‰¹é‡åå¤„ç†
            mouth_imgs = mouth_imgs.detach().cpu()
            mouth_imgs = (mouth_imgs / 2 + 0.5).clamp(0, 1)  # åå½’ä¸€åŒ–
            
            frames = []
            for i, bg_id in enumerate(batch_bg_ids):
                # æå–å•å¸§
                mouth_img = mouth_imgs[i].permute(1, 2, 0) * 255
                mouth_img = mouth_img.numpy().astype(np.uint8)
                
                # è°ƒæ•´å¤§å°
                mouth_img = cv2.resize(mouth_img, (self.x2 - self.x1, self.y2 - self.y1))
                mouth_img = mouth_img[:, :, ::-1]  # RGB to BGR
                
                # èåˆåˆ°èƒŒæ™¯
                full_img = self.bg_data_list[bg_id].copy()
                full_img[self.y1:self.y2, self.x1:self.x2, :] = (
                    mouth_img * (1 - self.merge_mask) +
                    full_img[self.y1:self.y2, self.x1:self.x2, :] * self.merge_mask
                )
                
                frames.append(full_img.astype(np.uint8))
            
            return frames
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ¸²æŸ“å¤±è´¥ (èµ·å§‹å¸§{start_idx}): {e}")
            import traceback
            logger.error(traceback.format_exc())
            # è¿”å›ç©ºå¸§
            return [np.zeros((self.resolution[1], self.resolution[0], 3), dtype=np.uint8) 
                   for _ in range(len(batch_params))]
    
    def _param_to_image(self, params: Dict[str, float], bg_frame_id: int) -> torch.Tensor:
        """å‚æ•°è½¬å˜´éƒ¨å›¾åƒ"""
        # è¾¹ç•Œæ£€æŸ¥
        if not self.ref_img_list or bg_frame_id >= len(self.ref_img_list):
            logger.error(
                f"ref_img_listè®¿é—®è¶Šç•Œ: bg_frame_id={bg_frame_id}, "
                f"ref_img_listé•¿åº¦={len(self.ref_img_list) if self.ref_img_list else 0}"
            )
            # è¿”å›é›¶å¼ é‡é¿å…å´©æºƒ
            return torch.zeros((1, 3, 384, 384))
        
        param_val = np.array([params[key] for key in self.p_list])
        
        # æ£€æµ‹å‚æ•°ä¸­çš„NaN
        if np.isnan(param_val).any():
            logger.warning(f"å£å‹å‚æ•°åŒ…å«NaNï¼Œä½¿ç”¨ä¸­æ€§å€¼æ›¿ä»£")
            param_val = np.nan_to_num(param_val, nan=0.0)
        
        with torch.no_grad():
            # ref_img_list[i]æ˜¯List[Tensor]ï¼Œéœ€è¦æ·»åŠ batchç»´åº¦
            ref_img = [t.unsqueeze(0).to(self.device) for t in self.ref_img_list[bg_frame_id]]
            output = self.generator(
                ref_img,
                torch.from_numpy(param_val).unsqueeze(0).float().to(self.device)
            )
            
            # æ£€æµ‹è¾“å‡ºä¸­çš„NaN
            if torch.isnan(output).any():
                logger.error(f"ç”Ÿæˆå™¨è¾“å‡ºåŒ…å«NaN (bg_frame_id={bg_frame_id})ï¼Œä½¿ç”¨é›¶å¼ é‡æ›¿ä»£")
                output = torch.zeros_like(output)
        
        return output.detach().cpu()
    
    def _merge_mouth_to_bg(self, mouth_image: torch.Tensor, bg_frame_id: int) -> Tuple[np.ndarray, np.ndarray]:
        """èåˆå˜´éƒ¨åˆ°èƒŒæ™¯"""
        # åå½’ä¸€åŒ–
        mouth_image = (mouth_image / 2 + 0.5).clamp(0, 1)
        mouth_image = mouth_image[0].permute(1, 2, 0) * 255
        mouth_image = mouth_image.numpy().astype(np.uint8)
        
        # è°ƒæ•´å¤§å°
        mouth_image = cv2.resize(mouth_image, (self.x2 - self.x1, self.y2 - self.y1))
        mouth_image = mouth_image[:, :, ::-1]  # RGB to BGR
        
        # èåˆ
        full_img = self.bg_data_list[bg_frame_id].copy()
        full_img[self.y1:self.y2, self.x1:self.x2, :] = (
            mouth_image * (1 - self.merge_mask) +
            full_img[self.y1:self.y2, self.x1:self.x2, :] * self.merge_mask
        )
        
        return full_img.astype(np.uint8), mouth_image.astype(np.uint8)
    
    async def _frames_to_video(self, frames: List[np.ndarray], audio_data: bytes) -> bytes:
        """å¸§åºåˆ—åˆæˆè§†é¢‘ï¼ˆä¼˜åŒ–ç‰ˆï¼šFFmpegç®¡é“ç¼–ç  + Fallbackï¼‰"""
        import subprocess
        
        height, width = frames[0].shape[:2]
        
        # å‡†å¤‡éŸ³é¢‘ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:
            headinfo = self._generate_wav_header(16000, 16, len(audio_data))
            tmp_audio.write(headinfo + audio_data)
            audio_path = tmp_audio.name
        
        # è¾“å‡ºè§†é¢‘ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_video:
            video_path = tmp_video.name
        
        try:
            # æ–¹æ³•1ï¼šFFmpegç®¡é“ç¼–ç ï¼ˆæé€Ÿä¼˜åŒ–ï¼‰
            logger.debug("å°è¯•FFmpegç®¡é“ç¼–ç ...")
            cmd = [
                'ffmpeg', '-y',
                '-f', 'rawvideo',
                '-vcodec', 'rawvideo',
                '-pix_fmt', 'bgr24',
                '-s', f'{width}x{height}',
                '-r', str(self.fps),
                '-i', '-',
                '-i', audio_path,
                # âš¡ æé€Ÿç¼–ç ä¼˜åŒ–
                '-c:v', 'libx264',
                '-preset', 'ultrafast',  # æœ€å¿«é¢„è®¾
                '-tune', 'zerolatency',  # é›¶å»¶è¿Ÿè°ƒä¼˜
                '-crf', '30',  # æé«˜åˆ°30ï¼Œé™ä½è´¨é‡æ¢é€Ÿåº¦
                '-g', '999',  # å…³é”®å¸§é—´éš”ï¼Œå‡å°‘ç¼–ç å¤æ‚åº¦
                '-threads', '2',  # é™åˆ¶ç¼–ç çº¿ç¨‹ï¼Œé¿å…æŠ¢å æ¸²æŸ“çº¿ç¨‹
                '-c:a', 'aac',
                '-b:a', '64k',  # é™ä½éŸ³é¢‘æ¯”ç‰¹ç‡
                '-movflags', '+faststart+frag_keyframe',
                '-loglevel', 'error',
                video_path
            ]
            
            # âš¡ ä¼˜åŒ–ï¼šé¢„å…ˆè¿æ¥å¸§æ•°æ®
            try:
                # ä½¿ç”¨numpyè¿ç»­æ•°ç»„åŠ é€Ÿ
                frames_array = np.ascontiguousarray(np.array(frames, dtype=np.uint8))
                frame_bytes = frames_array.tobytes()
            except Exception as e:
                logger.error(f"å¸§æ•°æ®å‡†å¤‡å¤±è´¥: {e}ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                raise
            
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: subprocess.run(cmd, input=frame_bytes, capture_output=True, timeout=30)
            )
            
            if result.returncode != 0:
                stderr_msg = result.stderr.decode() if result.stderr else "Unknown error"
                logger.warning(f"FFmpegç®¡é“ç¼–ç å¤±è´¥: {stderr_msg}ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                raise RuntimeError("FFmpegç®¡é“ç¼–ç å¤±è´¥")
            
            # è¯»å–è§†é¢‘æ•°æ®
            with open(video_path, 'rb') as f:
                video_data = f.read()
            
            if len(video_data) == 0:
                logger.warning("FFmpegç”Ÿæˆç©ºè§†é¢‘ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                raise RuntimeError("ç©ºè§†é¢‘")
            
            logger.debug(f"FFmpegç®¡é“ç¼–ç æˆåŠŸ: {len(video_data)} bytes")
            
        except (FileNotFoundError, subprocess.TimeoutExpired, RuntimeError, Exception) as e:
            # Fallbackï¼šä½¿ç”¨OpenCVç¼–ç  + FFmpegåˆå¹¶éŸ³é¢‘ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
            logger.warning(f"FFmpegç®¡é“å¤±è´¥ ({e})ï¼Œä½¿ç”¨OpenCV fallback")
            video_data = await self._frames_to_video_fallback(frames, audio_data, audio_path, video_path)
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(video_path).unlink(missing_ok=True)
            Path(audio_path).unlink(missing_ok=True)
        
        return video_data
    
    async def _frames_to_video_fallback(self, frames: List[np.ndarray], audio_data: bytes, 
                                       audio_path: str, video_path: str) -> bytes:
        """Fallbackæ–¹æ³•ï¼šOpenCVç¼–ç  + FFmpegåˆå¹¶éŸ³é¢‘"""
        import subprocess
        
        height, width = frames[0].shape[:2]
        
        try:
            # ä½¿ç”¨OpenCVå†™å…¥è§†é¢‘ï¼ˆæ— éŸ³é¢‘ï¼‰
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_video_no_audio:
                video_no_audio_path = tmp_video_no_audio.name
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(video_no_audio_path, fourcc, self.fps, (width, height))
            
            if not out.isOpened():
                logger.error("OpenCV VideoWriteråˆå§‹åŒ–å¤±è´¥")
                raise RuntimeError("OpenCVåˆå§‹åŒ–å¤±è´¥")
            
            for frame in frames:
                out.write(frame)
            out.release()
            
            # ä½¿ç”¨FFmpegåˆå¹¶éŸ³è§†é¢‘
            cmd = [
                'ffmpeg', '-y',
                '-i', video_no_audio_path,
                '-i', audio_path,
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-b:a', '128k',
                '-movflags', 'frag_keyframe+empty_moov',
                '-loglevel', 'error',
                video_path
            ]
            
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: subprocess.run(cmd, capture_output=True, timeout=30)
            )
            
            if result.returncode != 0:
                logger.error(f"FFmpegéŸ³è§†é¢‘åˆå¹¶å¤±è´¥: {result.stderr.decode()}")
                raise RuntimeError("éŸ³è§†é¢‘åˆå¹¶å¤±è´¥")
            
            # è¯»å–è§†é¢‘æ•°æ®
            with open(video_path, 'rb') as f:
                video_data = f.read()
            
            # æ¸…ç†OpenCVä¸´æ—¶æ–‡ä»¶
            Path(video_no_audio_path).unlink(missing_ok=True)
            
            logger.info(f"OpenCV fallbackæˆåŠŸ: {len(video_data)} bytes")
            return video_data
            
        except Exception as e:
            logger.error(f"Fallbackæ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            raise RuntimeError(f"è§†é¢‘åˆæˆå®Œå…¨å¤±è´¥: {e}")
    
    def _generate_wav_header(self, sample_rate: int, bits: int, sample_num: int) -> bytes:
        """ç”ŸæˆWAVæ–‡ä»¶å¤´"""
        import struct
        header = b'\x52\x49\x46\x46'
        file_length = struct.pack('i', sample_num + 36)
        header += file_length
        header += b'\x57\x41\x56\x45\x66\x6D\x74\x20\x10\x00\x00\x00\x01\x00\x01\x00'
        header += struct.pack('i', sample_rate)
        header += struct.pack('i', int(sample_rate * bits / 8))
        header += b'\x02\x00'
        header += struct.pack('H', bits)
        header += b'\x64\x61\x74\x61'
        header += struct.pack('i', sample_num)
        return header
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # åœæ­¢æ¸²æŸ“çº¿ç¨‹
        self.input_queue.put(None)
        for t in self.threads:
            t.join(timeout=5)
        
        # æ¸…ç†æ¨¡å‹
        self.audio2mouth = None
        self.encoder = None
        self.generator = None
        
        self.executor.shutdown(wait=False)
        
        await super().cleanup()
