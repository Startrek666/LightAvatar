"""
Web Search Handler - è”ç½‘æœç´¢å¤„ç†å™¨
ä½¿ç”¨ DuckDuckGo æœç´¢å¹¶æå–ç½‘é¡µæ­£æ–‡å†…å®¹
"""

import asyncio
from typing import List, Dict, Optional, AsyncGenerator, Callable, Tuple
import httpx
from loguru import logger
from backend.handlers.base import BaseHandler


class WebSearchHandler(BaseHandler):
    """è”ç½‘æœç´¢å¤„ç†å™¨"""
    
    async def _setup(self):
        """åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯"""
        try:
            # ä¼˜å…ˆå°è¯•æ–°åŒ…å
            try:
                from ddgs import DDGS
                logger.info("Using ddgs package (recommended)")
            except ImportError:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæ—§åŒ…å
                from duckduckgo_search import DDGS
                logger.warning("Using deprecated duckduckgo_search package. Consider upgrading: pip install ddgs")
            
            # ä¿å­˜ DDGS ç±»å¼•ç”¨ï¼Œé¿å…åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å…±äº«åŒä¸€å®ä¾‹
            self.ddgs_class = DDGS
            
        except ImportError as e:
            logger.error(f"Search package not installed: {e}")
            logger.error("Run: pip install ddgs")
            raise
        
        # HTTP å®¢æˆ·ç«¯ç”¨äºè·å–ç½‘é¡µå†…å®¹
        self.http_client = httpx.AsyncClient(
            timeout=15.0,
            follow_redirects=True,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        
        self.max_results = self.config.get('max_results', 5)
        self.fetch_content = self.config.get('fetch_content', True)
        self.content_max_length = self.config.get('content_max_length', 2000)
        
        logger.info(f"WebSearchHandler initialized: max_results={self.max_results}, fetch_content={self.fetch_content}")
    
    def _run_ddgs_search(self, params: Dict) -> List[Dict]:
        """åœ¨ç‹¬ç«‹å®¢æˆ·ç«¯ä¸­æ‰§è¡Œ DuckDuckGo æœç´¢ï¼Œé¿å…å…±äº«çŠ¶æ€"""
        with self.ddgs_class() as ddgs:
            logger.debug(f"Executing DDGS.text with params: {params}")
            return list(ddgs.text(**params))

    async def search_with_progress(
        self, 
        query: str, 
        max_results: Optional[int] = None,
        progress_callback: Optional[callable] = None
    ) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (step, total, message)
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        max_results = max_results or self.max_results
        
        try:
            # æ­¥éª¤1: å¼€å§‹æœç´¢
            if progress_callback:
                await progress_callback(1, 4, f"æ­£åœ¨æœç´¢: {query}")
            
            logger.info(f"Searching for: {query} (max_results={max_results})")

            # æœç´¢ç­–ç•¥åˆ†ä¸¤é˜¶æ®µï¼š
            # 1. çº¯ä¸­æ–‡æœç´¢ï¼ˆä¼˜å…ˆï¼‰
            # 2. å¤±è´¥æ—¶ï¼Œæ·»åŠ è‹±æ–‡å…³é”®è¯ä½œä¸ºè¡¥å……

            search_strategies: List[Tuple[str, Dict]] = []

            # ç­–ç•¥Aï¼šåŸå§‹ä¸­æ–‡æŸ¥è¯¢
            chinese_params = {
                'query': query,
                'max_results': max_results,
                'region': 'cn-zh',
                'safesearch': 'moderate',
                'timelimit': 'm'
            }
            search_strategies.append(('ä¸­æ–‡åŸå§‹æŸ¥è¯¢', chinese_params))

            # ç­–ç•¥Bï¼šè‹±æ–‡å¢å¼ºæŸ¥è¯¢ï¼ˆå¤‡é€‰ï¼Œç”¨äºæ²¡æœ‰ä¸­æ–‡æœ‰æ•ˆç»“æœæ—¶ï¼‰
            optimized_query = self._optimize_search_query(query)
            if optimized_query != query:
                english_params = chinese_params.copy()
                english_params['query'] = optimized_query
                search_strategies.append(('ä¸­æ–‡+è‹±æ–‡å¢å¼º', english_params))

            results = []
            used_strategy = None

            for strategy_name, params in search_strategies:
                try:
                    logger.info(f"å°è¯•æœç´¢ç­–ç•¥ï¼š{strategy_name} -> {params['query']}")
                    strategy_results = await asyncio.to_thread(self._run_ddgs_search, params)
                    logger.info(f" {strategy_name} è¿”å› {len(strategy_results)} æ¡ç»“æœ")

                    if strategy_results:
                        results = strategy_results
                        used_strategy = strategy_name
                        break

                    # æ— ç»“æœæ—¶å›é€€ï¼šç§»é™¤ timelimit å†è¯•ä¸€æ¬¡
                    logger.warning(f" {strategy_name} æ— ç»“æœï¼Œå°è¯•ç§»é™¤ timelimit")
                    fallback_params = params.copy()
                    fallback_params.pop('timelimit', None)
                    logger.debug(f" {strategy_name} fallback params: {fallback_params}")
                    strategy_results = await asyncio.to_thread(self._run_ddgs_search, fallback_params)
                    logger.info(f" {strategy_name}ï¼ˆæ— æ—¶é—´é™åˆ¶ï¼‰è¿”å› {len(strategy_results)} æ¡ç»“æœ")

                    if strategy_results:
                        results = strategy_results
                        used_strategy = f"{strategy_name} (æ—  timelimit)"
                        break

                    # ä»æ— ç»“æœæ—¶ï¼Œå°è¯•ç§»é™¤ region å’Œ safesearch
                    logger.warning(f" {strategy_name}ï¼ˆæ— æ—¶é—´é™åˆ¶ï¼‰ä»æ— ç»“æœï¼Œç§»é™¤ region/safesearch å†è¯•")
                    fallback_params.pop('region', None)
                    fallback_params.pop('safesearch', None)
                    logger.debug(f" {strategy_name} fallback params (no region/safesearch): {fallback_params}")
                    strategy_results = await asyncio.to_thread(self._run_ddgs_search, fallback_params)
                    logger.info(f" {strategy_name}ï¼ˆæ— æ—¶é—´é™åˆ¶/æ— åŒºåŸŸé™åˆ¶ï¼‰è¿”å› {len(strategy_results)} æ¡ç»“æœ")

                    if strategy_results:
                        results = strategy_results
                        used_strategy = f"{strategy_name} (æ—  timelimit/region/safesearch)"
                        break

                except Exception as search_error:
                    logger.error(f"ç­–ç•¥ {strategy_name} æœç´¢å¤±è´¥: {search_error}", exc_info=True)
                    continue

            if used_strategy:
                logger.info(f"ä½¿ç”¨æœç´¢ç­–ç•¥: {used_strategy}")
            else:
                logger.warning("æ‰€æœ‰ç­–ç•¥å‡æœªè·å¾—æœ‰æ•ˆç»“æœ")
            
            # æ­¥éª¤2: è·å–åˆ°æœç´¢ç»“æœ
            if progress_callback:
                await progress_callback(2, 4, f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            
            logger.info(f"Found {len(results)} search results")
            
            # æ­¥éª¤3: æå–ç½‘é¡µå†…å®¹
            formatted_results = []
            for i, r in enumerate(results, 1):
                result = {
                    'title': r.get('title', ''),
                    'url': r.get('href', ''),
                    'snippet': r.get('body', ''),
                }
                
                logger.info(f"\n{'='*80}")
                logger.info(f"æœç´¢ç»“æœ #{i}:")
                logger.info(f"  æ ‡é¢˜: {result['title']}")
                logger.info(f"  é“¾æ¥: {result['url']}")
                logger.info(f"  æ‘˜è¦: {result['snippet'][:200]}...")
                
                # å¦‚æœéœ€è¦è·å–æ­£æ–‡å†…å®¹
                if self.fetch_content:
                    if progress_callback:
                        await progress_callback(3, 4, f"æ­£åœ¨æå–ç¬¬ {i}/{len(results)} ä¸ªç½‘é¡µå†…å®¹")
                    
                    content = await self._fetch_page_content(result['url'])
                    result['content'] = content
                    
                    # è®°å½•æå–çš„å†…å®¹
                    if content:
                        logger.info(f"  âœ… æˆåŠŸæå–æ­£æ–‡å†…å®¹ ({len(content)} å­—ç¬¦)")
                        logger.info(f"  å†…å®¹é¢„è§ˆ: {content[:300]}...")
                        logger.debug(f"  å®Œæ•´å†…å®¹:\n{content}")
                    else:
                        logger.warning(f"  âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹")
                
                formatted_results.append(result)
                logger.info(f"{'='*80}\n")
            
            # æ­¥éª¤4: å®Œæˆ
            if progress_callback:
                await progress_callback(4, 4, "æœç´¢å®Œæˆ")
            
            # æ±‡æ€»æ—¥å¿—
            logger.info(f"\n{'#'*80}")
            logger.info(f"ğŸ“Š æœç´¢æ±‡æ€»:")
            logger.info(f"  æŸ¥è¯¢: {query}")
            logger.info(f"  ä¼˜åŒ–æŸ¥è¯¢: {optimized_query}")
            logger.info(f"  ç»“æœæ•°é‡: {len(formatted_results)}")
            
            total_content_length = sum(len(r.get('content', '')) for r in formatted_results)
            logger.info(f"  æ€»å†…å®¹é•¿åº¦: {total_content_length} å­—ç¬¦")
            
            # åˆ—å‡ºæ‰€æœ‰ç»“æœçš„æ ‡é¢˜
            logger.info(f"  ç»“æœåˆ—è¡¨:")
            for i, r in enumerate(formatted_results, 1):
                has_content = 'âœ“' if r.get('content') else 'âœ—'
                logger.info(f"    {i}. [{has_content}] {r['title'][:60]}...")
            
            logger.info(f"{'#'*80}\n")
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            if progress_callback:
                await progress_callback(0, 4, f"æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def process(self, query: str) -> List[Dict]:
        """
        å¤„ç†æœç´¢è¯·æ±‚ï¼ˆå®ç° BaseHandler çš„æŠ½è±¡æ–¹æ³•ï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        return await self.search(query)
    
    def _optimize_search_query(self, query: str) -> str:
        """
        ä¼˜åŒ–æœç´¢å…³é”®è¯ï¼Œä¸ºæŠ€æœ¯æœ¯è¯­æ·»åŠ è‹±æ–‡å…³é”®è¯ä»¥æ”¹å–„æœç´¢ç»“æœ
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        # æŠ€æœ¯æœ¯è¯­æ˜ å°„ï¼ˆä¸­æ–‡ -> è‹±æ–‡ï¼‰
        tech_terms = {
            'å¼€æºå¤§æ¨¡å‹': 'open source LLM',
            'å¤§æ¨¡å‹': 'large language model LLM',
            'AI': 'artificial intelligence',
            'äººå·¥æ™ºèƒ½': 'AI',
            'æ·±åº¦å­¦ä¹ ': 'deep learning',
            'æœºå™¨å­¦ä¹ ': 'machine learning',
            'ç¥ç»ç½‘ç»œ': 'neural network',
            'ChatGPT': 'ChatGPT OpenAI',
            'Python': 'Python programming',
            'JavaScript': 'JavaScript',
            'åŒºå—é“¾': 'blockchain',
            'åŠ å¯†è´§å¸': 'cryptocurrency'
        }
        
        optimized = query
        for cn_term, en_term in tech_terms.items():
            if cn_term in query and en_term not in query:
                # æ·»åŠ è‹±æ–‡æœ¯è¯­ï¼Œä½†ä¿æŒåŸæŸ¥è¯¢
                optimized = f"{query} {en_term}"
                break  # åªæ·»åŠ ç¬¬ä¸€ä¸ªåŒ¹é…çš„æœ¯è¯­
        
        return optimized
    
    async def search(self, query: str, max_results: Optional[int] = None) -> List[Dict]:
        """
        æ‰§è¡Œæœç´¢ï¼ˆä¸å¸¦è¿›åº¦å›è°ƒï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        return await self.search_with_progress(query, max_results, None)
    
    async def _fetch_page_content(self, url: str) -> str:
        """
        è·å–ç½‘é¡µæ­£æ–‡å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            æå–çš„æ­£æ–‡å†…å®¹
        """
        try:
            # ä¸‹è½½ç½‘é¡µ
            logger.debug(f"Fetching content from: {url}")
            response = await self.http_client.get(url)
            html = response.text
            
            # å°è¯•ä½¿ç”¨ trafilatura æå–æ­£æ–‡
            try:
                import trafilatura
                content = trafilatura.extract(
                    html,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False
                )
                
                if content:
                    # é™åˆ¶é•¿åº¦
                    if len(content) > self.content_max_length:
                        content = content[:self.content_max_length] + "..."
                    return content
            except ImportError:
                logger.warning("trafilatura not installed, using simple text extraction")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•æ–‡æœ¬æå–
            from html.parser import HTMLParser
            
            class TextExtractor(HTMLParser):
                def __init__(self):
                    super().__init__()
                    self.text = []
                
                def handle_data(self, data):
                    if data.strip():
                        self.text.append(data.strip())
            
            extractor = TextExtractor()
            extractor.feed(html)
            content = ' '.join(extractor.text)
            
            if len(content) > self.content_max_length:
                content = content[:self.content_max_length] + "..."
            
            return content if content else ""
                
        except Exception as e:
            logger.warning(f"Failed to fetch content from {url}: {e}")
            return ""
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'http_client'):
            await self.http_client.aclose()

