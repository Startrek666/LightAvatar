"""
Momo Search Multi-Agent Framework
å¤šAgentåä½œæœç´¢æ¡†æ¶
"""
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import time
from loguru import logger


class AgentStatus(Enum):
    """AgentçŠ¶æ€"""
    IDLE = "idle"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class AgentMessage:
    """Agentä¹‹é—´çš„æ¶ˆæ¯"""
    sender: str
    receiver: str
    message_type: str
    data: Dict[str, Any]
    timestamp: float = field(default_factory=time.time)


class BaseAgent(ABC):
    """åŸºç¡€Agentç±»"""
    
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self.status = AgentStatus.IDLE
        self.message_queue: asyncio.Queue = asyncio.Queue()
        self.result: Optional[Any] = None
        self.error: Optional[str] = None
        
    @abstractmethod
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å¤„ç†è¾“å…¥æ•°æ®
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«å…¶ä»–Agentçš„ç»“æœï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        pass
    
    async def send_message(self, receiver: 'BaseAgent', message_type: str, data: Dict[str, Any]):
        """å‘å…¶ä»–Agentå‘é€æ¶ˆæ¯"""
        message = AgentMessage(
            sender=self.name,
            receiver=receiver.name,
            message_type=message_type,
            data=data
        )
        await receiver.message_queue.put(message)
        logger.debug(f"ğŸ“¨ [{self.name}] -> [{receiver.name}]: {message_type}")
    
    async def receive_message(self, timeout: float = None) -> Optional[AgentMessage]:
        """æ¥æ”¶æ¶ˆæ¯"""
        try:
            return await asyncio.wait_for(self.message_queue.get(), timeout=timeout)
        except asyncio.TimeoutError:
            return None
    
    def set_status(self, status: AgentStatus):
        """è®¾ç½®çŠ¶æ€"""
        self.status = status
        logger.debug(f"ğŸ¤– [{self.name}] çŠ¶æ€: {status.value}")
    
    def reset(self):
        """é‡ç½®AgentçŠ¶æ€"""
        self.status = AgentStatus.IDLE
        self.result = None
        self.error = None
        # æ¸…ç©ºæ¶ˆæ¯é˜Ÿåˆ—
        while not self.message_queue.empty():
            try:
                self.message_queue.get_nowait()
            except:
                pass


class KeywordExtractionAgent(BaseAgent):
    """å…³é”®è¯æå–Agent"""
    
    def __init__(self, zhipu_api_key: str, zhipu_model: str = "glm-4.5-flash"):
        super().__init__(
            name="keyword_extractor",
            description="æå–æœç´¢å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰"
        )
        self.zhipu_api_key = zhipu_api_key
        self.zhipu_model = zhipu_model
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æå–å…³é”®è¯"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            if not query:
                raise ValueError("æŸ¥è¯¢ä¸ºç©º")
            
            from .momo_utils import extract_keywords
            
            logger.info(f"ğŸ”‘ [{self.name}] å¼€å§‹æå–å…³é”®è¯: {query}")
            keywords_dict = extract_keywords(
                query,
                api_key=self.zhipu_api_key,
                model=self.zhipu_model
            )
            
            if keywords_dict:
                zh_keys = keywords_dict.get("zh_keys", "").strip()
                en_keys = keywords_dict.get("en_keys", "").strip()
                
                result = {
                    "success": True,
                    "keywords": {
                        "zh": zh_keys,
                        "en": en_keys
                    },
                    "raw": keywords_dict
                }
                
                logger.info(f"âœ… [{self.name}] æå–æˆåŠŸ: ä¸­æ–‡={zh_keys}, è‹±æ–‡={en_keys}")
            else:
                result = {
                    "success": False,
                    "keywords": None,
                    "message": "å…³é”®è¯æå–å¤±è´¥"
                }
                logger.warning(f"âš ï¸ [{self.name}] æå–å¤±è´¥")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }


class SearchAgent(BaseAgent):
    """æœç´¢Agent - è´Ÿè´£æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢"""
    
    def __init__(self, searxng_url: str, searxng_language: str = "zh", 
                 searxng_time_range: str = "day", max_results: int = 50):
        super().__init__(
            name="searcher",
            description="æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢ï¼ˆSearXNG + DuckDuckGoï¼‰"
        )
        self.searxng_url = searxng_url
        self.searxng_language = searxng_language
        self.searxng_time_range = searxng_time_range
        self.max_results = max_results
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            queries = input_data.get("queries", [])  # [{query, language, source}, ...]
            all_results = []
            
            from .momo_utils import search_searxng, search_duckduckgo, SearchDocument
            
            # æ‰§è¡ŒSearXNGæœç´¢
            for search_item in queries:
                if search_item.get("source", "").startswith("ddg"):
                    continue  # DuckDuckGoæŸ¥è¯¢è·³è¿‡
                
                logger.info(f"ğŸ” [{self.name}] SearXNGæœç´¢: {search_item['query']} ({search_item['language']})")
                
                results = search_searxng(
                    query=search_item['query'],
                    num_results=self.max_results,
                    ip_address=self.searxng_url,
                    language=search_item['language'],
                    time_range=self.searxng_time_range,
                    deduplicate_by_url=True
                )
                
                # å»é‡åˆå¹¶
                seen_urls = {doc.url for doc in all_results}
                for doc in results:
                    if doc.url not in seen_urls:
                        all_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… [{self.name}] SearXNGå®Œæˆ: +{len(results)}ä¸ªç»“æœ, æ€»è®¡{len(all_results)}ä¸ª")
            
            # æ‰§è¡ŒDuckDuckGoæœç´¢
            ddg_queries = [q for q in queries if q.get("source", "").startswith("ddg")]
            for ddg_item in ddg_queries:
                logger.info(f"ğŸ¦† [{self.name}] DuckDuckGoæœç´¢: {ddg_item['query']} ({ddg_item['language']})")
                
                ddg_results = await search_duckduckgo(
                    query=ddg_item['query'],
                    max_results=20,
                    language=ddg_item['language'],
                    time_range=self.searxng_time_range if self.searxng_time_range else None
                )
                
                # å»é‡åˆå¹¶
                seen_urls = {doc.url for doc in all_results}
                for doc in ddg_results:
                    if doc.url not in seen_urls:
                        all_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… [{self.name}] DuckDuckGoå®Œæˆ: +{len(ddg_results)}ä¸ªç»“æœ, æ€»è®¡{len(all_results)}ä¸ª")
            
            result = {
                "success": True,
                "results": all_results,
                "count": len(all_results)
            }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }


class RetrievalAgent(BaseAgent):
    """æ£€ç´¢Agent - è´Ÿè´£å‘é‡æ£€ç´¢å’Œç›¸å…³æ€§åˆ†æ"""
    
    def __init__(self, retriever, sim_threshold: float = 0.45):
        super().__init__(
            name="retriever",
            description="å‘é‡æ£€ç´¢å’Œç›¸å…³æ€§åˆ†æ"
        )
        self.retriever = retriever
        self.sim_threshold = sim_threshold
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå‘é‡æ£€ç´¢"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            documents = input_data.get("documents", [])
            
            if not query or not documents:
                raise ValueError("æŸ¥è¯¢æˆ–æ–‡æ¡£ä¸ºç©º")
            
            logger.info(f"ğŸ“Š [{self.name}] å¼€å§‹åˆ†æç›¸å…³æ€§: {len(documents)}ä¸ªæ–‡æ¡£")
            
            # æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨
            self.retriever.add_documents(documents)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = self.retriever.get_relevant_documents(query)
            
            if not relevant_docs:
                logger.warning(f"âš ï¸ [{self.name}] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                result = {
                    "success": False,
                    "results": [],
                    "count": 0,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                }
            else:
                logger.info(f"âœ… [{self.name}] æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
                result = {
                    "success": True,
                    "results": relevant_docs,
                    "count": len(relevant_docs)
                }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }


class CrawlerAgent(BaseAgent):
    """çˆ¬å–Agent - è´Ÿè´£æ·±åº¦çˆ¬å–ç½‘é¡µå†…å®¹"""
    
    def __init__(self, crawler, score_threshold: float = 0.5, max_docs: int = 10):
        super().__init__(
            name="crawler",
            description="æ·±åº¦çˆ¬å–ç½‘é¡µå†…å®¹"
        )
        self.crawler = crawler
        self.score_threshold = score_threshold
        self.max_docs = max_docs
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦çˆ¬å–"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            documents = input_data.get("documents", [])
            
            if not documents:
                logger.warning(f"âš ï¸ [{self.name}] æ— æ–‡æ¡£éœ€è¦çˆ¬å–")
                return {
                    "success": True,
                    "results": [],
                    "count": 0
                }
            
            logger.info(f"ğŸ•·ï¸ [{self.name}] å¼€å§‹æ·±åº¦çˆ¬å–: {len(documents)}ä¸ªæ–‡æ¡£")
            
            # æ‰§è¡Œçˆ¬å–
            await self.crawler.crawl_many(
                documents,
                score_threshold=self.score_threshold,
                max_docs=self.max_docs
            )
            
            # çˆ¬å–åçš„æ–‡æ¡£ï¼ˆcrawlerä¼šæ›´æ–°æ–‡æ¡£çš„contentå­—æ®µï¼‰
            result = {
                "success": True,
                "results": documents,
                "count": len(documents)
            }
            
            logger.info(f"âœ… [{self.name}] çˆ¬å–å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": documents if 'documents' in locals() else []
            }


class DocumentProcessorAgent(BaseAgent):
    """æ–‡æ¡£å¤„ç†Agent - è´Ÿè´£æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢"""
    
    def __init__(self, retriever):
        super().__init__(
            name="document_processor",
            description="æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢"
        )
        self.retriever = retriever
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            documents = input_data.get("documents", [])
            
            if not query or not documents:
                raise ValueError("æŸ¥è¯¢æˆ–æ–‡æ¡£ä¸ºç©º")
            
            logger.info(f"âœ‚ï¸ [{self.name}] å¼€å§‹æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢: {len(documents)}ä¸ªæ–‡æ¡£")
            
            from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url
            
            # æ–‡æ¡£åˆ†å—
            docs_with_details = expand_docs_by_text_split(documents)
            
            # æ·»åŠ åˆ°æ£€ç´¢å™¨
            self.retriever.add_documents(docs_with_details)
            
            # äºŒæ¬¡æ£€ç´¢
            relevant_docs_detailed = self.retriever.get_relevant_documents(query)
            
            # åˆå¹¶æ–‡æ¡£
            relevant_docs = merge_docs_by_url(relevant_docs_detailed)
            
            logger.info(f"âœ… [{self.name}] å¤„ç†å®Œæˆ: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            result = {
                "success": True,
                "results": relevant_docs,
                "count": len(relevant_docs)
            }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": documents if 'documents' in locals() else []
            }


class SearchOrchestrator:
    """æœç´¢åè°ƒå™¨ - ç®¡ç†å¤šä¸ªAgentçš„åä½œ"""
    
    def __init__(self, agents: Dict[str, BaseAgent], progress_callback: Optional[Callable] = None):
        self.agents = agents
        self.progress_callback = progress_callback
        self.total_steps = 0
        self.current_step = 0
    
    async def execute(self, query: str, mode: str = "speed", detected_lang: str = "zh") -> tuple[List, str]:
        """
        æ‰§è¡Œå¤šAgentåä½œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            detected_lang: æ£€æµ‹åˆ°çš„è¯­è¨€
            
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯)
        """
        try:
            # è®¡ç®—æ€»æ­¥éª¤æ•°
            self._calculate_steps(mode)
            
            # Agent 1: å…³é”®è¯æå–
            keyword_agent = self.agents.get("keyword_extractor")
            if keyword_agent:
                await self._report_progress(0, "ğŸ”‘ æå–æœç´¢å…³é”®è¯")
                keyword_result = await keyword_agent.process({"query": query})
                
                if not keyword_result.get("success"):
                    logger.warning("å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                    keyword_result = {"keywords": {"zh": query, "en": ""}}
            else:
                keyword_result = {"keywords": {"zh": query, "en": ""}}
            
            # å‡†å¤‡æœç´¢æŸ¥è¯¢åˆ—è¡¨
            search_queries = []
            keywords = keyword_result.get("keywords", {})
            
            if keywords.get("zh"):
                search_queries.append({
                    "query": keywords["zh"],
                    "language": "zh",
                    "source": "keywords_zh"
                })
            
            if keywords.get("en"):
                search_queries.append({
                    "query": keywords["en"],
                    "language": "en",
                    "source": "keywords_en"
                })
            
            if not search_queries:
                search_queries.append({
                    "query": query,
                    "language": detected_lang,
                    "source": "original"
                })
            
            # å‡†å¤‡DuckDuckGoæŸ¥è¯¢
            ddg_queries = []
            if keywords.get("zh"):
                ddg_queries.append({
                    "query": keywords["zh"],
                    "language": "zh",
                    "source": "ddg_zh"
                })
            if keywords.get("en"):
                ddg_queries.append({
                    "query": keywords["en"],
                    "language": "en",
                    "source": "ddg_en"
                })
            elif detected_lang == "zh":
                # å°è¯•ç¿»è¯‘
                from .momo_utils import translate_text
                translated = translate_text(query, source="zh", target="en")
                if translated:
                    ddg_queries.append({
                        "query": translated,
                        "language": "en",
                        "source": "ddg_en_translated"
                    })
            
            # Agent 2: æœç´¢
            search_agent = self.agents.get("searcher")
            # åˆå§‹åŒ–æ­¥éª¤è®¡æ•°å™¨
            current_step = 1
            all_documents = []
            
            if search_agent:
                # é€ä¸ªæ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
                
                # æ‰§è¡ŒSearXNGæœç´¢
                for sq in search_queries:
                    await self._report_progress(
                        current_step,
                        f"ğŸ” æ­£åœ¨æœç´¢: {sq['query']} ({sq['source']})"
                    )
                    # å•ä¸ªæŸ¥è¯¢æœç´¢
                    single_result = await search_agent.process({"queries": [sq]})
                    docs = single_result.get("results", [])
                    # å»é‡åˆå¹¶
                    seen_urls = {doc.url for doc in all_documents}
                    for doc in docs:
                        if doc.url not in seen_urls:
                            all_documents.append(doc)
                            seen_urls.add(doc.url)
                    current_step += 1
                
                # æ‰§è¡ŒDuckDuckGoæœç´¢
                for dq in ddg_queries:
                    if dq['language'] == 'zh':
                        message = "æ­£åœ¨è¿›ä¸€æ­¥æ·±åº¦æœç´¢..."
                    else:
                        message = "æ­£åœ¨æ‰©å……æœç´¢è‹±è¯­èµ„æ–™..."
                    await self._report_progress(current_step, message)
                    # å•ä¸ªæŸ¥è¯¢æœç´¢
                    single_result = await search_agent.process({"queries": [dq]})
                    docs = single_result.get("results", [])
                    # å»é‡åˆå¹¶
                    seen_urls = {doc.url for doc in all_documents}
                    for doc in docs:
                        if doc.url not in seen_urls:
                            all_documents.append(doc)
                            seen_urls.add(doc.url)
                    current_step += 1
            else:
                all_documents = []
            
            if not all_documents:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœ")
                return [], ""
            
            # Agent 3: å‘é‡æ£€ç´¢
            retrieval_agent = self.agents.get("retriever")
            if retrieval_agent:
                vector_step = current_step
                await self._report_progress(
                    vector_step,
                    f"ğŸ“Š åˆ†æç›¸å…³æ€§ ({len(all_documents)}ä¸ªç»“æœ)"
                )
                
                retrieval_result = await retrieval_agent.process({
                    "query": query,
                    "documents": all_documents
                })
                relevant_docs = retrieval_result.get("results", [])
            else:
                relevant_docs = all_documents
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], ""
            
            # Agent 4: æ·±åº¦çˆ¬å–ï¼ˆä»…qualityæ¨¡å¼ï¼‰
            if mode == "quality":
                crawler_agent = self.agents.get("crawler")
                if crawler_agent:
                    crawl_step = vector_step + 1
                    await self._report_progress(
                        crawl_step,
                        f"ğŸ•·ï¸ æ·±åº¦çˆ¬å–å†…å®¹ (å‰{len(relevant_docs)}ä¸ª)"
                    )
                    
                    await crawler_agent.process({"documents": relevant_docs})
                    
                    # Agent 5: æ–‡æ¡£å¤„ç†
                    processor_agent = self.agents.get("document_processor")
                    if processor_agent:
                        split_step = crawl_step + 1
                        await self._report_progress(split_step, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                        
                        processor_result = await processor_agent.process({
                            "query": query,
                            "documents": relevant_docs
                        })
                        relevant_docs = processor_result.get("results", relevant_docs)
            
            # å®Œæˆ
            final_step = self.total_steps
            await self._report_progress(final_step, "âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå†…å®¹")
            await self._report_progress(final_step + 1, f"æ‰¾åˆ°{len(relevant_docs)}ç¯‡ç›¸å…³æ–‡æ¡£")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯ï¼ˆä½¿ç”¨é™æ€æ–¹æ³•æˆ–ç›´æ¥å®ç°ï¼‰
            citations = self._format_citations(relevant_docs)
            
            logger.info(f"âœ… å¤šAgentæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£")
            return relevant_docs, citations
            
        except Exception as e:
            logger.error(f"âŒ å¤šAgentæœç´¢å¤±è´¥: {e}", exc_info=True)
            return [], ""
    
    def _calculate_steps(self, mode: str):
        """è®¡ç®—æ€»æ­¥éª¤æ•°"""
        base_steps = 5  # å…³é”®è¯(1) + å‘é‡æ£€ç´¢(1) + æ·±åº¦çˆ¬å–(1) + æ–‡æ¡£åˆ†å—(1) + å®Œæˆ(1)
        search_steps = 2  # ä¼°ç®—ï¼šä¸­æ–‡+è‹±æ–‡å…³é”®è¯æœç´¢
        ddg_steps = 2  # DuckDuckGoä¸­è‹±æ–‡
        
        if mode == "quality":
            self.total_steps = base_steps + search_steps + ddg_steps
        else:
            self.total_steps = base_steps + search_steps + ddg_steps - 2  # æ— çˆ¬å–å’Œåˆ†å—
        
        self.current_step = 0
    
    async def _report_progress(self, step: int, message: str):
        """æŠ¥å‘Šè¿›åº¦"""
        self.current_step = step
        if self.progress_callback:
            await self.progress_callback(step, self.total_steps, message)
    
    def _format_citations(self, documents: List) -> str:
        """ç”Ÿæˆå¼•ç”¨ä¿¡æ¯"""
        if not documents:
            return ""
        
        citations = []
        for idx, doc in enumerate(documents[:10], 1):  # æœ€å¤š10ä¸ªå¼•ç”¨
            title = doc.title if hasattr(doc, 'title') else 'N/A'
            url = doc.url if hasattr(doc, 'url') else 'N/A'
            citations.append(f"{idx}. [{title}]({url})")
        
        return "\n".join(citations)

