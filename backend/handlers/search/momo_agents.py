"""
Momo Search Multi-Agent Framework
å¤šAgentåä½œæœç´¢æ¡†æ¶
"""
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import time
from loguru import logger


class AgentStatus(Enum):
    """AgentçŠ¶æ€"""
    IDLE = "idle"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class AgentMessage:
    """Agentä¹‹é—´çš„æ¶ˆæ¯"""
    sender: str
    receiver: str
    message_type: str
    data: Dict[str, Any]
    timestamp: float = field(default_factory=time.time)


class BaseAgent(ABC):
    """åŸºç¡€Agentç±»"""
    
    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self.status = AgentStatus.IDLE
        self.message_queue: asyncio.Queue = asyncio.Queue()
        self.result: Optional[Any] = None
        self.error: Optional[str] = None
        
    @abstractmethod
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        å¤„ç†è¾“å…¥æ•°æ®
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«å…¶ä»–Agentçš„ç»“æœï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        pass
    
    async def send_message(self, receiver: 'BaseAgent', message_type: str, data: Dict[str, Any]):
        """å‘å…¶ä»–Agentå‘é€æ¶ˆæ¯"""
        message = AgentMessage(
            sender=self.name,
            receiver=receiver.name,
            message_type=message_type,
            data=data
        )
        await receiver.message_queue.put(message)
        logger.debug(f"ğŸ“¨ [{self.name}] -> [{receiver.name}]: {message_type}")
    
    async def receive_message(self, timeout: float = None) -> Optional[AgentMessage]:
        """æ¥æ”¶æ¶ˆæ¯"""
        try:
            return await asyncio.wait_for(self.message_queue.get(), timeout=timeout)
        except asyncio.TimeoutError:
            return None
    
    def set_status(self, status: AgentStatus):
        """è®¾ç½®çŠ¶æ€"""
        self.status = status
        logger.debug(f"ğŸ¤– [{self.name}] çŠ¶æ€: {status.value}")
    
    def reset(self):
        """é‡ç½®AgentçŠ¶æ€"""
        self.status = AgentStatus.IDLE
        self.result = None
        self.error = None
        # æ¸…ç©ºæ¶ˆæ¯é˜Ÿåˆ—
        while not self.message_queue.empty():
            try:
                self.message_queue.get_nowait()
            except:
                pass


class KeywordExtractionAgent(BaseAgent):
    """å…³é”®è¯æå–Agent"""
    
    def __init__(self, zhipu_api_key: str, zhipu_model: str = "glm-4.5-flash"):
        super().__init__(
            name="keyword_extractor",
            description="æå–æœç´¢å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰"
        )
        self.zhipu_api_key = zhipu_api_key
        self.zhipu_model = zhipu_model
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æå–å…³é”®è¯"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            if not query:
                raise ValueError("æŸ¥è¯¢ä¸ºç©º")
            
            from .momo_utils import extract_keywords
            
            logger.info(f"[{self.name}] å¼€å§‹æå–å…³é”®è¯: {query}")
            keywords_dict = extract_keywords(
                query,
                api_key=self.zhipu_api_key,
                model=self.zhipu_model
            )
            
            if keywords_dict:
                zh_keys = keywords_dict.get("zh_keys", "").strip()
                en_keys = keywords_dict.get("en_keys", "").strip()
                
                result = {
                    "success": True,
                    "keywords": {
                        "zh": zh_keys,
                        "en": en_keys
                    },
                    "raw": keywords_dict
                }
                
                logger.info(f"âœ… [{self.name}] æå–æˆåŠŸ: ä¸­æ–‡={zh_keys}, è‹±æ–‡={en_keys}")
            else:
                result = {
                    "success": False,
                    "keywords": None,
                    "message": "å…³é”®è¯æå–å¤±è´¥"
                }
                logger.warning(f"âš ï¸ [{self.name}] æå–å¤±è´¥")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            logger.info(f"âœ… [{self.name}] Agentå·²å®Œæˆ")
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] Agentå¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }


class SearchAgent(BaseAgent):
    """æœç´¢Agent - è´Ÿè´£æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢"""
    
    def __init__(self, searxng_url: str, searxng_language: str = "zh", 
                 searxng_time_range: str = "day", max_results: int = 50):
        super().__init__(
            name="searcher",
            description="æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢ï¼ˆSearXNG + DuckDuckGoï¼‰"
        )
        self.searxng_url = searxng_url
        self.searxng_language = searxng_language
        self.searxng_time_range = searxng_time_range
        self.max_results = max_results
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            queries = input_data.get("queries", [])  # [{query, language, source}, ...]
            all_results = []
            
            from .momo_utils import search_searxng, search_duckduckgo, SearchDocument
            
            # æ‰§è¡ŒSearXNGæœç´¢
            for search_item in queries:
                if search_item.get("source", "").startswith("ddg"):
                    continue  # DuckDuckGoæŸ¥è¯¢è·³è¿‡
                
                logger.info(f"ğŸ” [{self.name}] SearXNGæœç´¢: {search_item['query']} ({search_item['language']})")
                
                # ä½¿ç”¨æŸ¥è¯¢é¡¹ä¸­æŒ‡å®šçš„max_resultsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                num_results = search_item.get("max_results", self.max_results)
                
                results = search_searxng(
                    query=search_item['query'],
                    num_results=num_results,
                    ip_address=self.searxng_url,
                    language=search_item['language'],
                    time_range=self.searxng_time_range,
                    deduplicate_by_url=True
                )
                
                # å»é‡åˆå¹¶
                seen_urls = {doc.url for doc in all_results}
                for doc in results:
                    if doc.url not in seen_urls:
                        all_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… [{self.name}] SearXNGå®Œæˆ: +{len(results)}ä¸ªç»“æœ, æ€»è®¡{len(all_results)}ä¸ª")
            
            # æ‰§è¡ŒDuckDuckGoæœç´¢
            ddg_queries = [q for q in queries if q.get("source", "").startswith("ddg")]
            for ddg_item in ddg_queries:
                logger.info(f"ğŸ¦† [{self.name}] DuckDuckGoæœç´¢: {ddg_item['query']} ({ddg_item['language']})")
                
                # æ ¹æ®max_resultså‚æ•°å†³å®šç»“æœæ•°é‡ï¼ˆè‹±è¯­40ï¼Œä¸­æ–‡20ï¼‰
                max_results = ddg_item.get("max_results", 20)
                
                ddg_results = await search_duckduckgo(
                    query=ddg_item['query'],
                    max_results=max_results,
                    language=ddg_item['language'],
                    time_range=self.searxng_time_range if self.searxng_time_range else None
                )
                
                # å»é‡åˆå¹¶
                seen_urls = {doc.url for doc in all_results}
                for doc in ddg_results:
                    if doc.url not in seen_urls:
                        all_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… [{self.name}] DuckDuckGoå®Œæˆ: +{len(ddg_results)}ä¸ªç»“æœ, æ€»è®¡{len(all_results)}ä¸ª")
            
            result = {
                "success": True,
                "results": all_results,
                "count": len(all_results)
            }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            logger.info(f"âœ… [{self.name}] Agentå·²å®Œæˆ: æ€»è®¡è·å¾— {len(all_results)} ä¸ªæœç´¢ç»“æœ")
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }


class RetrievalAgent(BaseAgent):
    """æ£€ç´¢Agent - è´Ÿè´£å‘é‡æ£€ç´¢å’Œç›¸å…³æ€§åˆ†æ"""
    
    def __init__(self, retriever, sim_threshold: float = 0.45):
        super().__init__(
            name="retriever",
            description="å‘é‡æ£€ç´¢å’Œç›¸å…³æ€§åˆ†æ"
        )
        self.retriever = retriever
        self.sim_threshold = sim_threshold
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå‘é‡æ£€ç´¢"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            queries = input_data.get("queries", [])  # æ”¯æŒå¤šæŸ¥è¯¢
            documents = input_data.get("documents", [])
            
            # å…¼å®¹æ€§ï¼šå¦‚æœæ²¡æœ‰queriesï¼Œä½¿ç”¨query
            if not queries and query:
                queries = [query]
            
            if not queries or not documents:
                raise ValueError("æŸ¥è¯¢æˆ–æ–‡æ¡£ä¸ºç©º")
            
            logger.info(f"[{self.name}] å¼€å§‹åˆ†æç›¸å…³æ€§: {len(documents)}ä¸ªæ–‡æ¡£, {len(queries)}ä¸ªæŸ¥è¯¢")
            
            # æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨
            self.retriever.add_documents(documents)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒå¤šæŸ¥è¯¢ï¼‰
            if len(queries) > 1:
                # å¤šä¸ªæŸ¥è¯¢ï¼šåˆ†å¼€æ£€ç´¢å¹¶åˆå¹¶ç»“æœ
                relevant_docs = self.retriever.get_relevant_documents_multi_query(queries)
            else:
                # å•ä¸ªæŸ¥è¯¢ï¼šä½¿ç”¨åŸæœ‰æ–¹æ³•
                relevant_docs = self.retriever.get_relevant_documents(queries[0])
            
            if not relevant_docs:
                logger.warning(f"âš ï¸ [{self.name}] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                result = {
                    "success": False,
                    "results": [],
                    "count": 0,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                }
            else:
                logger.info(f"âœ… [{self.name}] æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
                result = {
                    "success": True,
                    "results": relevant_docs,
                    "count": len(relevant_docs)
                }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            logger.info(f"âœ… [{self.name}] Agentå·²å®Œæˆ: æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }


class CrawlerAgent(BaseAgent):
    """çˆ¬å–Agent - è´Ÿè´£æ·±åº¦çˆ¬å–ç½‘é¡µå†…å®¹"""
    
    def __init__(self, crawler, score_threshold: float = 0.5, max_docs: int = 10):
        super().__init__(
            name="crawler",
            description="æ·±åº¦çˆ¬å–ç½‘é¡µå†…å®¹"
        )
        self.crawler = crawler
        self.score_threshold = score_threshold
        self.max_docs = max_docs
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦çˆ¬å–"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            documents = input_data.get("documents", [])
            
            if not documents:
                logger.warning(f"âš ï¸ [{self.name}] æ— æ–‡æ¡£éœ€è¦çˆ¬å–")
                return {
                    "success": True,
                    "results": [],
                    "count": 0
                }
            
            logger.info(f"ğŸ•·ï¸ [{self.name}] å¼€å§‹æ·±åº¦çˆ¬å–: {len(documents)}ä¸ªæ–‡æ¡£")
            
            # æ‰§è¡Œçˆ¬å–
            await self.crawler.crawl_many(
                documents,
                score_threshold=self.score_threshold,
                max_docs=self.max_docs
            )
            
            # çˆ¬å–åçš„æ–‡æ¡£ï¼ˆcrawlerä¼šæ›´æ–°æ–‡æ¡£çš„contentå­—æ®µï¼‰
            result = {
                "success": True,
                "results": documents,
                "count": len(documents)
            }
            
            logger.info(f"âœ… [{self.name}] çˆ¬å–å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            logger.info(f"âœ… [{self.name}] Agentå·²å®Œæˆ: çˆ¬å–äº† {len(documents)} ä¸ªæ–‡æ¡£")
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": documents if 'documents' in locals() else []
            }


class ProblemUnderstandingAgent(BaseAgent):
    """é—®é¢˜ç†è§£Agent - æ·±åº¦ç†è§£ç”¨æˆ·é—®é¢˜"""
    
    def __init__(self, zhipu_api_key: str, zhipu_model: str = "glm-4.5-flash"):
        super().__init__(
            name="problem_understanding",
            description="æ·±åº¦ç†è§£ç”¨æˆ·é—®é¢˜"
        )
        self.zhipu_api_key = zhipu_api_key
        self.zhipu_model = zhipu_model
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç†è§£é—®é¢˜"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            if not query:
                raise ValueError("æŸ¥è¯¢ä¸ºç©º")
            
            from datetime import datetime
            from .momo_utils import call_zhipu_llm
            
            current_date = datetime.now().strftime("%Y-%m-%d")
            
            prompt = f"""ä»Šå¤©æ˜¯{current_date}ã€‚è¯·æ·±å…¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ†æé—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚ã€èƒŒæ™¯å’Œä¸Šä¸‹æ–‡ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ
2. é—®é¢˜çš„èƒŒæ™¯å’Œä¸Šä¸‹æ–‡æ˜¯ä»€ä¹ˆï¼Ÿ
3. ç”¨æˆ·å¯èƒ½æƒ³è¦ä»€ä¹ˆæ ·çš„å›ç­”ï¼Ÿï¼ˆä¿¡æ¯ã€åˆ†æã€å»ºè®®ã€å¯¹æ¯”ç­‰ï¼‰
4. è¿™ä¸ªé—®é¢˜æ¶‰åŠå“ªäº›å…³é”®æ¦‚å¿µå’Œé¢†åŸŸï¼Ÿ

è¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€è¾“å‡ºä½ çš„ç†è§£ï¼Œæ§åˆ¶åœ¨200å­—ä»¥å†…ã€‚"""
            
            logger.info(f"[{self.name}] å¼€å§‹ç†è§£é—®é¢˜: {query}")
            understanding = call_zhipu_llm(
                prompt=prompt,
                api_key=self.zhipu_api_key,
                model=self.zhipu_model,
                temperature=0.7,
                max_tokens=500
            )
            
            if understanding:
                result = {
                    "success": True,
                    "understanding": understanding
                }
                logger.info(f"âœ… [{self.name}] ç†è§£å®Œæˆ")
            else:
                result = {
                    "success": False,
                    "understanding": None,
                    "message": "é—®é¢˜ç†è§£å¤±è´¥"
                }
                logger.warning(f"âš ï¸ [{self.name}] ç†è§£å¤±è´¥")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            self.set_status(AgentStatus.FAILED)
            return {
                "success": False,
                "understanding": None,
                "message": str(e)
            }


class MaterialAnalysisAgent(BaseAgent):
    """èµ„æ–™åˆ†æAgent - æ‰¹åˆ¤æ€§åˆ†ææœç´¢ç»“æœ"""
    
    def __init__(self, zhipu_api_key: str, zhipu_model: str = "glm-4.5-flash", analysis_score_threshold: float = 0.5):
        super().__init__(
            name="material_analysis",
            description="æ‰¹åˆ¤æ€§åˆ†ææœç´¢ç»“æœ"
        )
        self.zhipu_api_key = zhipu_api_key
        self.zhipu_model = zhipu_model
        self.analysis_score_threshold = analysis_score_threshold  # èµ„æ–™åˆ†æçš„ç›¸ä¼¼åº¦é˜ˆå€¼
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æèµ„æ–™"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            documents = input_data.get("documents", [])
            understanding = input_data.get("understanding", "")  # ä»å‰é¢çš„æ­¥éª¤è·å–
            
            if not query or not documents:
                raise ValueError("æŸ¥è¯¢æˆ–æ–‡æ¡£ä¸ºç©º")
            
            from .momo_utils import call_zhipu_llm
            
            # æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿›ä¸€æ­¥ç­›é€‰æ–‡æ¡£ï¼ˆä¸é™åˆ¶æ•°é‡ï¼Œä½†æé«˜è´¨é‡ï¼‰
            filtered_docs = []
            for doc in documents:
                score = getattr(doc, 'score', 0.0)
                if score >= self.analysis_score_threshold:
                    filtered_docs.append(doc)
            
            if not filtered_docs:
                logger.warning(f"âš ï¸ [{self.name}] æ²¡æœ‰æ–‡æ¡£è¾¾åˆ°åˆ†æé˜ˆå€¼ ({self.analysis_score_threshold})ï¼Œä½¿ç”¨æ‰€æœ‰æ–‡æ¡£")
                filtered_docs = documents
            
            # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
            filtered_docs.sort(key=lambda x: getattr(x, 'score', 0.0), reverse=True)
            
            # æ„å»ºèµ„æ–™æ‘˜è¦ï¼ˆä¸é™åˆ¶æ•°é‡ï¼Œä½¿ç”¨æ‰€æœ‰é€šè¿‡é˜ˆå€¼çš„æ–‡æ¡£ï¼‰
            materials_summary = []
            for idx, doc in enumerate(filtered_docs, 1):
                title = doc.title if hasattr(doc, 'title') else 'N/A'
                content = doc.content if hasattr(doc, 'content') else ''
                if not content and hasattr(doc, 'snippet'):
                    content = doc.snippet
                # é™åˆ¶å†…å®¹é•¿åº¦
                content = content[:500] if len(content) > 500 else content
                score = getattr(doc, 'score', 0.0)
                materials_summary.append(f"[èµ„æ–™{idx}] æ ‡é¢˜: {title}\nç›¸ä¼¼åº¦: {score:.3f}\nå†…å®¹: {content}\n")
            
            materials_text = "\n".join(materials_summary)
            
            understanding_context = f"\nä¹‹å‰å¯¹é—®é¢˜çš„ç†è§£ï¼š{understanding}\n" if understanding else ""
            
            prompt = f"""è¯·å¯¹ä»¥ä¸‹æœç´¢ç»“æœè¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}
{understanding_context}
æœç´¢ç»“æœï¼š
{materials_text}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. å“ªäº›èµ„æ–™æœ€ç›¸å…³ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. ä¸åŒèµ„æ–™ä¹‹é—´æœ‰ä»€ä¹ˆä¸€è‡´æ€§å’Œå·®å¼‚ï¼Ÿ
3. èµ„æ–™çš„å¯é æ€§å’Œæƒå¨æ€§å¦‚ä½•ï¼Ÿ
4. å“ªäº›ä¿¡æ¯å¯èƒ½è¿‡æ—¶æˆ–ä¸å‡†ç¡®ï¼Ÿ
5. æ˜¯å¦å­˜åœ¨è§‚ç‚¹å†²çªï¼Ÿå¦‚ä½•ç†è§£è¿™äº›å†²çªï¼Ÿ

è¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€è¾“å‡ºä½ çš„åˆ†æï¼Œæ§åˆ¶åœ¨300å­—ä»¥å†…ã€‚"""
            
            logger.info(f"[{self.name}] å¼€å§‹åˆ†æèµ„æ–™: {len(documents)}ä¸ªæ–‡æ¡£ -> {len(filtered_docs)}ä¸ªæ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.analysis_score_threshold}ï¼‰")
            analysis = call_zhipu_llm(
                prompt=prompt,
                api_key=self.zhipu_api_key,
                model=self.zhipu_model,
                temperature=0.7,
                max_tokens=800
            )
            
            if analysis:
                result = {
                    "success": True,
                    "analysis": analysis
                }
                logger.info(f"âœ… [{self.name}] åˆ†æå®Œæˆ")
            else:
                result = {
                    "success": False,
                    "analysis": None,
                    "message": "èµ„æ–™åˆ†æå¤±è´¥"
                }
                logger.warning(f"âš ï¸ [{self.name}] åˆ†æå¤±è´¥")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            self.set_status(AgentStatus.FAILED)
            return {
                "success": False,
                "analysis": None,
                "message": str(e)
            }


class DeepThinkingAgent(BaseAgent):
    """æ·±åº¦æ€è€ƒAgent - è¿›è¡Œæ·±åº¦æ¨ç†å’Œæ€è€ƒ"""
    
    def __init__(self, zhipu_api_key: str, zhipu_model: str = "glm-4.5-flash"):
        super().__init__(
            name="deep_thinking",
            description="æ·±åº¦æ€è€ƒä¸æ¨ç†"
        )
        self.zhipu_api_key = zhipu_api_key
        self.zhipu_model = zhipu_model
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ·±åº¦æ€è€ƒ"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            understanding = input_data.get("understanding", "")
            analysis = input_data.get("analysis", "")
            
            if not query:
                raise ValueError("æŸ¥è¯¢ä¸ºç©º")
            
            from .momo_utils import call_zhipu_llm
            
            understanding_context = f"\né—®é¢˜ç†è§£ï¼š{understanding}\n" if understanding else ""
            analysis_context = f"\nèµ„æ–™åˆ†æï¼š{analysis}\n" if analysis else ""
            
            prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±åº¦æ€è€ƒä¸æ¨ç†ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}
{understanding_context}
{analysis_context}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œæ·±åº¦æ€è€ƒï¼š
1. è¿™äº›ä¿¡æ¯èƒŒååæ˜ äº†ä»€ä¹ˆè¶‹åŠ¿æˆ–è§„å¾‹ï¼Ÿ
2. ä¸åŒè§‚ç‚¹æˆ–æ–¹æ¡ˆçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
3. å¯ä»¥ä»å“ªäº›è§’åº¦æ¥åˆ†æè¿™ä¸ªé—®é¢˜ï¼Ÿ
4. æœ‰ä»€ä¹ˆè¢«å¿½è§†çš„é‡è¦æ–¹é¢ï¼Ÿ
5. å¦‚ä½•å°†è¿™äº›ä¿¡æ¯è”ç³»èµ·æ¥ï¼Œå½¢æˆæ›´æ·±å…¥çš„è§è§£ï¼Ÿ

è¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€è¾“å‡ºä½ çš„æ€è€ƒï¼Œæ§åˆ¶åœ¨400å­—ä»¥å†…ã€‚"""
            
            logger.info(f"[{self.name}] å¼€å§‹æ·±åº¦æ€è€ƒ")
            thinking = call_zhipu_llm(
                prompt=prompt,
                api_key=self.zhipu_api_key,
                model=self.zhipu_model,
                temperature=0.8,  # ç¨é«˜æ¸©åº¦ä»¥å¢åŠ åˆ›é€ æ€§
                max_tokens=1000
            )
            
            if thinking:
                result = {
                    "success": True,
                    "thinking": thinking
                }
                logger.info(f"âœ… [{self.name}] æ€è€ƒå®Œæˆ")
            else:
                result = {
                    "success": False,
                    "thinking": None,
                    "message": "æ·±åº¦æ€è€ƒå¤±è´¥"
                }
                logger.warning(f"âš ï¸ [{self.name}] æ€è€ƒå¤±è´¥")
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            return result
            
        except Exception as e:
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            self.set_status(AgentStatus.FAILED)
            return {
                "success": False,
                "thinking": None,
                "message": str(e)
            }


class DocumentProcessorAgent(BaseAgent):
    """æ–‡æ¡£å¤„ç†Agent - è´Ÿè´£æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢"""
    
    def __init__(self, retriever):
        super().__init__(
            name="document_processor",
            description="æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢"
        )
        self.retriever = retriever
    
    async def process(self, input_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£"""
        self.set_status(AgentStatus.PROCESSING)
        
        try:
            query = input_data.get("query", "")
            documents = input_data.get("documents", [])
            
            if not query or not documents:
                raise ValueError("æŸ¥è¯¢æˆ–æ–‡æ¡£ä¸ºç©º")
            
            logger.info(f"âœ‚ï¸ [{self.name}] å¼€å§‹æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢: {len(documents)}ä¸ªæ–‡æ¡£")
            
            from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url
            
            # æ–‡æ¡£åˆ†å—
            docs_with_details = expand_docs_by_text_split(documents)
            
            # æ·»åŠ åˆ°æ£€ç´¢å™¨
            self.retriever.add_documents(docs_with_details)
            
            # äºŒæ¬¡æ£€ç´¢ï¼ˆæ”¯æŒå¤šæŸ¥è¯¢ï¼Œå¦‚æœcontextä¸­æœ‰ï¼‰
            retrieval_queries = [query]  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            if context and context.get("retrieval_queries"):
                retrieval_queries = context.get("retrieval_queries")
            
            if len(retrieval_queries) > 1:
                relevant_docs_detailed = self.retriever.get_relevant_documents_multi_query(retrieval_queries)
            else:
                relevant_docs_detailed = self.retriever.get_relevant_documents(retrieval_queries[0])
            
            # åˆå¹¶æ–‡æ¡£
            relevant_docs = merge_docs_by_url(relevant_docs_detailed)
            
            logger.info(f"âœ… [{self.name}] å¤„ç†å®Œæˆ: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            result = {
                "success": True,
                "results": relevant_docs,
                "count": len(relevant_docs)
            }
            
            self.result = result
            self.set_status(AgentStatus.COMPLETED)
            logger.info(f"âœ… [{self.name}] Agentå·²å®Œæˆ: å¤„ç†äº† {len(relevant_docs)} ä¸ªæ–‡æ¡£")
            return result
            
        except Exception as e:
            self.error = str(e)
            self.set_status(AgentStatus.FAILED)
            logger.error(f"âŒ [{self.name}] å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "results": documents if 'documents' in locals() else []
            }


class SearchOrchestrator:
    """æœç´¢åè°ƒå™¨ - ç®¡ç†å¤šä¸ªAgentçš„åä½œ"""
    
    def __init__(self, agents: Dict[str, BaseAgent], progress_callback: Optional[Callable] = None):
        self.agents = agents
        self.progress_callback = progress_callback
        self.total_steps = 0
        self.current_step = 0
    
    async def execute(self, query: str, mode: str = "speed", detected_lang: str = "zh") -> tuple[List, str, dict]:
        """
        æ‰§è¡Œå¤šAgentåä½œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            detected_lang: æ£€æµ‹åˆ°çš„è¯­è¨€
            
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯, æ€è€ƒç»“æœå­—å…¸)
        """
        try:
            # è®¡ç®—æ€»æ­¥éª¤æ•°
            self._calculate_steps(mode)
            
            # ç«‹å³å‘é€å¼€å§‹æ¶ˆæ¯
            await self._report_progress(0, "å¤šAgentæœç´¢å·¥ä½œå·²å¯åŠ¨")
            
            # ç”¨äºå­˜å‚¨æ€è€ƒç»“æœï¼ˆæ·±åº¦æ¨¡å¼ï¼‰
            thinking_results = {}
            
            # æ·±åº¦æ¨¡å¼ï¼šAgent 0: ç†è§£é—®é¢˜
            if mode == "quality":
                understanding_agent = self.agents.get("problem_understanding")
                if understanding_agent:
                    await self._report_progress(1, "ç†è§£é—®é¢˜")
                    understanding_result = await understanding_agent.process({"query": query})
                    if understanding_result.get("success"):
                        understanding_text = understanding_result.get("understanding", "")
                        thinking_results["understanding"] = understanding_text
                        logger.info(f"âœ… é—®é¢˜ç†è§£å®Œæˆ: {understanding_text[:50]}...")
                        # å‘é€ç†è§£ç»“æœï¼ˆå•ç‹¬å‘é€ï¼Œè®©å‰ç«¯å¯ä»¥æ˜¾ç¤ºï¼‰
                        await self._report_progress(1, f"ç†è§£é—®é¢˜\n{understanding_text}")
            
            # Agent 1: å…³é”®è¯æå–
            keyword_agent = self.agents.get("keyword_extractor")
            step_offset = 2 if mode == "quality" else 1  # æ·±åº¦æ¨¡å¼ï¼šç†è§£é—®é¢˜(1) + å…³é”®è¯(2)ï¼Œå¿«é€Ÿæ¨¡å¼ï¼šå¼€å§‹(0) + å…³é”®è¯(1)
            if keyword_agent:
                await self._report_progress(step_offset, "æå–æœç´¢å…³é”®è¯")
                keyword_result = await keyword_agent.process({"query": query})
                
                if not keyword_result.get("success"):
                    logger.warning("å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                    keyword_result = {"keywords": {"zh": query, "en": ""}}
            else:
                keyword_result = {"keywords": {"zh": query, "en": ""}}
            
            # å‡†å¤‡æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡ï¼‰
            search_queries = []
            keywords = keyword_result.get("keywords", {})
            
            # å¦‚æœæ£€æµ‹åˆ°æ˜¯è‹±è¯­ï¼Œåªä½¿ç”¨è‹±æ–‡æœç´¢ï¼Œè·³è¿‡ä¸­æ–‡æœç´¢
            if detected_lang == "en":
                # è‹±è¯­æŸ¥è¯¢ï¼šä¼˜å…ˆä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                if keywords.get("en"):
                    search_queries.append({
                        "query": keywords["en"],
                        "language": "en",
                        "source": "keywords_en",
                        "max_results": 60  # è‹±è¯­SearXNGæœç´¢å¢åŠ åˆ°60æ¡
                    })
                else:
                    search_queries.append({
                        "query": query,
                        "language": "en",
                        "source": "original",
                        "max_results": 60  # è‹±è¯­SearXNGæœç´¢å¢åŠ åˆ°60æ¡
                    })
            else:
                # ä¸­æ–‡æŸ¥è¯¢ï¼šå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡
                if keywords.get("en"):
                    search_queries.append({
                        "query": keywords["en"],
                        "language": "en",
                        "source": "keywords_en",
                        "max_results": 60  # è‹±è¯­SearXNGæœç´¢å¢åŠ åˆ°60æ¡
                    })
                
                if keywords.get("zh"):
                    search_queries.append({
                        "query": keywords["zh"],
                        "language": "zh",
                        "source": "keywords_zh",
                        "max_results": 50  # ä¸­æ–‡SearXNGæœç´¢ä¿æŒ50æ¡
                    })
            
            # å‡†å¤‡DuckDuckGoæŸ¥è¯¢ï¼ˆå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡ï¼‰
            ddg_queries = []
            if detected_lang == "en":
                # è‹±è¯­æŸ¥è¯¢ï¼šåªä½¿ç”¨è‹±æ–‡ï¼Œä¸”å¢åŠ ç»“æœæ•°é‡åˆ°60
                if keywords.get("en"):
                    ddg_queries.append({
                        "query": keywords["en"],
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 60  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°60æ¡
                    })
                else:
                    ddg_queries.append({
                        "query": query,
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 60  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°60æ¡
                    })
            else:
                # ä¸­æ–‡æŸ¥è¯¢ï¼šå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡
                if keywords.get("en"):
                    ddg_queries.append({
                        "query": keywords["en"],
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 40  # ä¸­æ–‡æœç´¢æ—¶çš„è‹±è¯­èµ„æ–™ä¸º40æ¡
                    })
                elif detected_lang == "zh":
                    # å¦‚æœæ²¡æœ‰è‹±æ–‡å…³é”®è¯ï¼Œå°è¯•ç¿»è¯‘
                    from .momo_utils import translate_text
                    translated = translate_text(query, source="zh", target="en")
                    if translated:
                        ddg_queries.append({
                            "query": translated,
                            "language": "en",
                            "source": "ddg_en_translated",
                            "max_results": 40  # ä¸­æ–‡æœç´¢æ—¶çš„è‹±è¯­èµ„æ–™ä¸º40æ¡
                        })
                
                if keywords.get("zh"):
                    ddg_queries.append({
                        "query": keywords["zh"],
                        "language": "zh",
                        "source": "ddg_zh",
                        "max_results": 20
                    })
            
            # Agent 2: æœç´¢
            search_agent = self.agents.get("searcher")
            # åˆå§‹åŒ–æ­¥éª¤è®¡æ•°å™¨
            current_step = 1
            all_documents = []
            
            if search_agent:
                # é€ä¸ªæ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
                
                # æ‰§è¡ŒSearXNGæœç´¢ï¼ˆå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡ï¼‰
                for sq in search_queries:
                    if sq['language'] == 'en':
                        message = f"æ­£åœ¨æœç´¢è‹±è¯­èµ„æ–™: {sq['query']}"
                    else:
                        message = f"æ­£åœ¨æœç´¢ä¸­æ–‡èµ„æ–™: {sq['query']}"
                    await self._report_progress(current_step, message)
                    # å•ä¸ªæŸ¥è¯¢æœç´¢
                    single_result = await search_agent.process({"queries": [sq]})
                    docs = single_result.get("results", [])
                    # å»é‡åˆå¹¶
                    seen_urls = {doc.url for doc in all_documents}
                    for doc in docs:
                        if doc.url not in seen_urls:
                            all_documents.append(doc)
                            seen_urls.add(doc.url)
                    current_step += 1
                
                # æ‰§è¡ŒDuckDuckGoæœç´¢ï¼ˆå…ˆè‹±æ–‡ï¼Œåä¸­æ–‡ï¼‰
                for dq in ddg_queries:
                    if dq['language'] == 'en':
                        message = "æ­£åœ¨æ‰©å……æœç´¢è‹±è¯­èµ„æ–™..."
                    else:
                        message = "æ­£åœ¨è¿›ä¸€æ­¥æ·±åº¦æœç´¢ä¸­æ–‡èµ„æ–™..."
                    await self._report_progress(current_step, message)
                    # å•ä¸ªæŸ¥è¯¢æœç´¢
                    single_result = await search_agent.process({"queries": [dq]})
                    docs = single_result.get("results", [])
                    # å»é‡åˆå¹¶
                    seen_urls = {doc.url for doc in all_documents}
                    for doc in docs:
                        if doc.url not in seen_urls:
                            all_documents.append(doc)
                            seen_urls.add(doc.url)
                    current_step += 1
            else:
                all_documents = []
            
            if not all_documents:
                logger.warning("âš ï¸ æœç´¢æœªè¿”å›ç»“æœ")
                return [], ""
            
            # Agent 3: å‘é‡æ£€ç´¢
            retrieval_agent = self.agents.get("retriever")
            if retrieval_agent:
                vector_step = current_step
                await self._report_progress(
                    vector_step,
                    f"åˆ†æç›¸å…³æ€§ ({len(all_documents)}ä¸ªç»“æœ)"
                )
                
                # æ„å»ºæ£€ç´¢æŸ¥è¯¢åˆ—è¡¨ï¼šåˆ†å¼€æŸ¥è¯¢ä¸­è‹±æ–‡ï¼Œæé«˜åŒ¹é…ç²¾åº¦
                retrieval_queries = [query]  # æ€»æ˜¯åŒ…å«åŸå§‹æŸ¥è¯¢
                if keywords.get("en"):
                    # å¦‚æœæœ‰è‹±æ–‡å…³é”®è¯ï¼Œåˆ†åˆ«æŸ¥è¯¢ä»¥æé«˜è‹±æ–‡æ–‡æ¡£åŒ¹é…åº¦
                    retrieval_queries.append(keywords["en"])
                    logger.info(f"[å‘é‡æ£€ç´¢] ä½¿ç”¨åˆ†å¼€æŸ¥è¯¢: ä¸­æ–‡='{query[:50]}...', è‹±æ–‡='{keywords['en'][:50]}...'")
                else:
                    logger.info(f"[å‘é‡æ£€ç´¢] ä½¿ç”¨åŸå§‹æŸ¥è¯¢: {query[:100]}")
                
                retrieval_result = await retrieval_agent.process({
                    "queries": retrieval_queries,  # ä¼ é€’æŸ¥è¯¢åˆ—è¡¨
                    "documents": all_documents
                })
                relevant_docs = retrieval_result.get("results", [])
            else:
                relevant_docs = all_documents
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], "", thinking_results
            
            # æ·±åº¦æ¨¡å¼ï¼šåœ¨çˆ¬å–ä¹‹å‰è¿›è¡Œæ€è€ƒæ­¥éª¤
            if mode == "quality":
                # Agent 4: åˆ†æèµ„æ–™
                analysis_agent = self.agents.get("material_analysis")
                if analysis_agent:
                    analysis_step = vector_step + 1
                    await self._report_progress(analysis_step, "åˆ†æèµ„æ–™")
                    analysis_result = await analysis_agent.process({
                        "query": query,
                        "documents": relevant_docs,
                        "understanding": thinking_results.get("understanding", "")
                    })
                    if analysis_result.get("success"):
                        thinking_results["analysis"] = analysis_result.get("analysis", "")
                        logger.info(f"âœ… èµ„æ–™åˆ†æå®Œæˆ: {thinking_results['analysis'][:50]}...")
                
                # Agent 5: æ·±åº¦æ€è€ƒ
                thinking_agent = self.agents.get("deep_thinking")
                if thinking_agent:
                    thinking_step = analysis_step + 1 if analysis_agent else vector_step + 1
                    await self._report_progress(thinking_step, "æ·±åº¦æ€è€ƒä¸æ¨ç†")
                    thinking_result = await thinking_agent.process({
                        "query": query,
                        "understanding": thinking_results.get("understanding", ""),
                        "analysis": thinking_results.get("analysis", "")
                    })
                    if thinking_result.get("success"):
                        thinking_results["thinking"] = thinking_result.get("thinking", "")
                        logger.info(f"âœ… æ·±åº¦æ€è€ƒå®Œæˆ: {thinking_results['thinking'][:50]}...")
                
                # Agent 6: æ·±åº¦çˆ¬å–ï¼ˆä»…qualityæ¨¡å¼ï¼‰
                crawler_agent = self.agents.get("crawler")
                if crawler_agent:
                    # è®¡ç®—çˆ¬å–æ­¥éª¤ä½ç½®
                    thinking_step = analysis_step + 1 if analysis_agent else vector_step + 1
                    crawl_step = thinking_step + 1 if thinking_agent else thinking_step
                    await self._report_progress(
                        crawl_step,
                        f"æ·±åº¦çˆ¬å–å†…å®¹ (å‰{len(relevant_docs)}ä¸ª)"
                    )
                    
                    await crawler_agent.process({"documents": relevant_docs})
                    
                    # Agent 7: æ–‡æ¡£å¤„ç†
                    processor_agent = self.agents.get("document_processor")
                    if processor_agent:
                        split_step = crawl_step + 1
                        await self._report_progress(split_step, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                        
                        # æ„å»ºæ£€ç´¢æŸ¥è¯¢åˆ—è¡¨ç”¨äºäºŒæ¬¡æ£€ç´¢
                        retrieval_queries_for_processor = [query]
                        if keywords.get("en"):
                            retrieval_queries_for_processor.append(keywords["en"])
                        
                        processor_result = await processor_agent.process({
                            "query": query,
                            "documents": relevant_docs
                        }, context={
                            "retrieval_queries": retrieval_queries_for_processor
                        })
                        relevant_docs = processor_result.get("results", relevant_docs)
            
            # å®Œæˆæœç´¢é˜¶æ®µ
            final_step = self.total_steps - 1  # æœç´¢å®Œæˆæ˜¯å€’æ•°ç¬¬äºŒæ­¥
            await self._report_progress(final_step, f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(relevant_docs)}ç¯‡ç›¸å…³æ–‡æ¡£")
            
            # ç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆå›ç­”ï¼ˆæœ€åä¸€æ­¥ï¼‰
            synthesizing_step = self.total_steps
            await self._report_progress(synthesizing_step, "ç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆå›ç­”")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯ï¼ˆä½¿ç”¨é™æ€æ–¹æ³•æˆ–ç›´æ¥å®ç°ï¼‰
            citations = self._format_citations(relevant_docs)
            
            logger.info(f"âœ… å¤šAgentæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£")
            return relevant_docs, citations, thinking_results
            
        except Exception as e:
            logger.error(f"âŒ å¤šAgentæœç´¢å¤±è´¥: {e}", exc_info=True)
            return [], "", {}
    
    def _calculate_steps(self, mode: str):
        """è®¡ç®—æ€»æ­¥éª¤æ•°"""
        search_steps = 2  # ä¼°ç®—ï¼šä¸­æ–‡+è‹±æ–‡å…³é”®è¯æœç´¢
        ddg_steps = 2  # DuckDuckGoä¸­è‹±æ–‡
        
        if mode == "quality":
            # æ·±åº¦æ¨¡å¼ï¼šç†è§£é—®é¢˜(1) + å…³é”®è¯(1) + æœç´¢(2) + å‘é‡æ£€ç´¢(1) + åˆ†æèµ„æ–™(1) + æ·±åº¦æ€è€ƒ(1) + çˆ¬å–(1) + åˆ†å—(1) + å®Œæˆ(1)
            base_steps = 9
            self.total_steps = base_steps + search_steps + ddg_steps
        else:
            # å¿«é€Ÿæ¨¡å¼ï¼šå…³é”®è¯(1) + æœç´¢(2) + å‘é‡æ£€ç´¢(1) + å®Œæˆ(1)
            base_steps = 5
            self.total_steps = base_steps + search_steps + ddg_steps
        
        self.current_step = 0
    
    async def _report_progress(self, step: int, message: str):
        """æŠ¥å‘Šè¿›åº¦"""
        self.current_step = step
        if self.progress_callback:
            await self.progress_callback(step, self.total_steps, message)
    
    def _format_citations(self, documents: List) -> str:
        """ç”Ÿæˆå¼•ç”¨ä¿¡æ¯"""
        if not documents:
            return ""
        
        citations = []
        for idx, doc in enumerate(documents[:10], 1):  # æœ€å¤š10ä¸ªå¼•ç”¨
            title = doc.title if hasattr(doc, 'title') else 'N/A'
            url = doc.url if hasattr(doc, 'url') else 'N/A'
            citations.append(f"{idx}. [{title}]({url})")
        
        return "\n".join(citations)

