"""
Momo Search Handler - é«˜çº§è”ç½‘æœç´¢å¤„ç†å™¨
é›†æˆ Momo-Search çš„å®Œæ•´åŠŸèƒ½
"""
from typing import List, Dict, Optional, AsyncGenerator
from datetime import datetime
import asyncio
import threading
from loguru import logger
from sentence_transformers import SentenceTransformer

from backend.handlers.base import BaseHandler
from .momo_utils import (
    SearchDocument, 
    search_searxng, 
    search_duckduckgo,
    FaissRetriever, 
    convert_to_markdown,
    detect_language,
    translate_text,
    extract_keywords
)
from .momo_crawler import SimpleCrawler
from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url
from .momo_agents import (
    KeywordExtractionAgent,
    SearchAgent,
    RetrievalAgent,
    CrawlerAgent,
    DocumentProcessorAgent,
    SearchOrchestrator
)


class MomoSearchHandler(BaseHandler):
    """Momo é«˜çº§æœç´¢å¤„ç†å™¨"""
    
    # ç±»çº§åˆ«çš„å…±äº«èµ„æºï¼ˆæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
    _shared_embedding_models: Dict[str, SentenceTransformer] = {}  # {model_name: model_instance}
    _model_lock = threading.Lock()  # ä¿æŠ¤æ¨¡å‹åˆå§‹åŒ–çš„é”
    _model_ref_count: Dict[str, int] = {}  # æ¨¡å‹å¼•ç”¨è®¡æ•°
    
    @classmethod
    def _get_shared_embedding_model(cls, model_name: str, device: str, torch_dtype) -> SentenceTransformer:
        """
        è·å–å…±äº«çš„embeddingæ¨¡å‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        å¤šä¸ªSessionå…±äº«åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼Œå‡å°‘å†…å­˜å ç”¨
        åªæœ‰åœ¨æ¨¡å‹ä¸å­˜åœ¨æ—¶æ‰åˆ›å»ºæ–°å®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡ (cuda/cpu)
            torch_dtype: torchæ•°æ®ç±»å‹
            
        Returns:
            SentenceTransformerå®ä¾‹
        """
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŒ…å«è®¾å¤‡ä¿¡æ¯ï¼Œå› ä¸ºä¸åŒè®¾å¤‡éœ€è¦ä¸åŒå®ä¾‹ï¼‰
        cache_key = f"{model_name}_{device}_{str(torch_dtype)}"
        
        # åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼
        if cache_key not in cls._shared_embedding_models:
            with cls._model_lock:
                # å†æ¬¡æ£€æŸ¥ï¼ˆé¿å…å¹¶å‘åˆ›å»ºï¼‰
                if cache_key not in cls._shared_embedding_models:
                    logger.info(f"ğŸ”§ é¦–æ¬¡åŠ è½½å…±äº«embeddingæ¨¡å‹: {cache_key}")
                    try:
                        if device == "cuda":
                            model = SentenceTransformer(
                                model_name,
                                device=device,
                                model_kwargs={"torch_dtype": torch_dtype}
                            )
                        else:
                            model = SentenceTransformer(
                                model_name,
                                device=device,
                                model_kwargs={"torch_dtype": torch_dtype}
                            )
                        cls._shared_embedding_models[cache_key] = model
                        cls._model_ref_count[cache_key] = 0
                        logger.info(f"âœ… å…±äº«embeddingæ¨¡å‹åŠ è½½æˆåŠŸ: {cache_key}")
                    except Exception as e:
                        logger.error(f"âŒ å…±äº«embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        logger.info("â„¹ï¸ å°è¯•ä½¿ç”¨é»˜è®¤è®¾ç½®...")
                        model = SentenceTransformer(model_name, device=device)
                        cls._shared_embedding_models[cache_key] = model
                        cls._model_ref_count[cache_key] = 0
                else:
                    logger.debug(f"â™»ï¸ ä½¿ç”¨å·²å­˜åœ¨çš„å…±äº«embeddingæ¨¡å‹: {cache_key}")
        
        # å¢åŠ å¼•ç”¨è®¡æ•°
        cls._model_ref_count[cache_key] = cls._model_ref_count.get(cache_key, 0) + 1
        logger.debug(f"ğŸ“Š æ¨¡å‹å¼•ç”¨è®¡æ•°: {cache_key} = {cls._model_ref_count[cache_key]}")
        
        return cls._shared_embedding_models[cache_key]
    
    @classmethod
    def _release_embedding_model(cls, model_name: str, device: str, torch_dtype):
        """
        é‡Šæ”¾æ¨¡å‹å¼•ç”¨ï¼ˆå½“Sessioné”€æ¯æ—¶è°ƒç”¨ï¼‰
        
        æ³¨æ„ï¼šå½“å‰å®ç°ä¸ä¼šçœŸæ­£å¸è½½æ¨¡å‹ï¼Œå› ä¸ºå¯èƒ½æœ‰å…¶ä»–Sessionåœ¨ä½¿ç”¨
        æœªæ¥å¯ä»¥å®ç°çœŸæ­£çš„å¸è½½é€»è¾‘ï¼ˆå½“å¼•ç”¨è®¡æ•°ä¸º0æ—¶ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¾å¤‡
            torch_dtype: torchæ•°æ®ç±»å‹
        """
        cache_key = f"{model_name}_{device}_{str(torch_dtype)}"
        if cache_key in cls._model_ref_count:
            cls._model_ref_count[cache_key] = max(0, cls._model_ref_count[cache_key] - 1)
            logger.debug(f"ğŸ“Š æ¨¡å‹å¼•ç”¨è®¡æ•°å‡å°‘: {cache_key} = {cls._model_ref_count[cache_key]}")
            # TODO: å½“å¼•ç”¨è®¡æ•°ä¸º0æ—¶ï¼Œå¯ä»¥è€ƒè™‘å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜
    
    async def _setup(self):
        """åˆå§‹åŒ–æœç´¢ç»„ä»¶"""
        try:
            # SearXNGé…ç½®
            self.searxng_url = self.config.get('searxng_url', 'http://localhost:9080')
            self.searxng_language = self.config.get('searxng_language', 'zh')
            self.searxng_time_range = self.config.get('searxng_time_range', 'day')
            self.max_search_results = self.config.get('max_search_results', 50)
            
            # å‘é‡æ£€ç´¢é…ç½®
            embedding_model_name = self.config.get(
                'embedding_model', 
                'BAAI/bge-small-zh-v1.5'
            )
            self.num_candidates = self.config.get('num_candidates', 40)
            self.sim_threshold = self.config.get('sim_threshold', 0.45)
            
            # çˆ¬è™«é…ç½®
            self.enable_deep_crawl = self.config.get('enable_deep_crawl', True)
            self.crawl_score_threshold = self.config.get('crawl_score_threshold', 0.5)
            self.max_crawl_docs = self.config.get('max_crawl_docs', 10)
            
            # å…³é”®è¯æå–é…ç½®
            self.enable_keyword_extraction = self.config.get('enable_keyword_extraction', True)
            self.zhipu_api_key = self.config.get('zhipu_api_key', '6f29a799833a4a5daf5752973e9d0cc4.uoelH21xYFMkDknh')
            self.zhipu_model = self.config.get('zhipu_model', 'glm-4.5-flash')
            
            logger.info("ğŸš€ åˆå§‹åŒ– Momo Search Handler...")
            logger.info(f"  SearXNG: {self.searxng_url}")
            logger.info(f"  è¯­è¨€: {self.searxng_language}")
            logger.info(f"  æ—¶é—´èŒƒå›´: {self.searxng_time_range}")
            logger.info(f"  åµŒå…¥æ¨¡å‹: {embedding_model_name}")
            logger.info(f"  æ·±åº¦çˆ¬å–: {'å¼€å¯' if self.enable_deep_crawl else 'å…³é—­'}")
            logger.info(f"  å…³é”®è¯æå–: {'å¼€å¯' if self.enable_keyword_extraction else 'å…³é—­'}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨å…±äº«æ¨¡å‹ä¼˜åŒ–å†…å­˜ï¼‰
            # CPUä¸æ”¯æŒfloat16ï¼Œä½¿ç”¨float32
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            torch_dtype = torch.float16 if device == "cuda" else torch.float32
            
            # ä½¿ç”¨å…±äº«æ¨¡å‹å®ä¾‹ï¼ˆå¤šä¸ªSessionå…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼Œå‡å°‘å†…å­˜å ç”¨ï¼‰
            self.embedding_model = self._get_shared_embedding_model(
                embedding_model_name,
                device=device,
                torch_dtype=torch_dtype
            )
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯ç”¨äºæ¸…ç†æ—¶é‡Šæ”¾å¼•ç”¨
            self._embedding_model_name = embedding_model_name
            self._embedding_device = device
            self._embedding_torch_dtype = torch_dtype
            logger.info(f"âœ… ä½¿ç”¨å…±äº«embeddingæ¨¡å‹: {embedding_model_name} (è®¾å¤‡: {device})")
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.retriever = FaissRetriever(
                self.embedding_model,
                num_candidates=self.num_candidates,
                sim_threshold=self.sim_threshold
            )
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.crawler = SimpleCrawler(
                timeout=15.0,
                max_concurrent=5
            )
            
            # åˆå§‹åŒ–å¤šAgentç³»ç»Ÿ
            self.use_multi_agent = self.config.get('use_multi_agent', True)  # é»˜è®¤å¯ç”¨å¤šAgent
            
            if self.use_multi_agent:
                logger.info("ğŸ¤– åˆå§‹åŒ–å¤šAgentç³»ç»Ÿ...")
                self._initialize_agents()
                logger.info("âœ… å¤šAgentç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            else:
                logger.info("âš ï¸ ä½¿ç”¨ä¼ ç»Ÿç®¡é“æ¨¡å¼ï¼ˆæœªå¯ç”¨å¤šAgentï¼‰")
            
            # ä¸Šä¸‹æ–‡å‹ç¼©é…ç½®
            compression_config = self.config.get('context_compression', {})
            self.compression_method = compression_config.get('method', 'rule_based')
            self.compression_max_messages = compression_config.get('max_messages', 4)
            self.compression_min_total_length = compression_config.get('min_total_length', 1200)
            self.compression_max_length = compression_config.get('max_compressed_length', 600)
            logger.info(f"ğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©é…ç½®: æ–¹æ³•={self.compression_method}, æ¶ˆæ¯é˜ˆå€¼={self.compression_max_messages}æ¡, å­—ç¬¦é˜ˆå€¼={self.compression_min_total_length}å­—ç¬¦, æœ€å¤§é•¿åº¦={self.compression_max_length}å­—ç¬¦")
            
            logger.info("âœ… Momo Search Handler åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Momo Search Handler åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_today_date(self) -> str:
        """è·å–ä»Šå¤©çš„æ—¥æœŸ"""
        return datetime.today().strftime('%Y-%m-%d')
    
    def format_sources_for_llm(self, sources: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºLLMå¯ç”¨çš„ä¸Šä¸‹æ–‡
        
        Args:
            sources: æœç´¢æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        sources_str = "\n\n".join([
            f"[ç½‘é¡µ {i+1} å¼€å§‹]\n"
            f"æ ‡é¢˜: {doc.title}\n"
            f"é“¾æ¥: {doc.url}\n"
            f"å†…å®¹: {doc.content if doc.content else doc.snippet}\n"
            f"[ç½‘é¡µ {i+1} ç»“æŸ]"
            for i, doc in enumerate(sources)
        ])
        return sources_str
    
    def _initialize_agents(self):
        """åˆå§‹åŒ–æ‰€æœ‰Agent"""
        self.agents = {}
        
        # æ·±åº¦æ€è€ƒç›¸å…³Agentï¼ˆä½¿ç”¨æ™ºè°±æ¸…è¨€ï¼‰
        from .momo_agents import ProblemUnderstandingAgent, MaterialAnalysisAgent, DeepThinkingAgent
        
        # é—®é¢˜ç†è§£Agentï¼ˆä»…æ·±åº¦æ¨¡å¼ï¼‰
        self.agents["problem_understanding"] = ProblemUnderstandingAgent(
            zhipu_api_key=self.zhipu_api_key,
            zhipu_model=self.zhipu_model
        )
        
        # èµ„æ–™åˆ†æAgentï¼ˆä»…æ·±åº¦æ¨¡å¼ï¼‰
        # ä½¿ç”¨æ›´é«˜çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5ï¼‰æ¥ç­›é€‰æ›´ç›¸å…³çš„æ–‡æ¡£è¿›è¡Œåˆ†æ
        analysis_score_threshold = self.config.get('analysis_score_threshold', 0.5)
        self.agents["material_analysis"] = MaterialAnalysisAgent(
            zhipu_api_key=self.zhipu_api_key,
            zhipu_model=self.zhipu_model,
            analysis_score_threshold=analysis_score_threshold
        )
        
        # æ·±åº¦æ€è€ƒAgentï¼ˆä»…æ·±åº¦æ¨¡å¼ï¼‰
        self.agents["deep_thinking"] = DeepThinkingAgent(
            zhipu_api_key=self.zhipu_api_key,
            zhipu_model=self.zhipu_model
        )
        
        # å…³é”®è¯æå–Agent
        if self.enable_keyword_extraction:
            self.agents["keyword_extractor"] = KeywordExtractionAgent(
                zhipu_api_key=self.zhipu_api_key,
                zhipu_model=self.zhipu_model
            )
        
        # æœç´¢Agent
        self.agents["searcher"] = SearchAgent(
            searxng_url=self.searxng_url,
            searxng_language=self.searxng_language,
            searxng_time_range=self.searxng_time_range,
            max_results=self.max_search_results
        )
        
        # æ£€ç´¢Agent
        self.agents["retriever"] = RetrievalAgent(
            retriever=self.retriever,
            sim_threshold=self.sim_threshold
        )
        
        # çˆ¬å–Agentï¼ˆä»…qualityæ¨¡å¼éœ€è¦ï¼‰
        if self.enable_deep_crawl:
            self.agents["crawler"] = CrawlerAgent(
                crawler=self.crawler,
                score_threshold=self.crawl_score_threshold,
                max_docs=self.max_crawl_docs
            )
            
            # æ–‡æ¡£å¤„ç†Agent
            self.agents["document_processor"] = DocumentProcessorAgent(
                retriever=self.retriever
            )
        
        logger.info(f"âœ… å·²åˆå§‹åŒ– {len(self.agents)} ä¸ªAgent: {list(self.agents.keys())}")
    
    def format_citations(self, docs: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„å¼•ç”¨åˆ—è¡¨
        """
        citations = []
        for i, doc in enumerate(docs):
            # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
            title = doc.title[:50] + "..." if len(doc.title) > 50 else doc.title
            citations.append(f"{i+1}. [{title}]({doc.url})")
        
        return "\n".join(citations)
    
    async def search_with_progress(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> tuple[List[SearchDocument], str, dict]:
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            conversation_history: å¯¹è¯å†å²è®°å½•ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£
        
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯, æ€è€ƒç»“æœå­—å…¸)
        """
        # å¦‚æœå¯ç”¨å¤šAgentæ¨¡å¼ï¼Œä½¿ç”¨Agentåä½œ
        if self.use_multi_agent and hasattr(self, 'agents'):
            return await self._search_with_agents(query, mode, progress_callback, conversation_history)
        
        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿç®¡é“æ¨¡å¼ï¼ˆè¿”å›ç©ºçš„æ€è€ƒç»“æœï¼‰
        docs, citations = await self._search_with_pipeline(query, mode, progress_callback, conversation_history)
        return docs, citations, {}
    
    async def _search_with_agents(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> tuple[List[SearchDocument], str, dict]:
        """ä½¿ç”¨å¤šAgentåä½œæ‰§è¡Œæœç´¢"""
        try:
            logger.info(f"ğŸ¤– [å¤šAgentæ¨¡å¼] å¼€å§‹æ‰§è¡Œæœç´¢: æŸ¥è¯¢='{query}', æ¨¡å¼={mode}")
            logger.info(f"ğŸ¤– [å¤šAgentæ¨¡å¼] å·²å¯ç”¨ {len(self.agents)} ä¸ªAgent: {list(self.agents.keys())}")
            if conversation_history:
                logger.info(f"ğŸ“š [å¤šAgentæ¨¡å¼] å¯¹è¯å†å²: {len(conversation_history)} æ¡æ¶ˆæ¯")
            
            detected_lang = detect_language(query)
            
            # åˆ›å»ºåè°ƒå™¨
            orchestrator = SearchOrchestrator(
                agents=self.agents,
                progress_callback=progress_callback
            )
            
            # ä¼ é€’å‹ç¼©é…ç½®ç»™orchestratorï¼ˆä»¥ä¾¿ä¼ é€’ç»™å„ä¸ªAgentï¼‰
            orchestrator._compression_config = {
                "compression_method": self.compression_method,
                "compression_max_messages": self.compression_max_messages,
                "compression_max_length": self.compression_max_length,
                "compression_min_total_length": self.compression_min_total_length
            }
            
            # æ‰§è¡Œå¤šAgentåä½œæœç´¢
            relevant_docs, citations, thinking_results = await orchestrator.execute(
                query=query,
                mode=mode,
                detected_lang=detected_lang,
                conversation_history=conversation_history
            )
            
            logger.info(f"âœ… [å¤šAgentæ¨¡å¼] æœç´¢å®Œæˆ: è¿”å› {len(relevant_docs)} ä¸ªæ–‡æ¡£")
            if thinking_results:
                logger.info(f"ğŸ§  æ€è€ƒç»“æœ: {list(thinking_results.keys())}")
            return relevant_docs, citations, thinking_results
            
        except Exception as e:
            logger.error(f"âŒ å¤šAgentæœç´¢å¤±è´¥: {e}", exc_info=True)
            return [], "", {}
    
    async def _search_with_pipeline(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> tuple[List[SearchDocument], str]:
        """ä½¿ç”¨ä¼ ç»Ÿç®¡é“æ¨¡å¼æ‰§è¡Œæœç´¢ï¼ˆåŸæœ‰å®ç°ï¼‰"""
        try:
            detected_lang = detect_language(query)
            all_search_results = []
            keywords_dict = None  # åˆå§‹åŒ–å…³é”®è¯å­—å…¸
            
            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ„å»ºä¸Šä¸‹æ–‡å¢å¼ºçš„æŸ¥è¯¢ï¼ˆä½¿ç”¨å‹ç¼©æŠ€æœ¯ï¼‰
            enhanced_query = query
            if conversation_history:
                from .momo_utils import compress_conversation_history
                
                # å°è¯•å‹ç¼©å¯¹è¯å†å²ï¼ˆåªåœ¨å†å²è¾ƒé•¿æ—¶å‹ç¼©ï¼‰
                # ä½¿ç”¨é…ç½®ä¸­çš„å‹ç¼©æ–¹æ³•
                compressed_context = compress_conversation_history(
                    conversation_history=conversation_history,
                    current_query=query,
                    max_messages=self.compression_max_messages,
                    max_compressed_length=self.compression_max_length,
                    min_total_length=self.compression_min_total_length,
                    compression_method=self.compression_method,
                    api_key=self.zhipu_api_key,
                    model=self.zhipu_model
                )
                
                # å¦‚æœé…ç½®çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•é™çº§ç­–ç•¥
                if not compressed_context and self.compression_method != "rule_based":
                    compressed_context = compress_conversation_history(
                        conversation_history=conversation_history,
                        current_query=query,
                        max_messages=self.compression_max_messages,
                        max_compressed_length=self.compression_max_length,
                        min_total_length=self.compression_min_total_length,
                        compression_method="rule_based"
                    )
                elif not compressed_context and self.compression_method != "smart_truncate":
                    compressed_context = compress_conversation_history(
                        conversation_history=conversation_history,
                        current_query=query,
                        max_messages=self.compression_max_messages,
                        max_compressed_length=self.compression_max_length,
                        min_total_length=self.compression_min_total_length,
                        compression_method="smart_truncate"
                    )
                
                if compressed_context:
                    # ä½¿ç”¨å‹ç¼©åçš„ä¸Šä¸‹æ–‡
                    enhanced_query = f"{query}\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{compressed_context}"
                    logger.info(f"ğŸ“š [ç®¡é“æ¨¡å¼] å·²æ·»åŠ å‹ç¼©åçš„å¯¹è¯ä¸Šä¸‹æ–‡åˆ°æŸ¥è¯¢ï¼ˆå‹ç¼©ç‰ˆæœ¬ï¼‰")
                else:
                    # å¦‚æœå‹ç¼©å¤±è´¥æˆ–ä¸éœ€è¦å‹ç¼©ï¼Œä½¿ç”¨ç®€å•çš„æˆªæ–­æ–¹å¼
                    recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
                    context_parts = []
                    for msg in recent_history:
                        role = msg.get("role", "unknown")
                        content = msg.get("content", "")
                        if role == "user" and content and content != query:
                            context_parts.append(f"ç”¨æˆ·ä¹‹å‰æåˆ°: {content}")
                        elif role == "assistant" and content:
                            # åªå–å‰200å­—ç¬¦çš„æ‘˜è¦ï¼Œé¿å…å¤ªé•¿
                            content_preview = content[:200] + "..." if len(content) > 200 else content
                            context_parts.append(f"AIä¹‹å‰å›ç­”: {content_preview}")
                    
                    if context_parts:
                        context_text = "\n".join(context_parts)
                        enhanced_query = f"{query}\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context_text}"
                        logger.info(f"ğŸ“š [ç®¡é“æ¨¡å¼] å·²æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡åˆ°æŸ¥è¯¢ï¼ˆæœªå‹ç¼©ç‰ˆæœ¬ï¼‰")
            
            # é¢„å…ˆè®¡ç®—æ€»æ­¥éª¤æ•°ï¼Œç”¨äºä¸€è‡´çš„è¿›åº¦æ˜¾ç¤º
            base_steps = 5  # å…³é”®è¯æå–(1) + å‘é‡æ£€ç´¢(1) + æ·±åº¦çˆ¬å–(1) + æ–‡æ¡£åˆ†å—(1) + å®Œæˆ(1)
            ddg_steps = 2  # DuckDuckGo ä¸­æ–‡ + è‹±æ–‡ï¼ˆæœ€å¤š2æ­¥ï¼‰
            # åˆå§‹ä¼°ç®—æœç´¢æŸ¥è¯¢æ•°é‡ï¼ˆé€šå¸¸æ˜¯2ä¸ªï¼šä¸­æ–‡+è‹±æ–‡å…³é”®è¯ï¼‰
            estimated_search_queries = 2 if self.enable_keyword_extraction else 1
            total_steps = base_steps + estimated_search_queries + ddg_steps
            
            # æ­¥éª¤0: å…³é”®è¯æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_keyword_extraction:
                if progress_callback:
                    await progress_callback(0, total_steps, "æå–æœç´¢å…³é”®è¯")
                
                logger.info(f"å¼€å§‹æå–å…³é”®è¯: {enhanced_query if enhanced_query != query else query}")
                keywords_dict = extract_keywords(
                    enhanced_query,  # ä½¿ç”¨å¢å¼ºçš„æŸ¥è¯¢ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
                    api_key=self.zhipu_api_key,
                    model=self.zhipu_model,
                    conversation_history=conversation_history  # ä¼ é€’å¯¹è¯å†å²
                )
                
                # å‡†å¤‡æœç´¢æŸ¥è¯¢åˆ—è¡¨
                search_queries = []
                
                # å¦‚æœæ£€æµ‹åˆ°æ˜¯è‹±è¯­ï¼Œåªä½¿ç”¨è‹±æ–‡æœç´¢ï¼Œè·³è¿‡ä¸­æ–‡æœç´¢
                if detected_lang == "en":
                    if keywords_dict:
                        en_keys = keywords_dict.get("en_keys", "").strip()
                        if en_keys:
                            search_queries.append({
                                "query": en_keys,
                                "language": "en",
                                "source": "keywords_en"
                            })
                            logger.info(f"âœ… æå–åˆ°è‹±æ–‡å…³é”®è¯: {en_keys}")
                    
                    # å¦‚æœæ²¡æœ‰æå–åˆ°è‹±æ–‡å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                    if not search_queries:
                        search_queries.append({
                            "query": query,
                            "language": "en",
                            "source": "original"
                        })
                else:
                    # ä¸­æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨ä¸­è‹±æ–‡å…³é”®è¯
                    if keywords_dict:
                        zh_keys = keywords_dict.get("zh_keys", "").strip()
                        en_keys = keywords_dict.get("en_keys", "").strip()
                        
                        # å¦‚æœæå–åˆ°ä¸­æ–‡å…³é”®è¯ï¼Œæ·»åŠ åˆ°æœç´¢åˆ—è¡¨
                        if zh_keys:
                            search_queries.append({
                                "query": zh_keys,
                                "language": "zh",
                                "source": "keywords_zh"
                            })
                            logger.info(f"âœ… æå–åˆ°ä¸­æ–‡å…³é”®è¯: {zh_keys}")
                        
                        # å¦‚æœæå–åˆ°è‹±æ–‡å…³é”®è¯ï¼Œæ·»åŠ åˆ°æœç´¢åˆ—è¡¨
                        if en_keys:
                            search_queries.append({
                                "query": en_keys,
                                "language": "en",
                                "source": "keywords_en"
                            })
                            logger.info(f"âœ… æå–åˆ°è‹±æ–‡å…³é”®è¯: {en_keys}")
                    
                    # å¦‚æœå…³é”®è¯æå–å¤±è´¥æˆ–æ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                    if not search_queries:
                        logger.warning("âš ï¸ å…³é”®è¯æå–å¤±è´¥æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                        search_queries.append({
                            "query": query,
                            "language": detected_lang,
                            "source": "original"
                        })
            else:
                # æœªå¯ç”¨å…³é”®è¯æå–ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                search_queries = [{
                    "query": query,
                    "language": detected_lang,
                    "source": "original"
                }]
            
            all_search_results = []
            # é‡æ–°è®¡ç®—ç²¾ç¡®çš„æ€»æ­¥éª¤æ•°ï¼ˆåŸºäºå®é™…search_queriesæ•°é‡ï¼‰
            actual_total_steps = base_steps + len(search_queries) + ddg_steps
            if actual_total_steps != total_steps:
                # å¦‚æœå®é™…æ­¥æ•°ä¸ä¼°ç®—ä¸åŒï¼Œæ›´æ–°total_steps
                total_steps = actual_total_steps
            
            # æ­¥éª¤1: ä½¿ç”¨æå–çš„å…³é”®è¯è¿›è¡Œæœç´¢
            for idx, search_item in enumerate(search_queries):
                step_num = idx + 1
                if progress_callback:
                    await progress_callback(step_num, total_steps, f"æ­£åœ¨æœç´¢: {search_item['query']} ({search_item['source']})")
                
                logger.info(f"å¼€å§‹æœç´¢: {search_item['query']} (è¯­è¨€: {search_item['language']}, æ¥æº: {search_item['source']})")
                
                search_results = search_searxng(
                    query=search_item['query'],
                    num_results=self.max_search_results,
                    ip_address=self.searxng_url,
                    language=search_item['language'],
                    time_range=self.searxng_time_range,
                    deduplicate_by_url=True
                )
                
                # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                seen_urls = {doc.url for doc in all_search_results}
                for doc in search_results:
                    if doc.url not in seen_urls:
                        all_search_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… {search_item['source']}æœç´¢å®Œæˆ: è·å¾—{len(search_results)}ä¸ªç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            # å¦‚æœåªæå–åˆ°ä¸­æ–‡å…³é”®è¯ä½†æ²¡æœ‰è‹±æ–‡å…³é”®è¯ï¼Œä¸”åŸå§‹æŸ¥è¯¢æ˜¯ä¸­æ–‡ï¼Œå°è¯•ç¿»è¯‘å¹¶æœç´¢
            if detected_lang == "zh" and keywords_dict and keywords_dict.get("en_keys") and not any(item['source'] == 'keywords_en' for item in search_queries):
                if progress_callback:
                    await progress_callback(2, total_steps, "ğŸŒ ç¿»è¯‘æŸ¥è¯¢å¹¶æœç´¢è‹±æ–‡ç»“æœ")
                
                # ç¿»è¯‘åŸå§‹æŸ¥è¯¢ä½œä¸ºè¡¥å……
                translated_query = translate_text(query, source="zh", target="en")
                
                if translated_query:
                    logger.info(f"ğŸŒ ç¿»è¯‘ç»“æœ: {query} -> {translated_query}")
                    
                    english_search_results = search_searxng(
                        query=translated_query,
                        num_results=self.max_search_results,
                        ip_address=self.searxng_url,
                        language="en",
                        time_range=self.searxng_time_range,
                        deduplicate_by_url=True
                    )
                    
                    # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                    seen_urls = {doc.url for doc in all_search_results}
                    for doc in english_search_results:
                        if doc.url not in seen_urls:
                            all_search_results.append(doc)
                            seen_urls.add(doc.url)
                    
                    logger.info(f"âœ… ç¿»è¯‘æœç´¢å®Œæˆ: è·å¾—{len(english_search_results)}ä¸ªæ–°ç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            # æ­¥éª¤2: ä½¿ç”¨ DuckDuckGo è¿›è¡Œè¡¥å……æœç´¢
            # å‡†å¤‡ DuckDuckGo æœç´¢æŸ¥è¯¢
            ddg_queries = []
            
            if detected_lang == "en":
                # è‹±è¯­æŸ¥è¯¢ï¼šåªä½¿ç”¨è‹±æ–‡ï¼Œä¸”å¢åŠ ç»“æœæ•°é‡åˆ°40
                if keywords_dict and keywords_dict.get("en_keys"):
                    ddg_queries.append({
                        "query": keywords_dict.get("en_keys"),
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 40  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°40æ¡
                    })
                else:
                    ddg_queries.append({
                        "query": query,
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 40  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°40æ¡
                    })
            else:
                # ä¸­æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨ä¸­è‹±æ–‡
                # å¦‚æœæœ‰ä¸­æ–‡å…³é”®è¯ï¼Œä½¿ç”¨ä¸­æ–‡å…³é”®è¯ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                if keywords_dict and keywords_dict.get("zh_keys"):
                    ddg_queries.append({
                        "query": keywords_dict.get("zh_keys"),
                        "language": "zh",
                        "source": "ddg_zh",
                        "max_results": 20
                    })
                elif detected_lang == "zh":
                    ddg_queries.append({
                        "query": query,
                        "language": "zh",
                        "source": "ddg_zh",
                        "max_results": 20
                    })
                
                # å¦‚æœæœ‰è‹±æ–‡å…³é”®è¯ï¼Œä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼›å¦åˆ™å°è¯•ç¿»è¯‘
                if keywords_dict and keywords_dict.get("en_keys"):
                    ddg_queries.append({
                        "query": keywords_dict.get("en_keys"),
                        "language": "en",
                        "source": "ddg_en",
                        "max_results": 40  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°40æ¡
                    })
                elif detected_lang == "zh":
                    # ä¸­æ–‡æŸ¥è¯¢å°è¯•ç¿»è¯‘ä¸ºè‹±æ–‡
                    translated_query = translate_text(query, source="zh", target="en")
                    if translated_query:
                        ddg_queries.append({
                            "query": translated_query,
                            "language": "en",
                            "source": "ddg_en_translated",
                            "max_results": 40  # è‹±è¯­æŸ¥è¯¢å¢åŠ åˆ°40æ¡
                        })
            
            # æ‰§è¡Œ DuckDuckGo æœç´¢
            for idx, ddg_item in enumerate(ddg_queries):
                step_num = len(search_queries) + idx + 1
                if progress_callback:
                    if ddg_item['language'] == 'zh':
                        message = "æ­£åœ¨è¿›ä¸€æ­¥æ·±åº¦æœç´¢..."
                    else:  # en
                        message = "æ­£åœ¨æ‰©å……æœç´¢è‹±è¯­èµ„æ–™..."
                    await progress_callback(
                        step_num, 
                        total_steps, 
                        message
                    )
                
                logger.info(f"ğŸ¦† å¼€å§‹DuckDuckGoæœç´¢: {ddg_item['query']} (è¯­è¨€: {ddg_item['language']})")
                
                # æ ¹æ®max_resultså‚æ•°å†³å®šç»“æœæ•°é‡ï¼ˆè‹±è¯­40ï¼Œä¸­æ–‡20ï¼‰
                max_results = ddg_item.get("max_results", 20)
                
                ddg_results = await search_duckduckgo(
                    query=ddg_item['query'],
                    max_results=max_results,
                    language=ddg_item['language'],
                    time_range=self.searxng_time_range if self.searxng_time_range else None
                )
                
                # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                seen_urls = {doc.url for doc in all_search_results}
                for doc in ddg_results:
                    if doc.url not in seen_urls:
                        all_search_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… DuckDuckGo {ddg_item['source']}æœç´¢å®Œæˆ: è·å¾—{len(ddg_results)}ä¸ªç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            if not all_search_results:
                logger.warning("âš ï¸ æ‰€æœ‰æœç´¢å‡æœªè¿”å›ç»“æœ")
                return [], ""
            
            # æ­¥éª¤3: å‘é‡æ£€ç´¢ï¼ˆåˆå¹¶åçš„ç»“æœï¼‰
            vector_step = len(search_queries) + len(ddg_queries) + 1
            if progress_callback:
                await progress_callback(vector_step, total_steps, f"åˆ†æç›¸å…³æ€§ ({len(all_search_results)}ä¸ªç»“æœ)")
            
            # æ„å»ºæ£€ç´¢æŸ¥è¯¢åˆ—è¡¨ï¼šåˆ†å¼€æŸ¥è¯¢ä¸­è‹±æ–‡ï¼Œæé«˜åŒ¹é…ç²¾åº¦
            retrieval_queries = [query]  # æ€»æ˜¯åŒ…å«åŸå§‹æŸ¥è¯¢
            if self.enable_keyword_extraction and keywords_dict:
                en_keys = keywords_dict.get("en_keys", "").strip()
                if en_keys:
                    # å¦‚æœæœ‰è‹±æ–‡å…³é”®è¯ï¼Œåˆ†åˆ«æŸ¥è¯¢ä»¥æé«˜è‹±æ–‡æ–‡æ¡£åŒ¹é…åº¦
                    retrieval_queries.append(en_keys)
                    logger.info(f"[å‘é‡æ£€ç´¢-Pipeline] ä½¿ç”¨åˆ†å¼€æŸ¥è¯¢: ä¸­æ–‡='{query[:50]}...', è‹±æ–‡='{en_keys[:50]}...'")
                else:
                    logger.info(f"[å‘é‡æ£€ç´¢-Pipeline] ä½¿ç”¨åŸå§‹æŸ¥è¯¢: {query[:100]}")
            else:
                logger.info(f"[å‘é‡æ£€ç´¢-Pipeline] ä½¿ç”¨åŸå§‹æŸ¥è¯¢: {query[:100]}")
            
            self.retriever.add_documents(all_search_results)
            # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢
            if len(retrieval_queries) > 1:
                relevant_docs = self.retriever.get_relevant_documents_multi_query(retrieval_queries)
            else:
                relevant_docs = self.retriever.get_relevant_documents(retrieval_queries[0])
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], ""
            
            logger.info(f"âœ… æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ­¥éª¤4: æ·±åº¦çˆ¬å– (ä»…qualityæ¨¡å¼)
            if mode == "quality" and self.enable_deep_crawl:
                crawl_step = vector_step + 1
                if progress_callback:
                    await progress_callback(crawl_step, total_steps, f"ğŸ•·ï¸ æ·±åº¦çˆ¬å–å†…å®¹ (å‰{self.max_crawl_docs}ä¸ª)")
                
                await self.crawler.crawl_many(
                    relevant_docs,
                    score_threshold=self.crawl_score_threshold,
                    max_docs=self.max_crawl_docs
                )
                
                # æ­¥éª¤5: æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢
                split_step = crawl_step + 1
                if progress_callback:
                    await progress_callback(split_step, total_steps, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                
                docs_with_details = expand_docs_by_text_split(relevant_docs)
                self.retriever.add_documents(docs_with_details)
                # äºŒæ¬¡æ£€ç´¢ä¹Ÿä½¿ç”¨å¤šæŸ¥è¯¢ï¼ˆretrieval_querieså·²åœ¨ä¸Šæ–¹å®šä¹‰ï¼‰
                if len(retrieval_queries) > 1:
                    relevant_docs_detailed = self.retriever.get_relevant_documents_multi_query(retrieval_queries)
                else:
                    relevant_docs_detailed = self.retriever.get_relevant_documents(retrieval_queries[0])
                relevant_docs = merge_docs_by_url(relevant_docs_detailed)
                
                logger.info(f"ğŸ“„ äºŒæ¬¡æ£€ç´¢å: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            # æœ€åä¸€æ­¥: å®Œæˆ
            if progress_callback:
                await progress_callback(total_steps, total_steps, "âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå†…å®¹")
                # é¢å¤–å‘é€æ–‡æ¡£æ•°é‡ä¿¡æ¯
                await progress_callback(total_steps + 1, total_steps + 1, f"æ‰¾åˆ°{len(relevant_docs)}ç¯‡ç›¸å…³æ–‡æ¡£")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            citations = self.format_citations(relevant_docs)
            
            logger.info(f"âœ… Momoæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£ (è¯­è¨€: {detected_lang})")
            return relevant_docs, citations
            
        except Exception as e:
            logger.error(f"âŒ Momoæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return [], ""
    
    async def process(self, query: str, mode: str = "speed") -> Dict:
        """
        å¤„ç†æœç´¢è¯·æ±‚ï¼ˆåŒæ­¥æ¥å£ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼
        
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        relevant_docs, citations = await self.search_with_progress(query, mode)
        
        if not relevant_docs:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
            }
        
        # æ ¼å¼åŒ–ä¸ºLLMä¸Šä¸‹æ–‡
        context = self.format_sources_for_llm(relevant_docs)
        
        return {
            "success": True,
            "context": context,
            "citations": citations,
            "num_results": len(relevant_docs),
            "documents": relevant_docs
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # é‡Šæ”¾embeddingæ¨¡å‹å¼•ç”¨
        if hasattr(self, '_embedding_model_name'):
            self._release_embedding_model(
                self._embedding_model_name,
                self._embedding_device,
                self._embedding_torch_dtype
            )
        
        if hasattr(self, 'crawler'):
            await self.crawler.close()
        logger.info("ğŸ§¹ Momo Search Handler èµ„æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ³¨æ„ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œææ„å‡½æ•°ä¸­çš„å¼‚æ­¥è°ƒç”¨å¯èƒ½ä¸ä¼šæ‰§è¡Œ
        # ä½†åœ¨Pythoné€€å‡ºæ—¶ä»å¯èƒ½è¢«è°ƒç”¨ï¼Œç”¨äºé‡Šæ”¾èµ„æº
        pass
