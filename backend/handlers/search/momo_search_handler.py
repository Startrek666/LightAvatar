"""
Momo Search Handler - é«˜çº§è”ç½‘æœç´¢å¤„ç†å™¨
é›†æˆ Momo-Search çš„å®Œæ•´åŠŸèƒ½
"""
from typing import List, Dict, Optional, AsyncGenerator
from datetime import datetime
import asyncio

from loguru import logger
from sentence_transformers import SentenceTransformer

from backend.handlers.base import BaseHandler
from .momo_utils import (
    SearchDocument, 
    search_searxng, 
    search_duckduckgo,
    FaissRetriever, 
    convert_to_markdown,
    detect_language,
    translate_text,
    extract_keywords
)
from .momo_crawler import SimpleCrawler
from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url


class MomoSearchHandler(BaseHandler):
    """Momo é«˜çº§æœç´¢å¤„ç†å™¨"""
    
    async def _setup(self):
        """åˆå§‹åŒ–æœç´¢ç»„ä»¶"""
        try:
            # SearXNGé…ç½®
            self.searxng_url = self.config.get('searxng_url', 'http://localhost:9080')
            self.searxng_language = self.config.get('searxng_language', 'zh')
            self.searxng_time_range = self.config.get('searxng_time_range', 'day')
            self.max_search_results = self.config.get('max_search_results', 50)
            
            # å‘é‡æ£€ç´¢é…ç½®
            embedding_model_name = self.config.get(
                'embedding_model', 
                'BAAI/bge-small-zh-v1.5'
            )
            self.num_candidates = self.config.get('num_candidates', 40)
            self.sim_threshold = self.config.get('sim_threshold', 0.45)
            
            # çˆ¬è™«é…ç½®
            self.enable_deep_crawl = self.config.get('enable_deep_crawl', True)
            self.crawl_score_threshold = self.config.get('crawl_score_threshold', 0.5)
            self.max_crawl_docs = self.config.get('max_crawl_docs', 10)
            
            # å…³é”®è¯æå–é…ç½®
            self.enable_keyword_extraction = self.config.get('enable_keyword_extraction', True)
            self.zhipu_api_key = self.config.get('zhipu_api_key', '6f29a799833a4a5daf5752973e9d0cc4.uoelH21xYFMkDknh')
            self.zhipu_model = self.config.get('zhipu_model', 'glm-4.5-flash')
            
            logger.info("ğŸš€ åˆå§‹åŒ– Momo Search Handler...")
            logger.info(f"  SearXNG: {self.searxng_url}")
            logger.info(f"  è¯­è¨€: {self.searxng_language}")
            logger.info(f"  æ—¶é—´èŒƒå›´: {self.searxng_time_range}")
            logger.info(f"  åµŒå…¥æ¨¡å‹: {embedding_model_name}")
            logger.info(f"  æ·±åº¦çˆ¬å–: {'å¼€å¯' if self.enable_deep_crawl else 'å…³é—­'}")
            logger.info(f"  å…³é”®è¯æå–: {'å¼€å¯' if self.enable_keyword_extraction else 'å…³é—­'}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            # CPUä¸æ”¯æŒfloat16ï¼Œä½¿ç”¨float32
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            try:
                if device == "cuda":
                    # GPUå¯ä»¥ä½¿ç”¨float16åŠ é€Ÿ
                    self.embedding_model = SentenceTransformer(
                        embedding_model_name,
                        device=device,
                        model_kwargs={"torch_dtype": torch.float16}
                    )
                else:
                    # CPUå¿…é¡»ä½¿ç”¨float32
                    self.embedding_model = SentenceTransformer(
                        embedding_model_name,
                        device=device,
                        model_kwargs={"torch_dtype": torch.float32}
                    )
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {embedding_model_name} (è®¾å¤‡: {device})")
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.info("â„¹ï¸ å°è¯•ä½¿ç”¨é»˜è®¤è®¾ç½®...")
                self.embedding_model = SentenceTransformer(embedding_model_name, device=device)
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.retriever = FaissRetriever(
                self.embedding_model,
                num_candidates=self.num_candidates,
                sim_threshold=self.sim_threshold
            )
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.crawler = SimpleCrawler(
                timeout=15.0,
                max_concurrent=5
            )
            
            logger.info("âœ… Momo Search Handler åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Momo Search Handler åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_today_date(self) -> str:
        """è·å–ä»Šå¤©çš„æ—¥æœŸ"""
        return datetime.today().strftime('%Y-%m-%d')
    
    def format_sources_for_llm(self, sources: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºLLMå¯ç”¨çš„ä¸Šä¸‹æ–‡
        
        Args:
            sources: æœç´¢æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        sources_str = "\n\n".join([
            f"[ç½‘é¡µ {i+1} å¼€å§‹]\n"
            f"æ ‡é¢˜: {doc.title}\n"
            f"é“¾æ¥: {doc.url}\n"
            f"å†…å®¹: {doc.content if doc.content else doc.snippet}\n"
            f"[ç½‘é¡µ {i+1} ç»“æŸ]"
            for i, doc in enumerate(sources)
        ])
        return sources_str
    
    def format_citations(self, docs: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„å¼•ç”¨åˆ—è¡¨
        """
        citations = []
        for i, doc in enumerate(docs):
            # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
            title = doc.title[:50] + "..." if len(doc.title) > 50 else doc.title
            citations.append(f"{i+1}. [{title}]({doc.url})")
        
        return "\n".join(citations)
    
    async def search_with_progress(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None
    ) -> tuple[List[SearchDocument], str]:
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯)
        """
        try:
            detected_lang = detect_language(query)
            all_search_results = []
            keywords_dict = None  # åˆå§‹åŒ–å…³é”®è¯å­—å…¸
            
            # é¢„å…ˆè®¡ç®—æ€»æ­¥éª¤æ•°ï¼Œç”¨äºä¸€è‡´çš„è¿›åº¦æ˜¾ç¤º
            base_steps = 5  # å…³é”®è¯æå–(1) + å‘é‡æ£€ç´¢(1) + æ·±åº¦çˆ¬å–(1) + æ–‡æ¡£åˆ†å—(1) + å®Œæˆ(1)
            ddg_steps = 2  # DuckDuckGo ä¸­æ–‡ + è‹±æ–‡ï¼ˆæœ€å¤š2æ­¥ï¼‰
            # åˆå§‹ä¼°ç®—æœç´¢æŸ¥è¯¢æ•°é‡ï¼ˆé€šå¸¸æ˜¯2ä¸ªï¼šä¸­æ–‡+è‹±æ–‡å…³é”®è¯ï¼‰
            estimated_search_queries = 2 if self.enable_keyword_extraction else 1
            total_steps = base_steps + estimated_search_queries + ddg_steps
            
            # æ­¥éª¤0: å…³é”®è¯æå–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_keyword_extraction:
                if progress_callback:
                    await progress_callback(0, total_steps, "ğŸ”‘ æå–æœç´¢å…³é”®è¯")
                
                logger.info(f"ğŸ”‘ å¼€å§‹æå–å…³é”®è¯: {query}")
                keywords_dict = extract_keywords(
                    query,
                    api_key=self.zhipu_api_key,
                    model=self.zhipu_model
                )
                
                # å‡†å¤‡æœç´¢æŸ¥è¯¢åˆ—è¡¨
                search_queries = []
                
                if keywords_dict:
                    zh_keys = keywords_dict.get("zh_keys", "").strip()
                    en_keys = keywords_dict.get("en_keys", "").strip()
                    
                    # å¦‚æœæå–åˆ°ä¸­æ–‡å…³é”®è¯ï¼Œæ·»åŠ åˆ°æœç´¢åˆ—è¡¨
                    if zh_keys:
                        search_queries.append({
                            "query": zh_keys,
                            "language": "zh",
                            "source": "keywords_zh"
                        })
                        logger.info(f"âœ… æå–åˆ°ä¸­æ–‡å…³é”®è¯: {zh_keys}")
                    
                    # å¦‚æœæå–åˆ°è‹±æ–‡å…³é”®è¯ï¼Œæ·»åŠ åˆ°æœç´¢åˆ—è¡¨
                    if en_keys:
                        search_queries.append({
                            "query": en_keys,
                            "language": "en",
                            "source": "keywords_en"
                        })
                        logger.info(f"âœ… æå–åˆ°è‹±æ–‡å…³é”®è¯: {en_keys}")
                
                # å¦‚æœå…³é”®è¯æå–å¤±è´¥æˆ–æ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                if not search_queries:
                    logger.warning("âš ï¸ å…³é”®è¯æå–å¤±è´¥æˆ–ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                    search_queries.append({
                        "query": query,
                        "language": detected_lang,
                        "source": "original"
                    })
            else:
                # æœªå¯ç”¨å…³é”®è¯æå–ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
                search_queries = [{
                    "query": query,
                    "language": detected_lang,
                    "source": "original"
                }]
            
            all_search_results = []
            # é‡æ–°è®¡ç®—ç²¾ç¡®çš„æ€»æ­¥éª¤æ•°ï¼ˆåŸºäºå®é™…search_queriesæ•°é‡ï¼‰
            actual_total_steps = base_steps + len(search_queries) + ddg_steps
            if actual_total_steps != total_steps:
                # å¦‚æœå®é™…æ­¥æ•°ä¸ä¼°ç®—ä¸åŒï¼Œæ›´æ–°total_steps
                total_steps = actual_total_steps
            
            # æ­¥éª¤1: ä½¿ç”¨æå–çš„å…³é”®è¯è¿›è¡Œæœç´¢
            for idx, search_item in enumerate(search_queries):
                step_num = idx + 1
                if progress_callback:
                    await progress_callback(step_num, total_steps, f"ğŸ” æ­£åœ¨æœç´¢: {search_item['query']} ({search_item['source']})")
                
                logger.info(f"ğŸ” å¼€å§‹æœç´¢: {search_item['query']} (è¯­è¨€: {search_item['language']}, æ¥æº: {search_item['source']})")
                
                search_results = search_searxng(
                    query=search_item['query'],
                    num_results=self.max_search_results,
                    ip_address=self.searxng_url,
                    language=search_item['language'],
                    time_range=self.searxng_time_range,
                    deduplicate_by_url=True
                )
                
                # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                seen_urls = {doc.url for doc in all_search_results}
                for doc in search_results:
                    if doc.url not in seen_urls:
                        all_search_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… {search_item['source']}æœç´¢å®Œæˆ: è·å¾—{len(search_results)}ä¸ªç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            # å¦‚æœåªæå–åˆ°ä¸­æ–‡å…³é”®è¯ä½†æ²¡æœ‰è‹±æ–‡å…³é”®è¯ï¼Œä¸”åŸå§‹æŸ¥è¯¢æ˜¯ä¸­æ–‡ï¼Œå°è¯•ç¿»è¯‘å¹¶æœç´¢
            if detected_lang == "zh" and keywords_dict and keywords_dict.get("en_keys") and not any(item['source'] == 'keywords_en' for item in search_queries):
                if progress_callback:
                    await progress_callback(2, total_steps, "ğŸŒ ç¿»è¯‘æŸ¥è¯¢å¹¶æœç´¢è‹±æ–‡ç»“æœ")
                
                # ç¿»è¯‘åŸå§‹æŸ¥è¯¢ä½œä¸ºè¡¥å……
                translated_query = translate_text(query, source="zh", target="en")
                
                if translated_query:
                    logger.info(f"ğŸŒ ç¿»è¯‘ç»“æœ: {query} -> {translated_query}")
                    
                    english_search_results = search_searxng(
                        query=translated_query,
                        num_results=self.max_search_results,
                        ip_address=self.searxng_url,
                        language="en",
                        time_range=self.searxng_time_range,
                        deduplicate_by_url=True
                    )
                    
                    # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                    seen_urls = {doc.url for doc in all_search_results}
                    for doc in english_search_results:
                        if doc.url not in seen_urls:
                            all_search_results.append(doc)
                            seen_urls.add(doc.url)
                    
                    logger.info(f"âœ… ç¿»è¯‘æœç´¢å®Œæˆ: è·å¾—{len(english_search_results)}ä¸ªæ–°ç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            # æ­¥éª¤2: ä½¿ç”¨ DuckDuckGo è¿›è¡Œè¡¥å……æœç´¢ï¼ˆä¸­è‹±æ–‡å„20æ¡ï¼‰
            # å‡†å¤‡ DuckDuckGo æœç´¢æŸ¥è¯¢
            ddg_queries = []
            
            # å¦‚æœæœ‰ä¸­æ–‡å…³é”®è¯ï¼Œä½¿ç”¨ä¸­æ–‡å…³é”®è¯ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            if keywords_dict and keywords_dict.get("zh_keys"):
                ddg_queries.append({
                    "query": keywords_dict.get("zh_keys"),
                    "language": "zh",
                    "source": "ddg_zh"
                })
            elif detected_lang == "zh":
                ddg_queries.append({
                    "query": query,
                    "language": "zh",
                    "source": "ddg_zh"
                })
            
            # å¦‚æœæœ‰è‹±æ–‡å…³é”®è¯ï¼Œä½¿ç”¨è‹±æ–‡å…³é”®è¯ï¼›å¦åˆ™å°è¯•ç¿»è¯‘
            if keywords_dict and keywords_dict.get("en_keys"):
                ddg_queries.append({
                    "query": keywords_dict.get("en_keys"),
                    "language": "en",
                    "source": "ddg_en"
                })
            elif detected_lang == "en":
                ddg_queries.append({
                    "query": query,
                    "language": "en",
                    "source": "ddg_en"
                })
            elif detected_lang == "zh":
                # ä¸­æ–‡æŸ¥è¯¢å°è¯•ç¿»è¯‘ä¸ºè‹±æ–‡
                translated_query = translate_text(query, source="zh", target="en")
                if translated_query:
                    ddg_queries.append({
                        "query": translated_query,
                        "language": "en",
                        "source": "ddg_en_translated"
                    })
            
            # æ‰§è¡Œ DuckDuckGo æœç´¢
            for idx, ddg_item in enumerate(ddg_queries):
                step_num = len(search_queries) + idx + 1
                if progress_callback:
                    await progress_callback(
                        step_num, 
                        total_steps, 
                        f"ğŸ¦† DuckDuckGo {ddg_item['language']}æœç´¢"
                    )
                
                logger.info(f"ğŸ¦† å¼€å§‹DuckDuckGoæœç´¢: {ddg_item['query']} (è¯­è¨€: {ddg_item['language']})")
                
                ddg_results = await search_duckduckgo(
                    query=ddg_item['query'],
                    max_results=20,
                    language=ddg_item['language'],
                    time_range=self.searxng_time_range if self.searxng_time_range else None
                )
                
                # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                seen_urls = {doc.url for doc in all_search_results}
                for doc in ddg_results:
                    if doc.url not in seen_urls:
                        all_search_results.append(doc)
                        seen_urls.add(doc.url)
                
                logger.info(f"âœ… DuckDuckGo {ddg_item['source']}æœç´¢å®Œæˆ: è·å¾—{len(ddg_results)}ä¸ªç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
            
            if not all_search_results:
                logger.warning("âš ï¸ æ‰€æœ‰æœç´¢å‡æœªè¿”å›ç»“æœ")
                return [], ""
            
            # æ­¥éª¤3: å‘é‡æ£€ç´¢ï¼ˆåˆå¹¶åçš„ç»“æœï¼‰
            vector_step = len(search_queries) + len(ddg_queries) + 1
            if progress_callback:
                await progress_callback(vector_step, total_steps, f"ğŸ“Š åˆ†æç›¸å…³æ€§ ({len(all_search_results)}ä¸ªç»“æœ)")
            
            self.retriever.add_documents(all_search_results)
            relevant_docs = self.retriever.get_relevant_documents(query)
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], ""
            
            logger.info(f"âœ… æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ­¥éª¤4: æ·±åº¦çˆ¬å– (ä»…qualityæ¨¡å¼)
            if mode == "quality" and self.enable_deep_crawl:
                crawl_step = vector_step + 1
                if progress_callback:
                    await progress_callback(crawl_step, total_steps, f"ğŸ•·ï¸ æ·±åº¦çˆ¬å–å†…å®¹ (å‰{self.max_crawl_docs}ä¸ª)")
                
                await self.crawler.crawl_many(
                    relevant_docs,
                    score_threshold=self.crawl_score_threshold,
                    max_docs=self.max_crawl_docs
                )
                
                # æ­¥éª¤5: æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢
                split_step = crawl_step + 1
                if progress_callback:
                    await progress_callback(split_step, total_steps, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                
                docs_with_details = expand_docs_by_text_split(relevant_docs)
                self.retriever.add_documents(docs_with_details)
                relevant_docs_detailed = self.retriever.get_relevant_documents(query)
                relevant_docs = merge_docs_by_url(relevant_docs_detailed)
                
                logger.info(f"ğŸ“„ äºŒæ¬¡æ£€ç´¢å: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            # æœ€åä¸€æ­¥: å®Œæˆ
            if progress_callback:
                await progress_callback(total_steps, total_steps, "âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå†…å®¹")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            citations = self.format_citations(relevant_docs)
            
            logger.info(f"âœ… Momoæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£ (è¯­è¨€: {detected_lang})")
            return relevant_docs, citations
            
        except Exception as e:
            logger.error(f"âŒ Momoæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return [], ""
    
    async def process(self, query: str, mode: str = "speed") -> Dict:
        """
        å¤„ç†æœç´¢è¯·æ±‚ï¼ˆåŒæ­¥æ¥å£ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼
        
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        relevant_docs, citations = await self.search_with_progress(query, mode)
        
        if not relevant_docs:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
            }
        
        # æ ¼å¼åŒ–ä¸ºLLMä¸Šä¸‹æ–‡
        context = self.format_sources_for_llm(relevant_docs)
        
        return {
            "success": True,
            "context": context,
            "citations": citations,
            "num_results": len(relevant_docs),
            "documents": relevant_docs
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'crawler'):
            await self.crawler.close()
        logger.info("ğŸ§¹ Momo Search Handler èµ„æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ³¨æ„ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œææ„å‡½æ•°ä¸­çš„å¼‚æ­¥è°ƒç”¨å¯èƒ½ä¸ä¼šæ‰§è¡Œ
        pass



