"""
Momo Search Handler - é«˜çº§è”ç½‘æœç´¢å¤„ç†å™¨
é›†æˆ Momo-Search çš„å®Œæ•´åŠŸèƒ½
"""
from typing import List, Dict, Optional, AsyncGenerator
from datetime import datetime
import asyncio

from loguru import logger
from sentence_transformers import SentenceTransformer

from backend.handlers.base import BaseHandler
from .momo_utils import SearchDocument, search_searxng, FaissRetriever, convert_to_markdown
from .momo_crawler import SimpleCrawler
from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url


class MomoSearchHandler(BaseHandler):
    """Momo é«˜çº§æœç´¢å¤„ç†å™¨"""
    
    async def _setup(self):
        """åˆå§‹åŒ–æœç´¢ç»„ä»¶"""
        try:
            # SearXNGé…ç½®
            self.searxng_url = self.config.get('searxng_url', 'http://localhost:9080')
            self.searxng_language = self.config.get('searxng_language', 'zh')
            self.searxng_time_range = self.config.get('searxng_time_range', 'day')
            self.max_search_results = self.config.get('max_search_results', 50)
            
            # å‘é‡æ£€ç´¢é…ç½®
            embedding_model_name = self.config.get(
                'embedding_model', 
                'BAAI/bge-small-zh-v1.5'
            )
            self.num_candidates = self.config.get('num_candidates', 40)
            self.sim_threshold = self.config.get('sim_threshold', 0.45)
            
            # çˆ¬è™«é…ç½®
            self.enable_deep_crawl = self.config.get('enable_deep_crawl', True)
            self.crawl_score_threshold = self.config.get('crawl_score_threshold', 0.5)
            self.max_crawl_docs = self.config.get('max_crawl_docs', 10)
            
            logger.info("ğŸš€ åˆå§‹åŒ– Momo Search Handler...")
            logger.info(f"  SearXNG: {self.searxng_url}")
            logger.info(f"  è¯­è¨€: {self.searxng_language}")
            logger.info(f"  æ—¶é—´èŒƒå›´: {self.searxng_time_range}")
            logger.info(f"  åµŒå…¥æ¨¡å‹: {embedding_model_name}")
            logger.info(f"  æ·±åº¦çˆ¬å–: {'å¼€å¯' if self.enable_deep_crawl else 'å…³é—­'}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            try:
                self.embedding_model = SentenceTransformer(
                    embedding_model_name,
                    model_kwargs={"torch_dtype": "float16"}
                )
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {embedding_model_name}")
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.info("â„¹ï¸ å°è¯•ä½¿ç”¨CPUç‰ˆæœ¬...")
                self.embedding_model = SentenceTransformer(embedding_model_name)
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.retriever = FaissRetriever(
                self.embedding_model,
                num_candidates=self.num_candidates,
                sim_threshold=self.sim_threshold
            )
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.crawler = SimpleCrawler(
                timeout=15.0,
                max_concurrent=5
            )
            
            logger.info("âœ… Momo Search Handler åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Momo Search Handler åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_today_date(self) -> str:
        """è·å–ä»Šå¤©çš„æ—¥æœŸ"""
        return datetime.today().strftime('%Y-%m-%d')
    
    def format_sources_for_llm(self, sources: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºLLMå¯ç”¨çš„ä¸Šä¸‹æ–‡
        
        Args:
            sources: æœç´¢æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        sources_str = "\n\n".join([
            f"[ç½‘é¡µ {i+1} å¼€å§‹]\n"
            f"æ ‡é¢˜: {doc.title}\n"
            f"é“¾æ¥: {doc.url}\n"
            f"å†…å®¹: {doc.content if doc.content else doc.snippet}\n"
            f"[ç½‘é¡µ {i+1} ç»“æŸ]"
            for i, doc in enumerate(sources)
        ])
        return sources_str
    
    def format_citations(self, docs: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„å¼•ç”¨åˆ—è¡¨
        """
        citations = []
        for i, doc in enumerate(docs):
            # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
            title = doc.title[:50] + "..." if len(doc.title) > 50 else doc.title
            citations.append(f"{i+1}. [{title}]({doc.url})")
        
        return "\n".join(citations)
    
    async def search_with_progress(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None
    ) -> tuple[List[SearchDocument], str]:
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯)
        """
        try:
            # æ­¥éª¤1: SearXNGæœç´¢
            if progress_callback:
                await progress_callback(1, 5, f"ğŸ” æ­£åœ¨æœç´¢: {query}")
            
            logger.info(f"ğŸ” å¼€å§‹Momoæœç´¢: {query} (æ¨¡å¼: {mode})")
            
            search_results = search_searxng(
                query=query,
                num_results=self.max_search_results,
                ip_address=self.searxng_url,
                language=self.searxng_language,
                time_range=self.searxng_time_range
            )
            
            if not search_results:
                logger.warning("âš ï¸ SearXNGæœç´¢æœªè¿”å›ç»“æœ")
                return [], ""
            
            # æ­¥éª¤2: å‘é‡æ£€ç´¢
            if progress_callback:
                await progress_callback(2, 5, f"ğŸ“Š åˆ†æç›¸å…³æ€§ ({len(search_results)}ä¸ªç»“æœ)")
            
            self.retriever.add_documents(search_results)
            relevant_docs = self.retriever.get_relevant_documents(query)
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], ""
            
            logger.info(f"âœ… æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ­¥éª¤3: æ·±åº¦çˆ¬å– (ä»…qualityæ¨¡å¼)
            if mode == "quality" and self.enable_deep_crawl:
                if progress_callback:
                    await progress_callback(3, 5, f"ğŸ•·ï¸ æ·±åº¦çˆ¬å–å†…å®¹ (å‰{self.max_crawl_docs}ä¸ª)")
                
                await self.crawler.crawl_many(
                    relevant_docs,
                    score_threshold=self.crawl_score_threshold,
                    max_docs=self.max_crawl_docs
                )
                
                # æ­¥éª¤4: æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢
                if progress_callback:
                    await progress_callback(4, 5, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                
                docs_with_details = expand_docs_by_text_split(relevant_docs)
                self.retriever.add_documents(docs_with_details)
                relevant_docs_detailed = self.retriever.get_relevant_documents(query)
                relevant_docs = merge_docs_by_url(relevant_docs_detailed)
                
                logger.info(f"ğŸ“„ äºŒæ¬¡æ£€ç´¢å: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            # æ­¥éª¤5: å®Œæˆ
            if progress_callback:
                await progress_callback(5, 5, "âœ… æœç´¢å®Œæˆ")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            citations = self.format_citations(relevant_docs)
            
            logger.info(f"âœ… Momoæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£")
            return relevant_docs, citations
            
        except Exception as e:
            logger.error(f"âŒ Momoæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return [], ""
    
    async def process(self, query: str, mode: str = "speed") -> Dict:
        """
        å¤„ç†æœç´¢è¯·æ±‚ï¼ˆåŒæ­¥æ¥å£ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼
        
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        relevant_docs, citations = await self.search_with_progress(query, mode)
        
        if not relevant_docs:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
            }
        
        # æ ¼å¼åŒ–ä¸ºLLMä¸Šä¸‹æ–‡
        context = self.format_sources_for_llm(relevant_docs)
        
        return {
            "success": True,
            "context": context,
            "citations": citations,
            "num_results": len(relevant_docs),
            "documents": relevant_docs
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'crawler'):
            await self.crawler.close()
        logger.info("ğŸ§¹ Momo Search Handler èµ„æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ³¨æ„ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œææ„å‡½æ•°ä¸­çš„å¼‚æ­¥è°ƒç”¨å¯èƒ½ä¸ä¼šæ‰§è¡Œ
        pass



