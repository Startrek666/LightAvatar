"""
Momo Search Handler - é«˜çº§è”ç½‘æœç´¢å¤„ç†å™¨
é›†æˆ Momo-Search çš„å®Œæ•´åŠŸèƒ½
"""
from typing import List, Dict, Optional, AsyncGenerator
from datetime import datetime
import asyncio

from loguru import logger
from sentence_transformers import SentenceTransformer

from backend.handlers.base import BaseHandler
from .momo_utils import (
    SearchDocument, 
    search_searxng, 
    FaissRetriever, 
    convert_to_markdown,
    detect_language,
    translate_text
)
from .momo_crawler import SimpleCrawler
from .momo_retriever import expand_docs_by_text_split, merge_docs_by_url


class MomoSearchHandler(BaseHandler):
    """Momo é«˜çº§æœç´¢å¤„ç†å™¨"""
    
    async def _setup(self):
        """åˆå§‹åŒ–æœç´¢ç»„ä»¶"""
        try:
            # SearXNGé…ç½®
            self.searxng_url = self.config.get('searxng_url', 'http://localhost:9080')
            self.searxng_language = self.config.get('searxng_language', 'zh')
            self.searxng_time_range = self.config.get('searxng_time_range', 'day')
            self.max_search_results = self.config.get('max_search_results', 50)
            
            # å‘é‡æ£€ç´¢é…ç½®
            embedding_model_name = self.config.get(
                'embedding_model', 
                'BAAI/bge-small-zh-v1.5'
            )
            self.num_candidates = self.config.get('num_candidates', 40)
            self.sim_threshold = self.config.get('sim_threshold', 0.45)
            
            # çˆ¬è™«é…ç½®
            self.enable_deep_crawl = self.config.get('enable_deep_crawl', True)
            self.crawl_score_threshold = self.config.get('crawl_score_threshold', 0.5)
            self.max_crawl_docs = self.config.get('max_crawl_docs', 10)
            
            logger.info("ğŸš€ åˆå§‹åŒ– Momo Search Handler...")
            logger.info(f"  SearXNG: {self.searxng_url}")
            logger.info(f"  è¯­è¨€: {self.searxng_language}")
            logger.info(f"  æ—¶é—´èŒƒå›´: {self.searxng_time_range}")
            logger.info(f"  åµŒå…¥æ¨¡å‹: {embedding_model_name}")
            logger.info(f"  æ·±åº¦çˆ¬å–: {'å¼€å¯' if self.enable_deep_crawl else 'å…³é—­'}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            # CPUä¸æ”¯æŒfloat16ï¼Œä½¿ç”¨float32
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            try:
                if device == "cuda":
                    # GPUå¯ä»¥ä½¿ç”¨float16åŠ é€Ÿ
                    self.embedding_model = SentenceTransformer(
                        embedding_model_name,
                        device=device,
                        model_kwargs={"torch_dtype": torch.float16}
                    )
                else:
                    # CPUå¿…é¡»ä½¿ç”¨float32
                    self.embedding_model = SentenceTransformer(
                        embedding_model_name,
                        device=device,
                        model_kwargs={"torch_dtype": torch.float32}
                    )
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {embedding_model_name} (è®¾å¤‡: {device})")
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.info("â„¹ï¸ å°è¯•ä½¿ç”¨é»˜è®¤è®¾ç½®...")
                self.embedding_model = SentenceTransformer(embedding_model_name, device=device)
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.retriever = FaissRetriever(
                self.embedding_model,
                num_candidates=self.num_candidates,
                sim_threshold=self.sim_threshold
            )
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.crawler = SimpleCrawler(
                timeout=15.0,
                max_concurrent=5
            )
            
            logger.info("âœ… Momo Search Handler åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Momo Search Handler åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_today_date(self) -> str:
        """è·å–ä»Šå¤©çš„æ—¥æœŸ"""
        return datetime.today().strftime('%Y-%m-%d')
    
    def format_sources_for_llm(self, sources: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºLLMå¯ç”¨çš„ä¸Šä¸‹æ–‡
        
        Args:
            sources: æœç´¢æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        sources_str = "\n\n".join([
            f"[ç½‘é¡µ {i+1} å¼€å§‹]\n"
            f"æ ‡é¢˜: {doc.title}\n"
            f"é“¾æ¥: {doc.url}\n"
            f"å†…å®¹: {doc.content if doc.content else doc.snippet}\n"
            f"[ç½‘é¡µ {i+1} ç»“æŸ]"
            for i, doc in enumerate(sources)
        ])
        return sources_str
    
    def format_citations(self, docs: List[SearchDocument]) -> str:
        """
        æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„å¼•ç”¨åˆ—è¡¨
        """
        citations = []
        for i, doc in enumerate(docs):
            # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
            title = doc.title[:50] + "..." if len(doc.title) > 50 else doc.title
            citations.append(f"{i+1}. [{title}]({doc.url})")
        
        return "\n".join(citations)
    
    async def search_with_progress(
        self,
        query: str,
        mode: str = "speed",
        progress_callback: Optional[callable] = None
    ) -> tuple[List[SearchDocument], str]:
        """
        æ‰§è¡Œæœç´¢å¹¶æŠ¥å‘Šè¿›åº¦
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼ (speed/quality)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            (ç›¸å…³æ–‡æ¡£åˆ—è¡¨, å¼•ç”¨ä¿¡æ¯)
        """
        try:
            # æ­¥éª¤0: æ£€æµ‹è¯­è¨€å¹¶å†³å®šæœç´¢ç­–ç•¥
            detected_lang = detect_language(query)
            logger.info(f"ğŸ” æ£€æµ‹åˆ°æŸ¥è¯¢è¯­è¨€: {detected_lang} (æŸ¥è¯¢: {query})")
            
            all_search_results = []
            total_steps = 6 if detected_lang == "zh" else 5  # ä¸­æ–‡éœ€è¦é¢å¤–ç¿»è¯‘æ­¥éª¤
            
            # æ­¥éª¤1: é¦–æ¬¡æœç´¢ï¼ˆæ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€ï¼‰
            if progress_callback:
                await progress_callback(1, total_steps, f"ğŸ” æ­£åœ¨æœç´¢: {query}")
            
            logger.info(f"ğŸ” å¼€å§‹Momoæœç´¢: {query} (æ¨¡å¼: {mode}, è¯­è¨€: {detected_lang})")
            
            # é¦–æ¬¡æœç´¢ï¼šä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€
            first_search_results = search_searxng(
                query=query,
                num_results=self.max_search_results,
                ip_address=self.searxng_url,
                language=detected_lang,
                time_range=self.searxng_time_range,
                deduplicate_by_url=True
            )
            
            all_search_results.extend(first_search_results)
            logger.info(f"âœ… é¦–æ¬¡æœç´¢å®Œæˆ: è·å¾—{len(first_search_results)}ä¸ªç»“æœ")
            
            # æ­¥éª¤2: å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç¿»è¯‘å¹¶å†æ¬¡æœç´¢
            if detected_lang == "zh":
                if progress_callback:
                    await progress_callback(2, total_steps, "ğŸŒ ç¿»è¯‘æŸ¥è¯¢å¹¶æœç´¢è‹±æ–‡ç»“æœ")
                
                # ç¿»è¯‘æŸ¥è¯¢
                translated_query = translate_text(query, source="zh", target="en")
                
                if translated_query:
                    logger.info(f"ğŸŒ ç¿»è¯‘ç»“æœ: {query} -> {translated_query}")
                    
                    # ä½¿ç”¨è‹±æ–‡æŸ¥è¯¢å†æ¬¡æœç´¢
                    english_search_results = search_searxng(
                        query=translated_query,
                        num_results=self.max_search_results,
                        ip_address=self.searxng_url,
                        language="en",
                        time_range=self.searxng_time_range,
                        deduplicate_by_url=True
                    )
                    
                    # åˆå¹¶ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                    seen_urls = {doc.url for doc in all_search_results}
                    for doc in english_search_results:
                        if doc.url not in seen_urls:
                            all_search_results.append(doc)
                            seen_urls.add(doc.url)
                    
                    logger.info(f"âœ… è‹±æ–‡æœç´¢å®Œæˆ: è·å¾—{len(english_search_results)}ä¸ªæ–°ç»“æœï¼Œæ€»è®¡{len(all_search_results)}ä¸ª")
                else:
                    logger.warning("âš ï¸ ç¿»è¯‘å¤±è´¥ï¼Œè·³è¿‡è‹±æ–‡æœç´¢")
            else:
                logger.info("â„¹ï¸ æŸ¥è¯¢ä¸ºè‹±æ–‡ï¼Œè·³è¿‡ç¿»è¯‘æ­¥éª¤")
            
            if not all_search_results:
                logger.warning("âš ï¸ æ‰€æœ‰æœç´¢å‡æœªè¿”å›ç»“æœ")
                return [], ""
            
            # æ­¥éª¤3: å‘é‡æ£€ç´¢ï¼ˆåˆå¹¶åçš„ç»“æœï¼‰
            step_num = 3 if detected_lang == "zh" else 2
            if progress_callback:
                await progress_callback(step_num, total_steps, f"ğŸ“Š åˆ†æç›¸å…³æ€§ ({len(all_search_results)}ä¸ªç»“æœ)")
            
            self.retriever.add_documents(all_search_results)
            relevant_docs = self.retriever.get_relevant_documents(query)
            
            if not relevant_docs:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return [], ""
            
            logger.info(f"âœ… æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ­¥éª¤4: æ·±åº¦çˆ¬å– (ä»…qualityæ¨¡å¼)
            step_num = 4 if detected_lang == "zh" else 3
            if mode == "quality" and self.enable_deep_crawl:
                if progress_callback:
                    await progress_callback(step_num, total_steps, f"ğŸ•·ï¸ æ·±åº¦çˆ¬å–å†…å®¹ (å‰{self.max_crawl_docs}ä¸ª)")
                
                await self.crawler.crawl_many(
                    relevant_docs,
                    score_threshold=self.crawl_score_threshold,
                    max_docs=self.max_crawl_docs
                )
                
                # æ­¥éª¤5: æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢
                step_num = 5 if detected_lang == "zh" else 4
                if progress_callback:
                    await progress_callback(step_num, total_steps, "âœ‚ï¸ æ–‡æ¡£åˆ†å—å’ŒäºŒæ¬¡æ£€ç´¢")
                
                docs_with_details = expand_docs_by_text_split(relevant_docs)
                self.retriever.add_documents(docs_with_details)
                relevant_docs_detailed = self.retriever.get_relevant_documents(query)
                relevant_docs = merge_docs_by_url(relevant_docs_detailed)
                
                logger.info(f"ğŸ“„ äºŒæ¬¡æ£€ç´¢å: {len(relevant_docs)}ä¸ªæ–‡æ¡£")
            
            # æœ€åä¸€æ­¥: å®Œæˆ
            final_step = total_steps
            if progress_callback:
                await progress_callback(final_step, total_steps, "âœ… æœç´¢å®Œæˆ")
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            citations = self.format_citations(relevant_docs)
            
            logger.info(f"âœ… Momoæœç´¢å®Œæˆ: è¿”å›{len(relevant_docs)}ä¸ªæ–‡æ¡£ (è¯­è¨€: {detected_lang})")
            return relevant_docs, citations
            
        except Exception as e:
            logger.error(f"âŒ Momoæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return [], ""
    
    async def process(self, query: str, mode: str = "speed") -> Dict:
        """
        å¤„ç†æœç´¢è¯·æ±‚ï¼ˆåŒæ­¥æ¥å£ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            mode: æœç´¢æ¨¡å¼
        
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        relevant_docs, citations = await self.search_with_progress(query, mode)
        
        if not relevant_docs:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
            }
        
        # æ ¼å¼åŒ–ä¸ºLLMä¸Šä¸‹æ–‡
        context = self.format_sources_for_llm(relevant_docs)
        
        return {
            "success": True,
            "context": context,
            "citations": citations,
            "num_results": len(relevant_docs),
            "documents": relevant_docs
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'crawler'):
            await self.crawler.close()
        logger.info("ğŸ§¹ Momo Search Handler èµ„æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ³¨æ„ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œææ„å‡½æ•°ä¸­çš„å¼‚æ­¥è°ƒç”¨å¯èƒ½ä¸ä¼šæ‰§è¡Œ
        pass



