"""
Momo Search Crawler - ç½‘é¡µå†…å®¹çˆ¬å–
ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ trafilatura æ›¿ä»£ crawl4ai
"""
import asyncio
from typing import List
import httpx
from loguru import logger

try:
    import trafilatura
    TRAFILATURA_AVAILABLE = True
except ImportError:
    TRAFILATURA_AVAILABLE = False
    logger.warning("âš ï¸ trafilatura æœªå®‰è£…ï¼Œæ·±åº¦çˆ¬å–åŠŸèƒ½å°†ä¸å¯ç”¨")

from .momo_utils import SearchDocument


class SimpleCrawler:
    """ç®€åŒ–ç‰ˆç½‘é¡µçˆ¬è™«"""
    
    def __init__(self, timeout: float = 15.0, max_concurrent: int = 5):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        """
        self.timeout = timeout
        self.max_concurrent = max_concurrent
        self.http_client = None
        logger.info(f"ğŸ•·ï¸ ç®€åŒ–çˆ¬è™«åˆå§‹åŒ–: timeout={timeout}s, å¹¶å‘={max_concurrent}")
    
    async def _get_client(self):
        """è·å–æˆ–åˆ›å»ºHTTPå®¢æˆ·ç«¯"""
        if self.http_client is None:
            self.http_client = httpx.AsyncClient(
                timeout=self.timeout,
                follow_redirects=True,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
            )
        return self.http_client
    
    async def crawl_one(self, doc: SearchDocument) -> bool:
        """
        çˆ¬å–å•ä¸ªæ–‡æ¡£çš„å†…å®¹
        
        Args:
            doc: æœç´¢æ–‡æ¡£å¯¹è±¡ï¼ˆä¼šç›´æ¥ä¿®æ”¹å…¶contentå­—æ®µï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not TRAFILATURA_AVAILABLE:
            logger.warning(f"âš ï¸ trafilatura æœªå®‰è£…ï¼Œè·³è¿‡ {doc.url}")
            return False
        
        try:
            client = await self._get_client()
            
            # è·å–ç½‘é¡µå†…å®¹
            response = await client.get(doc.url)
            response.raise_for_status()
            
            # ä½¿ç”¨ trafilatura æå–æ­£æ–‡
            content = trafilatura.extract(
                response.text,
                include_comments=False,
                include_tables=True,
                no_fallback=False
            )
            
            if content and len(content) > 100:
                doc.content = content
                logger.info(f"âœ… æˆåŠŸçˆ¬å–: {doc.url[:60]}... ({len(content)}å­—)")
                return True
            else:
                logger.warning(f"âš ï¸ æå–å†…å®¹ä¸ºç©º: {doc.url[:60]}...")
                return False
        
        except httpx.HTTPStatusError as e:
            logger.warning(f"âš ï¸ HTTPé”™è¯¯ {e.response.status_code}: {doc.url[:60]}...")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ çˆ¬å–å¤±è´¥: {doc.url[:60]}... - {str(e)[:50]}")
            return False
    
    async def crawl_many(
        self, 
        docs: List[SearchDocument],
        score_threshold: float = 0.5,
        max_docs: int = 10
    ):
        """
        æ‰¹é‡çˆ¬å–å¤šä¸ªæ–‡æ¡£
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            score_threshold: åªçˆ¬å–ç›¸ä¼¼åº¦é«˜äºæ­¤é˜ˆå€¼çš„æ–‡æ¡£
            max_docs: æœ€å¤šçˆ¬å–çš„æ–‡æ¡£æ•°é‡
        """
        if not TRAFILATURA_AVAILABLE:
            logger.warning("âš ï¸ trafilatura æœªå®‰è£…ï¼Œè·³è¿‡æ·±åº¦çˆ¬å–")
            return
        
        # è¿‡æ»¤æ–‡æ¡£ï¼šåªçˆ¬å–é«˜åˆ†æ–‡æ¡£
        filtered_docs = [
            doc for doc in docs 
            if doc.score > score_threshold
        ][:max_docs]
        
        if not filtered_docs:
            logger.info("â„¹ï¸ æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·±åº¦çˆ¬å–")
            return
        
        logger.info(f"ğŸ•·ï¸ å¼€å§‹æ·±åº¦çˆ¬å–: {len(filtered_docs)}/{len(docs)} ä¸ªæ–‡æ¡£")
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_semaphore(doc):
            async with semaphore:
                return await self.crawl_one(doc)
        
        # å¹¶å‘çˆ¬å–
        tasks = [crawl_with_semaphore(doc) for doc in filtered_docs]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r is True)
        logger.info(f"âœ… æ·±åº¦çˆ¬å–å®Œæˆ: {success_count}/{len(filtered_docs)} æˆåŠŸ")
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        if self.http_client:
            await self.http_client.aclose()
            self.http_client = None



