"""
Momo Search Utils - æœç´¢å·¥å…·å‡½æ•°
"""
from dataclasses import dataclass
import urllib.parse
from json import JSONDecodeError
from typing import List, Optional
import re

import faiss
import numpy as np
import requests
from loguru import logger

# ç¿»è¯‘APIé…ç½®
TRANSLATE_API_URL = "https://api-utils.lemomate.com/translate"
TRANSLATE_API_KEY = "L5kGzmjwqXbk0ViD@"


@dataclass
class SearchDocument:
    """æœç´¢ç»“æœæ–‡æ¡£"""
    title: str = ""
    url: str = ""
    snippet: str = ""
    content: str = ""
    score: float = 0.0


def encode_url(url: str) -> str:
    """URLç¼–ç """
    return urllib.parse.quote(url)


def decode_url(url: str) -> str:
    """URLè§£ç """
    return urllib.parse.unquote(url)


def escape_markdown(text: str) -> str:
    """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""
    special_chars = r'_\*\[\]\(\)~`>#\+\-=\|\{\}\.\!'
    return re.sub(f'([{special_chars}])', r'\\\1', text)


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
    
    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡
    """
    if not text:
        return "en"
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s+', '', text))  # å»é™¤ç©ºæ ¼åçš„æ€»å­—ç¬¦æ•°
    
    if total_chars == 0:
        return "en"
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    if chinese_ratio > 0.3:
        return "zh"
    else:
        return "en"


def translate_text(query: str, source: str = "zh", target: str = "en") -> Optional[str]:
    """
    è°ƒç”¨ç¿»è¯‘APIç¿»è¯‘æ–‡æœ¬
    
    Args:
        query: è¦ç¿»è¯‘çš„æ–‡æœ¬
        source: æºè¯­è¨€ (zh/en)
        target: ç›®æ ‡è¯­è¨€ (zh/en)
    
    Returns:
        ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": TRANSLATE_API_KEY
        }
        
        data = {
            "q": query,
            "source": source,
            "target": target
        }
        
        response = requests.post(
            TRANSLATE_API_URL,
            headers=headers,
            json=data,
            timeout=10
        )
        
        response.raise_for_status()
        result = response.json()
        
        translated_text = result.get("translatedText", "")
        if translated_text:
            logger.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {query} -> {translated_text}")
            return translated_text
        else:
            logger.warning(f"âš ï¸ ç¿»è¯‘APIè¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        return None


def convert_to_markdown(text: str) -> str:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    lines = text.split('\n')
    result = []
    
    for line in lines:
        line = line.strip()
        
        if not line:
            result.append('\n')
            continue
        
        # å¤„ç†å¼•ç”¨æ ‡è®° [citation:X]
        if '[citation:' in line:
            line = re.sub(r'\[citation:(\d+)\]', r'[\1]', line)
        
        # å¤„ç†æ ‡é¢˜
        if line.startswith('#'):
            header_text = line.strip('#').strip()
            # å¤„ç†åŠ ç²—æ–‡æœ¬
            if '**' in header_text:
                header_text = re.sub(r'\*\*(.*?)\*\*', r'**\1**', header_text)
            result.append(f"{line}\n")
        
        # å¤„ç†åˆ—è¡¨é¡¹
        elif line.strip().startswith('- '):
            bullet_text = line.strip()[2:]
            result.append(f"- {bullet_text}\n")
        
        # å¤„ç†åˆ†éš”çº¿
        elif line.strip() == '---':
            result.append("---\n")
        
        # å¤„ç†æ™®é€šæ–‡æœ¬
        else:
            result.append(f"{line}\n")
    
    return ''.join(result)


def search_searxng(
    query: str,
    num_results: int,
    ip_address: str = "http://localhost:9080",
    language: str = "zh",
    time_range: str = "",
    deduplicate_by_url: bool = True
) -> List[SearchDocument]:
    """
    ä½¿ç”¨SearXNGæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: éœ€è¦çš„ç»“æœæ•°é‡
        ip_address: SearXNGæœåŠ¡åœ°å€
        language: æœç´¢è¯­è¨€ (zh/en)
        time_range: æ—¶é—´èŒƒå›´ (day/week/month/year/"")
    
    Returns:
        æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5"
    }
    
    # æ„å»ºè¯·æ±‚URL
    params = {
        "q": query,
        "format": "json",
        "language": language,
    }
    
    # æ·»åŠ æ—¶é—´èŒƒå›´å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
    if time_range:
        params["time_range"] = time_range
    
    # æ„å»ºåŸºç¡€URL
    base_url = ip_address.rstrip('/')
    if not base_url.startswith('http'):
        base_url = f"http://{base_url}"
    
    res = []
    seen_urls = set() if deduplicate_by_url else None
    pageno = 1
    
    while len(res) < num_results:
        params["pageno"] = pageno
        query_string = urllib.parse.urlencode(params)
        url = f"{base_url}/search?{query_string}"
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            response_dict = response.json()
            
        except JSONDecodeError as e:
            logger.error(f"âŒ SearXNG JSONè§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
            raise ValueError("JSONDecodeError: è¯·ç¡®ä¿SearXNGå®ä¾‹å¯ä»¥è¿”å›JSONæ ¼å¼æ•°æ®")
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ SearXNGè¯·æ±‚å¤±è´¥: {e}")
            raise
        
        result_dicts = response_dict.get("results", [])
        if not result_dicts:
            logger.debug(f"ç¬¬{pageno}é¡µæ— æ›´å¤šç»“æœ")
            break
        
        for result in result_dicts:
            # æå–å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
            content = result.get("content", "") or result.get("snippet", "")
            result_url = result.get("url", "")
            
            # å»é‡ï¼šå¦‚æœå¯ç”¨äº†å»é‡ä¸”URLå·²å­˜åœ¨ï¼Œè·³è¿‡
            if deduplicate_by_url and seen_urls is not None:
                if result_url in seen_urls:
                    continue
                seen_urls.add(result_url)
            
            if content:
                doc = SearchDocument(
                    title=result.get("title", ""),
                    url=result_url,
                    snippet=result.get("snippet", ""),
                    content=content,
                    score=result.get("score", 0.0)
                )
                res.append(doc)
                
                if len(res) >= num_results:
                    break
        
        # å¦‚æœæ²¡æœ‰æ›´å¤šç»“æœï¼Œåœæ­¢åˆ†é¡µ
        if len(result_dicts) < 20:  # é€šå¸¸æ¯é¡µ20ä¸ªç»“æœ
            break
        
        pageno += 1
    
    logger.info(f"âœ… SearXNGæœç´¢å®Œæˆ: è·å¾—{len(res)}ä¸ªç»“æœ")
    return res


class FaissRetriever:
    """FAISSå‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, embedding_model, num_candidates: int = 40, sim_threshold: float = 0.45) -> None:
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹ï¼ˆSentenceTransformerå®ä¾‹ï¼‰
            num_candidates: å€™é€‰æ–‡æ¡£æ•°é‡
            sim_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.embedding_model = embedding_model
        self.num_candidates = num_candidates
        self.sim_threshold = sim_threshold
        self.embeddings_dim = embedding_model.get_sentence_embedding_dimension()
        self.reset_state()
        logger.info(f"ğŸ“¦ FAISSæ£€ç´¢å™¨åˆå§‹åŒ–: dim={self.embeddings_dim}, candidates={num_candidates}, threshold={sim_threshold}")
    
    def reset_state(self) -> None:
        """é‡ç½®çŠ¶æ€"""
        self.index = faiss.IndexFlatIP(self.embeddings_dim)  # ä½¿ç”¨å†…ç§¯ï¼ˆcosineç›¸ä¼¼åº¦ï¼‰
        self.documents = []
    
    def encode_doc(self, doc: str | List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æ¡£ä¸ºå‘é‡"""
        return self.embedding_model.encode(doc, normalize_embeddings=True)
    
    def add_documents(self, documents: List[SearchDocument]) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ æ²¡æœ‰æ–‡æ¡£æ·»åŠ åˆ°æ£€ç´¢å™¨")
            return
        
        self.reset_state()
        self.documents = documents
        
        # æå–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
        doc_texts = [doc.content if doc.content else doc.snippet for doc in documents]
        
        # ç¼–ç æ–‡æ¡£
        doc_embeddings = self.encode_doc(doc_texts)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(doc_embeddings)
        logger.debug(f"ğŸ“š æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£åˆ°FAISSç´¢å¼•")
    
    def filter_by_sim(self, distances: np.ndarray, indices: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ
        
        Args:
            distances: ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„
            indices: ç´¢å¼•æ•°ç»„
        
        Returns:
            è¿‡æ»¤åçš„ç´¢å¼•æ•°ç»„
        """
        cutoff_idx = -1
        for idx, sim in enumerate(distances):
            if sim >= self.sim_threshold:
                cutoff_idx = idx
            else:
                break
        
        if cutoff_idx == -1:
            return np.array([])
        
        return indices[:cutoff_idx + 1]
    
    def get_relevant_documents(self, query: str) -> List[SearchDocument]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.documents:
            logger.warning("âš ï¸ æ£€ç´¢å™¨ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode_doc(query)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1),
            min(self.num_candidates, len(self.documents))
        )
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°åˆ°æ–‡æ¡£
        for idx, sim in enumerate(distances[0]):
            doc_idx = indices[0][idx]
            if doc_idx < len(self.documents):
                self.documents[doc_idx].score = float(sim)
        
        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        top_indices = self.filter_by_sim(distances[0], indices[0])
        
        if len(top_indices) == 0:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>{self.sim_threshold}ï¼‰")
            return []
        
        relevant_docs = [self.documents[int(idx)] for idx in top_indices]
        
        logger.info(f"ğŸ¯ æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
        
        # è®°å½•å‰å‡ ä¸ªç»“æœ
        for idx, doc in enumerate(relevant_docs[:5]):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (sim: {doc.score:.3f})")
        
        return relevant_docs
