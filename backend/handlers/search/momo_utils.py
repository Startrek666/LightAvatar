"""
Momo Search Utils - æœç´¢å’Œæ£€ç´¢å·¥å…·
åŸºäº Momo-Search é¡¹ç›®æ”¹é€ 
"""
from dataclasses import dataclass
import urllib.parse
from json import JSONDecodeError
import requests
from typing import List, Optional
import re

import faiss
import numpy as np
from loguru import logger


@dataclass
class SearchDocument:
    """æœç´¢æ–‡æ¡£æ•°æ®ç»“æ„"""
    title: str = ""
    url: str = ""
    snippet: str = ""
    content: str = ""
    score: float = 0.0


def encode_url(url: str) -> str:
    """URLç¼–ç """
    return urllib.parse.quote(url)


def decode_url(url: str) -> str:
    """URLè§£ç """
    return urllib.parse.unquote(url)


def escape_special_chars(text: str) -> str:
    """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""
    # ç”¨äºTelegram Markdown V2æ ¼å¼
    special_chars = r'_\*\[\]\(\)~`>#\+\-=\|\{\}\.\!'
    return re.sub(f'([{special_chars}])', r'\\\1', text)


def escape_special_chars_for_link(text: str) -> str:
    """è½¬ä¹‰é“¾æ¥ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    # åœ¨é“¾æ¥çš„()éƒ¨åˆ†ï¼Œæ‰€æœ‰ ) å’Œ \ å¿…é¡»è½¬ä¹‰
    return re.sub(r'([\\)])', r'\\\1', text)


def process_bold_text(text: str) -> str:
    """å¤„ç†ç²—ä½“æ–‡æœ¬"""
    # å°† **text** æ›¿æ¢ä¸º *escaped_text*
    processed_text = re.sub(r'\*\*(.*?)\*\*', lambda m: f"*{escape_special_chars(m.group(1))}*", text)
    parts = []
    in_bold = False
    for part in processed_text.split('*'):
        if in_bold:
            parts.append(f"*{part}*")
        else:
            parts.append(escape_special_chars(part))
        in_bold = not in_bold
    return ''.join(parts)


def convert_to_markdown(text: str) -> str:
    """
    å°†LLMè¾“å‡ºè½¬æ¢ä¸ºé€‚åˆæ˜¾ç¤ºçš„Markdownæ ¼å¼
    (ç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨Telegramç‰¹æ®Šè½¬ä¹‰)
    """
    lines = text.split('\n')
    result = []
    
    for line in lines:
        line_stripped = line.strip()
        
        # å¤„ç†å¼•ç”¨æ ‡è®° [citation:X] -> [X]
        if '[citation:' in line_stripped:
            line_stripped = re.sub(r'\[citation:(\d+)\]', r'[\1]', line_stripped)
        
        # å¤„ç†æ ‡é¢˜
        if line_stripped.startswith('#'):
            header_text = line_stripped.strip('#').strip()
            result.append(f"**{header_text}**\n")
        # å¤„ç†åˆ—è¡¨é¡¹
        elif line_stripped.startswith('- '):
            bullet_text = line_stripped[2:]
            result.append(f"â€¢ {bullet_text}\n")
        # å¤„ç†åˆ†éš”çº¿
        elif line_stripped == '---':
            result.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        else:
            result.append(f"{line_stripped}\n")
    
    return ''.join(result)


def search_searxng(
    query: str, 
    num_results: int,
    ip_address: str = "http://localhost:8080",
    language: str = "zh",
    time_range: str = "day"
) -> List[SearchDocument]:
    """
    ä½¿ç”¨SearXNGæœç´¢å¼•æ“è¿›è¡Œæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: æœŸæœ›çš„ç»“æœæ•°é‡
        ip_address: SearXNGå®ä¾‹åœ°å€
        language: æœç´¢è¯­è¨€
        time_range: æ—¶é—´èŒƒå›´ (day/week/month/year/'')
    
    Returns:
        æœç´¢æ–‡æ¡£åˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }
    
    request_str = f"/search?q={encode_url(query)}&time_range={time_range}&format=json&language={language}&pageno="
    pageno = 1
    base_url = ip_address
    results = []
    
    logger.info(f"ğŸ” SearXNGæœç´¢: {query} (æœŸæœ›{num_results}ä¸ªç»“æœ)")
    
    while len(results) < num_results:
        url = base_url + request_str + str(pageno)
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response_dict = response.json()
        except JSONDecodeError:
            logger.error("âŒ SearXNGè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSONï¼Œè¯·æ£€æŸ¥SearXNGé…ç½®")
            break
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ SearXNGè¯·æ±‚å¤±è´¥: {e}")
            break
        
        result_dicts = response_dict.get("results", [])
        if not result_dicts:
            logger.warning(f"âš ï¸ ç¬¬{pageno}é¡µæ²¡æœ‰æ›´å¤šç»“æœ")
            break
        
        for result in result_dicts:
            if "content" in result:
                doc = SearchDocument(
                    title=result.get("title", ""),
                    url=result.get("url", ""),
                    snippet=result.get("content", "")
                )
                results.append(doc)
                
                if len(results) >= num_results:
                    break
        
        pageno += 1
    
    logger.info(f"âœ… SearXNGæœç´¢å®Œæˆ: è·å¾—{len(results)}ä¸ªç»“æœ")
    return results


class FaissRetriever:
    """åŸºäºFAISSçš„å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(
        self, 
        embedding_model, 
        num_candidates: int = 40, 
        sim_threshold: float = 0.45
    ):
        """
        åˆå§‹åŒ–FAISSæ£€ç´¢å™¨
        
        Args:
            embedding_model: SentenceTransformeråµŒå…¥æ¨¡å‹
            num_candidates: å€™é€‰æ–‡æ¡£æ•°é‡
            sim_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.embedding_model = embedding_model
        self.num_candidates = num_candidates
        self.sim_threshold = sim_threshold
        self.embeddings_dim = embedding_model.get_sentence_embedding_dimension()
        self.reset_state()
        logger.info(f"ğŸ“¦ FAISSæ£€ç´¢å™¨åˆå§‹åŒ–: dim={self.embeddings_dim}, candidates={num_candidates}, threshold={sim_threshold}")
    
    def reset_state(self):
        """é‡ç½®æ£€ç´¢å™¨çŠ¶æ€"""
        self.index = faiss.IndexFlatIP(self.embeddings_dim)  # å†…ç§¯ç´¢å¼•
        self.documents = []
    
    def encode_doc(self, doc: str | List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æ¡£ä¸ºå‘é‡"""
        return self.embedding_model.encode(doc, normalize_embeddings=True)
    
    def add_documents(self, documents: List[SearchDocument]):
        """æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨"""
        if not documents:
            logger.warning("âš ï¸ æ²¡æœ‰æ–‡æ¡£æ·»åŠ åˆ°æ£€ç´¢å™¨")
            return
        
        self.reset_state()
        self.documents = documents
        
        # ç¼–ç æ–‡æ¡£
        doc_texts = [doc.content if doc.content else doc.snippet for doc in documents]
        doc_embeddings = self.encode_doc(doc_texts)
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(doc_embeddings)
        logger.info(f"âœ… å·²æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£åˆ°FAISSç´¢å¼•")
    
    def filter_by_sim(self, distances: np.ndarray, indices: np.ndarray) -> np.ndarray:
        """æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤"""
        cutoff_idx = -1
        for idx, sim in enumerate(distances):
            if sim > self.sim_threshold:
                cutoff_idx = idx
            else:
                break
        top_sim_indices = indices[:cutoff_idx + 1]
        return top_sim_indices
    
    def get_relevant_documents(self, query: str) -> List[SearchDocument]:
        """è·å–ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£"""
        if not self.documents:
            raise ValueError("âŒ æ£€ç´¢å™¨ä¸­æ²¡æœ‰æ–‡æ¡£")
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode_doc(query)
        
        # FAISSæœç´¢
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1), 
            self.num_candidates
        )
        
        # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯
        for idx, sim in enumerate(distances[0]):
            if indices[0][idx] < len(self.documents):
                self.documents[indices[0][idx]].score = float(sim)
        
        # æ ¹æ®é˜ˆå€¼è¿‡æ»¤
        top_indices = self.filter_by_sim(distances[0], indices[0])
        logger.info(f"ğŸ¯ æ‰¾åˆ°{len(top_indices)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>{self.sim_threshold}ï¼‰")
        
        relevant_docs = [self.documents[idx] for idx in top_indices]
        
        # æ‰“å°ç›¸å…³æ–‡æ¡£ä¿¡æ¯
        for idx, doc in enumerate(relevant_docs):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (ç›¸ä¼¼åº¦: {doc.score:.3f})")
        
        return relevant_docs



