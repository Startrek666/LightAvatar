"""
Momo Search Utils - æœç´¢å·¥å…·å‡½æ•°
"""
from dataclasses import dataclass
import urllib.parse
from json import JSONDecodeError
from typing import List, Optional, Dict
import re
import asyncio

import faiss
import numpy as np
import requests
from loguru import logger

# ç¿»è¯‘APIé…ç½®
TRANSLATE_API_URL = "https://api-utils.lemomate.com/translate"
TRANSLATE_API_KEY = "L5kGzmjwqXbk0ViD@"

# æ™ºè°±æ¸…è¨€å…³é”®è¯æå–APIé…ç½®
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
ZHIPU_API_KEY = "6f29a799833a4a5daf5752973e9d0cc4.uoelH21xYFMkDknh"
ZHIPU_MODEL = "glm-4.5-flash"


@dataclass
class SearchDocument:
    """æœç´¢ç»“æœæ–‡æ¡£"""
    title: str = ""
    url: str = ""
    snippet: str = ""
    content: str = ""
    score: float = 0.0


def encode_url(url: str) -> str:
    """URLç¼–ç """
    return urllib.parse.quote(url)


def decode_url(url: str) -> str:
    """URLè§£ç """
    return urllib.parse.unquote(url)


def escape_markdown(text: str) -> str:
    """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""
    special_chars = r'_\*\[\]\(\)~`>#\+\-=\|\{\}\.\!'
    return re.sub(f'([{special_chars}])', r'\\\1', text)


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
    
    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡
    """
    if not text:
        return "en"
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s+', '', text))  # å»é™¤ç©ºæ ¼åçš„æ€»å­—ç¬¦æ•°
    
    if total_chars == 0:
        return "en"
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    if chinese_ratio > 0.3:
        return "zh"
    else:
        return "en"


def extract_key_entities(text: str) -> List[str]:
    """
    ä½¿ç”¨è§„åˆ™æå–æ–‡æœ¬ä¸­çš„å…³é”®å®ä½“å’Œæ¦‚å¿µï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
    
    æå–è§„åˆ™ï¼š
    - ä¸­æ–‡ï¼šæå–å¸¸è§çš„å®ä½“æ¨¡å¼ï¼ˆåè¯çŸ­è¯­ã€äººåã€åœ°åã€ä½œå“åç­‰ï¼‰
    - è‹±æ–‡ï¼šæå–ä¸“æœ‰åè¯å’Œå¤§å†™è¯æ±‡
    - æŠ€æœ¯æœ¯è¯­ï¼šæå–ç‰¹å®šæ¨¡å¼çš„æœ¯è¯­
    - é€šç”¨ï¼šæå–å¼•å·å†…å®¹ã€å…³é”®è¯
    """
    entities = []
    import re
    
    # 1. è‹±æ–‡ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å…¬å¸åç­‰ï¼‰
    tech_patterns = [
        r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*',  # è‹±æ–‡ä¸“æœ‰åè¯ (å¦‚ Python, Machine Learning)
        r'[a-z]+-[a-z]+',  # è¿å­—ç¬¦æœ¯è¯­ (å¦‚ deep-learning, state-of-the-art)
        r'\d+[GBKM]',  # å¤§å°å•ä½ (å¦‚ 16GB, 500MB)
        r'[a-z]{2,}\d+',  # äº§å“å‹å· (å¦‚ gpt4, llama3, iPhone15)
        r'[A-Z]{2,}',  # å…¨å¤§å†™ç¼©å†™ (å¦‚ API, NLP, AI)
    ]
    
    for pattern in tech_patterns:
        matches = re.findall(pattern, text)
        entities.extend(matches)
    
    # 2. ä¸­æ–‡å®ä½“æå–ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
    # äººåï¼š2-4ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½åŒ…å«Â·å·ï¼ˆå¦‚ æÂ·æ˜ï¼‰
    chinese_name_pattern = r'[\u4e00-\u9fa5]{2,4}(?:Â·[\u4e00-\u9fa5]{1,3})?'
    chinese_names = re.findall(chinese_name_pattern, text)
    entities.extend(chinese_names)
    
    # 3. å¸¸è§çš„ä¸­æ–‡åè¯çŸ­è¯­ï¼ˆ2-6ä¸ªå­—ç¬¦çš„æŠ€æœ¯æœ¯è¯­ã€æ¦‚å¿µã€äº§å“åç­‰ï¼‰
    # åŒ¹é…ï¼šæ•°å­—+å•ä½ã€å½¢å®¹è¯+åè¯ã€åè¯+åè¯ç­‰å¸¸è§ç»„åˆ
    chinese_concept_patterns = [
        r'[\u4e00-\u9fa5]{2,6}',  # 2-6ä¸ªæ±‰å­—çš„åè¯çŸ­è¯­ï¼ˆä¼šåŒ¹é…å¾ˆå¤šï¼Œéœ€è¦åç»­è¿‡æ»¤ï¼‰
    ]
    for pattern in chinese_concept_patterns:
        matches = re.findall(pattern, text)
        # è¿‡æ»¤æ‰å¸¸è§çš„åœç”¨è¯å’Œè™šè¯
        stop_words = {'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 
                     'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'å¦‚æœ', 'é‚£ä¹ˆ', 'æˆ–è€…', 'è€Œä¸”', 'ä»¥åŠ', 'ç­‰ç­‰', 'ä¾‹å¦‚',
                     'è¿˜æœ‰', 'å¦å¤–', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'ç„¶å', 'æ¥ä¸‹æ¥', 'åŒæ—¶', 'å› æ­¤'}
        filtered_matches = [m for m in matches if len(m) >= 2 and m not in stop_words]
        entities.extend(filtered_matches)
    
    # 4. æå–å¼•å·å†…çš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯é‡è¦æ¦‚å¿µã€å¼•ç”¨ï¼‰
    quoted = re.findall(r'["""](.*?)["""]', text)
    entities.extend(quoted)
    
    # 5. æå–ã€ã€‘å†…çš„å†…å®¹ï¼ˆä¸­æ–‡å¸¸ç”¨å¼ºè°ƒæ ¼å¼ï¼‰
    bracketed = re.findall(r'ã€(.*?)ã€‘', text)
    entities.extend(bracketed)
    
    # 6. æå–ã€Šã€‹å†…çš„å†…å®¹ï¼ˆä¹¦åã€ä½œå“åï¼‰
    book_titles = re.findall(r'ã€Š(.*?)ã€‹', text)
    entities.extend(book_titles)
    
    # 7. æå–å¸¸è§çš„å…³é”®è¯æ¨¡å¼ï¼ˆå¦‚æœæ–‡æœ¬ä¸­æœ‰æ˜ç¡®çš„ä¸»é¢˜è¯ï¼‰
    # åŒ¹é…"å…³äºXXX"ã€"XXXçš„"ç­‰æ¨¡å¼
    topic_patterns = [
        r'å…³äº([\u4e00-\u9fa5]{2,10})',
        r'([\u4e00-\u9fa5]{2,10})çš„',
        r'(?:ä»‹ç»|è®²è§£|è¯´æ˜|åˆ†æ)([^ï¼Œã€‚ï¼š;]{2,10})',
    ]
    for pattern in topic_patterns:
        matches = re.findall(pattern, text)
        entities.extend(matches)
    
    # å»é‡å¹¶è¿‡æ»¤
    entities = list(set([e.strip() for e in entities if 2 <= len(e) <= 30]))
    
    # æŒ‰é•¿åº¦å’Œé‡è¦æ€§æ’åºï¼ˆçŸ­çš„å¯èƒ½æ›´é‡è¦ï¼Œå¦‚"AI"ã€"Python"ï¼‰
    entities.sort(key=lambda x: (len(x), x))
    
    return entities[:25]  # è¿”å›æœ€å¤š25ä¸ªå®ä½“ï¼ˆå¢åŠ æ•°é‡ä»¥è¦†ç›–æ›´å¤šé¢†åŸŸï¼‰


def compress_conversation_history_rule_based(
    conversation_history: List[Dict],
    current_query: str,
    max_messages: int = 4,
    max_compressed_length: int = 800,
    min_total_length: int = 1600
) -> Optional[str]:
    """
    åŸºäºè§„åˆ™çš„ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼Œé€‚é…ä¸åŒé—®é¢˜é¢†åŸŸï¼‰
    
    ç­–ç•¥ï¼š
    1. æå–æœ€è¿‘å‡ è½®å¯¹è¯çš„å…³é”®å®ä½“å’Œæ¦‚å¿µï¼ˆæ”¯æŒæŠ€æœ¯ã€æ–‡å­¦ã€è‰ºæœ¯ã€å†å²ç­‰ï¼‰
    2. æå–ç”¨æˆ·æåˆ°çš„ä¸»è¦ä¸»é¢˜ï¼ˆä¿ç•™å®Œæ•´çš„é—®é¢˜è¡¨è¿°ï¼‰
    3. æå–AIå›ç­”ä¸­çš„æ ¸å¿ƒè¦ç‚¹ï¼ˆä¸ä»…ä»…æ˜¯ç¬¬ä¸€å¥ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯å¥ï¼‰
    4. ä¿ç•™å¯¹è¯é€»è¾‘å’Œå…³è”æ€§ï¼ˆé€šè¿‡å®ä½“å…³è”ï¼‰
    5. è¿‡æ»¤æ‰æ— å…³çš„ç»†èŠ‚ï¼Œä½†ä¿ç•™é¢†åŸŸç›¸å…³çš„å…³é”®ä¿¡æ¯
    
    é€‚ç”¨é¢†åŸŸï¼š
    - âœ… æŠ€æœ¯/äº§å“ï¼šå®ä½“æå–æ•ˆæœå¥½
    - âœ… å­¦æœ¯ç ”ç©¶ï¼šä¿ç•™å…³é”®æ¦‚å¿µå’Œæœ¯è¯­
    - âœ… å†å²/æ–‡åŒ–ï¼šæå–äººåã€åœ°åã€ä½œå“å
    - âš ï¸ æ–‡å­¦/è‰ºæœ¯ï¼šä¿ç•™å¼•å·å’Œä½œå“åï¼Œä½†å¯èƒ½ä¸¢å¤±æƒ…æ„Ÿç»†èŠ‚
    - âš ï¸ æŠ½è±¡è®¨è®ºï¼šä¾èµ–å®ä½“æå–ï¼Œå¯èƒ½ä¸å¤Ÿæ·±å…¥
    """
    if not conversation_history or len(conversation_history) <= max_messages:
        return None
    
    try:
        total_length = sum(len(msg.get("content", "")) for msg in conversation_history)
        if total_length <= min_total_length:
            return None
        
        # æå–æœ€è¿‘çš„å¯¹è¯ï¼ˆç”¨äºåˆ†æï¼‰
        recent_history = conversation_history[-max_messages*2:]
        
        # æ”¶é›†å…³é”®ä¿¡æ¯
        user_queries = []
        ai_summaries = []
        key_entities = set()
        
        for msg in recent_history:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            
            if not content:
                continue
            
            if role == "user" and content != current_query:
                user_queries.append(content)
                # æå–å®ä½“
                entities = extract_key_entities(content)
                key_entities.update(entities)
            
            elif role == "assistant":
                # æå–AIå›ç­”çš„å…³é”®å¥å­ï¼ˆä¸ä»…é™å‰3å¥ï¼‰
                # åˆ†å¥å¤„ç†ï¼ˆè€ƒè™‘ä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]|\.\s+|! |\? ', content)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                # æå–å…³é”®å¥å­ç­–ç•¥ï¼š
                # 1. ç¬¬ä¸€å¥ï¼ˆé€šå¸¸æ˜¯æ€»ç»“æˆ–å¼€å¤´ï¼‰
                # 2. åŒ…å«æœ€å¤šå®ä½“çš„å¥å­ï¼ˆæ ¸å¿ƒå†…å®¹ï¼‰
                # 3. åŒ…å«å¼•å·æˆ–ç‰¹æ®Šæ ‡è®°çš„å¥å­ï¼ˆé‡è¦å¼•ç”¨æˆ–å¼ºè°ƒï¼‰
                
                key_sentences = []
                
                # æ·»åŠ ç¬¬ä¸€å¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if sentences:
                    first_sentence = sentences[0]
                    if len(first_sentence) > 0:
                        key_sentences.append(first_sentence)
                
                # æ‰¾å‡ºåŒ…å«æœ€å¤šå®ä½“çš„å¥å­ï¼ˆæ›´å¯èƒ½æ˜¯æ ¸å¿ƒå†…å®¹ï¼‰
                if len(sentences) > 1:
                    sentence_entity_counts = []
                    for i, sent in enumerate(sentences[:10]):  # åªæ£€æŸ¥å‰10å¥ï¼ˆé¿å…å¤ªé•¿ï¼‰
                        entities_in_sent = extract_key_entities(sent)
                        # ç»™åŒ…å«å¼•å·ã€ä¹¦åå·ç­‰çš„å¥å­åŠ åˆ†
                        score = len(entities_in_sent)
                        if '""' in sent or '"' in sent or 'ã€Š' in sent or 'ã€' in sent:
                            score += 3
                        if i > 0:  # ç¬¬ä¸€å¥å·²ç»æ·»åŠ ï¼Œå…¶ä»–å¥å­åŠ åˆ†å°‘ä¸€ç‚¹
                            score += 1
                        sentence_entity_counts.append((score, i, sent))
                    
                    # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰2ä¸ªï¼ˆæ’é™¤å·²ç»æ·»åŠ çš„ç¬¬ä¸€å¥ï¼‰
                    sentence_entity_counts.sort(reverse=True)
                    for score, idx, sent in sentence_entity_counts[:2]:
                        if idx != 0:  # é¿å…é‡å¤æ·»åŠ ç¬¬ä¸€å¥
                            key_sentences.append(sent)
                
                # æ„å»ºæ‘˜è¦ï¼ˆæœ€å¤š3ä¸ªå…³é”®å¥å­ï¼‰
                for sent in key_sentences[:3]:
                    if len(sent) > 200:
                        sent = sent[:200] + "..."
                    ai_summaries.append(sent)
                
                # æå–å…³é”®å®ä½“ï¼ˆä»æ•´ä¸ªå†…å®¹ä¸­æå–ï¼Œä¸ä»…ä»…æ˜¯å‰500å­—ç¬¦ï¼‰
                # ä½†ä¸ºäº†é¿å…è¿‡é•¿ï¼Œåˆ†æ®µè½æå–
                content_sample = content[:1000] if len(content) > 1000 else content  # å–å‰1000å­—ç¬¦
                entities = extract_key_entities(content_sample)
                key_entities.update(entities)
        
        # æ„å»ºå‹ç¼©åçš„ä¸Šä¸‹æ–‡
        compressed_parts = []
        
        # 1. ç”¨æˆ·ä¹‹å‰çš„é—®é¢˜
        if user_queries:
            # å¦‚æœç”¨æˆ·é—®é¢˜å¾ˆé•¿ï¼Œåªä¿ç•™æ ¸å¿ƒéƒ¨åˆ†
            for q in user_queries[-2:]:  # æœ€å¤šä¿ç•™æœ€è¿‘2ä¸ªé—®é¢˜
                if len(q) > 100:
                    # å°è¯•æå–é—®é¢˜å…³é”®è¯éƒ¨åˆ†
                    q_short = q[:100] + "..."
                else:
                    q_short = q
                compressed_parts.append(f"ç”¨æˆ·æåˆ°: {q_short}")
        
        # 2. æ ¸å¿ƒå®ä½“å’Œæ¦‚å¿µï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
        if key_entities:
            # æ›´å…¨é¢çš„åœç”¨è¯è¿‡æ»¤
            stop_words = {
                'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ',
                'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'å¦‚æœ', 'é‚£ä¹ˆ', 'æˆ–è€…', 'è€Œä¸”', 'ä»¥åŠ', 'ç­‰ç­‰', 'ä¾‹å¦‚',
                'è¿˜æœ‰', 'å¦å¤–', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'ç„¶å', 'æ¥ä¸‹æ¥', 'åŒæ—¶', 'å› æ­¤',
                'ä¸€ä¸‹', 'ä¸€ç‚¹', 'ä¸€äº›', 'ä¸€ç§', 'ä¸€ä¸ª', 'ä¸€èˆ¬', 'ä¸€ç›´', 'ä¸€å®š', 'ä¸€æ ·',
                'å¾ˆå¥½', 'éå¸¸', 'æ¯”è¾ƒ', 'ç›¸å½“', 'å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'åº”è¯¥',
            }
            
            # è¿‡æ»¤å¹¶å»é‡
            filtered_entities = [
                e for e in key_entities 
                if len(e) >= 2 and e not in stop_words 
                and not e.isdigit()  # æ’é™¤çº¯æ•°å­—
            ]
            
            # æŒ‰é•¿åº¦å’Œç±»å‹æ’åºï¼ˆçŸ­è¯å¯èƒ½åœ¨å‰é¢ï¼Œä½†ä¹Ÿè¦è€ƒè™‘å¤šæ ·æ€§ï¼‰
            # ä¼˜å…ˆä¿ç•™ï¼šæœ‰ç‰¹æ®Šæ ¼å¼çš„ï¼ˆå¼•å·ã€ä¹¦åå·ï¼‰ã€è‹±æ–‡ä¸“æœ‰åè¯ã€æŠ€æœ¯æœ¯è¯­
            def entity_score(entity):
                score = 0
                # åŒ…å«å¤§å†™å­—æ¯çš„åŠ åˆ†ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼‰
                if re.search(r'[A-Z]', entity):
                    score += 10
                # åŒ…å«æ•°å­—çš„åŠ åˆ†ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å·ã€å‹å·ï¼‰
                if re.search(r'\d', entity):
                    score += 5
                # åŒ…å«è¿å­—ç¬¦çš„åŠ åˆ†ï¼ˆå¯èƒ½æ˜¯å¤åˆæœ¯è¯­ï¼‰
                if '-' in entity:
                    score += 5
                # é•¿åº¦é€‚ä¸­çš„åŠ åˆ†ï¼ˆ2-8å­—ç¬¦æœ€ä½³ï¼‰
                if 2 <= len(entity) <= 8:
                    score += 3
                return score
            
            filtered_entities.sort(key=lambda x: (entity_score(x), -len(x)), reverse=True)
            
            if filtered_entities:
                entities_str = "ã€".join(filtered_entities[:20])  # å¢åŠ åˆ°20ä¸ªå®ä½“ä»¥æé«˜è¦†ç›–ç‡
                compressed_parts.append(f"æ¶‰åŠçš„å…³é”®æ¦‚å¿µ: {entities_str}")
        
        # 3. AIä¹‹å‰å›ç­”çš„è¦ç‚¹ï¼ˆä¿ç•™æ›´å¤šå…³é”®å¥å­ï¼‰
        if ai_summaries:
            # å»é‡ï¼ˆé¿å…é‡å¤çš„å¥å­ï¼‰
            unique_summaries = []
            seen = set()
            for summary in ai_summaries:
                summary_normalized = summary[:50].strip()  # ç”¨å‰50å­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
                if summary_normalized not in seen:
                    unique_summaries.append(summary)
                    seen.add(summary_normalized)
            
            for summary in unique_summaries[:3]:  # å¢åŠ åˆ°æœ€å¤š3ä¸ªæ‘˜è¦ï¼Œæé«˜ä¿¡æ¯ä¿ç•™ç‡
                compressed_parts.append(f"AIä¹‹å‰å›ç­”è¦ç‚¹: {summary}")
        
        if compressed_parts:
            compressed = "\n".join(compressed_parts)
            # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
            if len(compressed) > max_compressed_length:
                compressed = compressed[:max_compressed_length] + "..."
            
            logger.info(f"ğŸ“¦ [è§„åˆ™å‹ç¼©] å¯¹è¯å†å²å·²å‹ç¼©: {total_length} å­—ç¬¦ â†’ {len(compressed)} å­—ç¬¦")
            return compressed
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ è§„åˆ™å‹ç¼©å¤±è´¥: {e}", exc_info=True)
        return None


def compress_conversation_history_smart_truncate(
    conversation_history: List[Dict],
    current_query: str,
    max_messages: int = 4,
    max_compressed_length: int = 800,
    min_total_length: int = 1600
) -> Optional[str]:
    """
    æ™ºèƒ½æˆªæ–­å‹ç¼©ï¼šä¿ç•™æœ€é‡è¦çš„å¼€å¤´å’Œç»“å°¾éƒ¨åˆ†
    
    ç­–ç•¥ï¼š
    - ä¿ç•™ç”¨æˆ·ç¬¬ä¸€æ¬¡æåˆ°çš„æ ¸å¿ƒé—®é¢˜ï¼ˆå¼€å¤´ï¼‰
    - ä¿ç•™æœ€è¿‘å‡ è½®å¯¹è¯çš„å…³é”®ä¿¡æ¯ï¼ˆç»“å°¾ï¼‰
    - ä¸­é—´éƒ¨åˆ†ç”¨æ‘˜è¦ä»£æ›¿
    """
    if not conversation_history or len(conversation_history) <= max_messages:
        return None
    
    try:
        total_length = sum(len(msg.get("content", "")) for msg in conversation_history)
        if total_length <= min_total_length:
            return None
        
        # æå–å¼€å¤´å’Œç»“å°¾
        start_messages = conversation_history[:max_messages]
        end_messages = conversation_history[-max_messages:]
        
        compressed_parts = []
        
        # å¼€å¤´ï¼šç”¨æˆ·æœ€åˆçš„é—®é¢˜
        for msg in start_messages:
            if msg.get("role") == "user":
                content = msg.get("content", "")
                if content and content != current_query:
                    compressed_parts.append(f"æœ€åˆé—®é¢˜: {content[:150]}")
                    break
        
        # ç»“å°¾ï¼šæœ€è¿‘çš„å¯¹è¯
        recent_user_query = None
        for msg in reversed(end_messages):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            
            if role == "user" and content and content != current_query:
                recent_user_query = content[:100] if len(content) > 100 else content
                break
        
        if recent_user_query:
            compressed_parts.append(f"æœ€è¿‘æåˆ°: {recent_user_query}")
        
        # å¦‚æœæœ‰ä¸­é—´éƒ¨åˆ†è¢«çœç•¥ï¼Œæ·»åŠ è¯´æ˜
        if len(conversation_history) > max_messages * 2:
            compressed_parts.append(f"ï¼ˆçœç•¥ä¸­é—´ {len(conversation_history) - max_messages * 2} è½®å¯¹è¯ï¼‰")
        
        if compressed_parts:
            compressed = "\n".join(compressed_parts)
            if len(compressed) > max_compressed_length:
                compressed = compressed[:max_compressed_length] + "..."
            
            logger.info(f"ğŸ“¦ [æ™ºèƒ½æˆªæ–­] å¯¹è¯å†å²å·²å‹ç¼©: {total_length} å­—ç¬¦ â†’ {len(compressed)} å­—ç¬¦")
            return compressed
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ æ™ºèƒ½æˆªæ–­å¤±è´¥: {e}", exc_info=True)
        return None


def compress_conversation_history(
    conversation_history: List[Dict],
    current_query: str,
    max_messages: int = 4,
    max_compressed_length: int = 800,
    min_total_length: int = 1600,
    compression_method: str = "rule_based",  # "rule_based", "smart_truncate", "llm"
    api_key: Optional[str] = None,
    model: Optional[str] = None
) -> Optional[str]:
    """
    å‹ç¼©å¯¹è¯å†å²ï¼Œæå–ä¸å½“å‰æŸ¥è¯¢æœ€ç›¸å…³çš„å…³é”®ä¿¡æ¯
    
    æ”¯æŒå¤šç§å‹ç¼©æ–¹æ³•ï¼š
    - "rule_based": åŸºäºè§„åˆ™çš„å‹ç¼©ï¼ˆå¿«é€Ÿï¼Œæ— éœ€APIè°ƒç”¨ï¼‰
    - "smart_truncate": æ™ºèƒ½æˆªæ–­ï¼ˆå¿«é€Ÿï¼Œä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼‰
    - "llm": ä½¿ç”¨LLMå‹ç¼©ï¼ˆæœ€å‡†ç¡®ï¼Œä½†éœ€è¦APIè°ƒç”¨ï¼‰
    
    Args:
        conversation_history: å®Œæ•´å¯¹è¯å†å²
        current_query: å½“å‰ç”¨æˆ·æŸ¥è¯¢
        max_messages: æœ€å¤šä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆå¦‚æœå†å²è¾ƒçŸ­ï¼Œä¸å‹ç¼©ï¼‰
        max_compressed_length: å‹ç¼©åæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
        min_total_length: å†å²å¯¹è¯æ€»å­—ç¬¦æ•°é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼æ‰å¼€å§‹å‹ç¼©
        compression_method: å‹ç¼©æ–¹æ³• ("rule_based", "smart_truncate", "llm")
        api_key: æ™ºè°±æ¸…è¨€APIå¯†é’¥ï¼ˆä»…llmæ–¹æ³•éœ€è¦ï¼‰
        model: æ™ºè°±æ¸…è¨€æ¨¡å‹åç§°ï¼ˆä»…llmæ–¹æ³•éœ€è¦ï¼‰
    
    Returns:
        å‹ç¼©åçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¦‚æœæ²¡æœ‰å†å²æˆ–ä¸éœ€è¦å‹ç¼©åˆ™è¿”å›None
    """
    if not conversation_history or len(conversation_history) <= max_messages:
        # å¦‚æœå†å²å¾ˆçŸ­ï¼Œä¸éœ€è¦å‹ç¼©
        return None
    
    # æ ¹æ®å‹ç¼©æ–¹æ³•é€‰æ‹©ç­–ç•¥
    if compression_method == "rule_based":
        return compress_conversation_history_rule_based(
            conversation_history=conversation_history,
            current_query=current_query,
            max_messages=max_messages,
            max_compressed_length=max_compressed_length,
            min_total_length=min_total_length
        )
    
    elif compression_method == "smart_truncate":
        return compress_conversation_history_smart_truncate(
            conversation_history=conversation_history,
            current_query=current_query,
            max_messages=max_messages,
            max_compressed_length=max_compressed_length,
            min_total_length=min_total_length
        )
    
    elif compression_method == "llm":
        # LLMå‹ç¼©ï¼ˆåŸæœ‰çš„å®ç°ï¼‰
        try:
            # è®¡ç®—åŸå§‹æ–‡æœ¬é•¿åº¦
            total_length = sum(len(msg.get("content", "")) for msg in conversation_history)
            
            # å¦‚æœæ€»é•¿åº¦å·²ç»å¾ˆå°ï¼Œä¸éœ€è¦å‹ç¼©
            if total_length <= min_total_length:
                return None
            
            # æå–å¯¹è¯å†å²çš„å…³é”®ä¿¡æ¯
            history_text_parts = []
            for i, msg in enumerate(conversation_history[-max_messages*2:]):  # å–æœ€è¿‘çš„æ¶ˆæ¯ç”¨äºæ‘˜è¦
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                if role == "user":
                    history_text_parts.append(f"ç”¨æˆ·: {content}")
                elif role == "assistant":
                    # å¯¹AIå›ç­”è¿›è¡Œæˆªæ–­ï¼Œåªä¿ç•™å‰500å­—ç¬¦ç”¨äºæ‘˜è¦
                    content_preview = content[:500] + "..." if len(content) > 500 else content
                    history_text_parts.append(f"AI: {content_preview}")
            
            history_text = "\n".join(history_text_parts)
            
            # ä½¿ç”¨LLMå‹ç¼©å†å²å¯¹è¯
            prompt = f"""è¯·å¯¹ä»¥ä¸‹å¯¹è¯å†å²è¿›è¡Œå‹ç¼©æ‘˜è¦ï¼Œæå–ä¸å½“å‰æŸ¥è¯¢æœ€ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚

å½“å‰æŸ¥è¯¢ï¼š{current_query}

å¯¹è¯å†å²ï¼š
{history_text}

**å‹ç¼©è¦æ±‚**ï¼š
1. åªä¿ç•™ä¸å½“å‰æŸ¥è¯¢ç›¸å…³çš„æ ¸å¿ƒæ¦‚å¿µã€å®ä½“ã€ä¸»é¢˜
2. å¦‚æœå½“å‰æŸ¥è¯¢æ˜¯ä¸å®Œæ•´çš„ï¼ˆå¦‚"ç”¨è¡¨æ ¼å¯¹æ¯”"ã€"è¯¦ç»†è¯´æ˜"ï¼‰ï¼Œå¿…é¡»ä¿ç•™å†å²ä¸­æåˆ°çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚"å¼€æºå¤§æ¨¡å‹"ï¼‰
3. åˆ é™¤æ— å…³çš„ç»†èŠ‚ã€å†—ä½™ä¿¡æ¯ã€é‡å¤å†…å®¹ã€å†—é•¿çš„æ€è€ƒè¿‡ç¨‹
4. é‡ç‚¹æå–ï¼šå®ä½“åç§°ã€å…³é”®æ¦‚å¿µã€è®¨è®ºä¸»é¢˜ã€ç”¨æˆ·æ„å›¾
5. å¿½ç•¥ï¼šè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ã€é•¿ç¯‡è§£é‡Šã€é‡å¤çš„æ€»ç»“
6. ä¿æŒå…³é”®ä¿¡æ¯å®Œæ•´ï¼Œä½†è¦å°½é‡ç®€æ´
7. è¾“å‡ºæ ¼å¼ï¼šç”¨ç®€æ´çš„æ–‡æœ¬æè¿°å†å²å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œæ§åˆ¶åœ¨{max_compressed_length}å­—ä»¥å†…

**é‡è¦**ï¼šå¦‚æœå½“å‰æŸ¥è¯¢çœ‹èµ·æ¥æ˜¯å¯¹ä¹‹å‰è®¨è®ºçš„å»¶ç»­æˆ–è¡¥å……ï¼Œå¿…é¡»ä¿ç•™å†å²ä¸­æåˆ°çš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®æ¦‚å¿µã€‚

å‹ç¼©æ‘˜è¦ï¼š"""
            
            try:
                compressed = call_zhipu_llm(
                    prompt=prompt,
                    api_key=api_key,
                    model=model,
                    temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯æ‘˜è¦å‡†ç¡®æ€§
                    max_tokens=600   # æ§åˆ¶è¾“å‡ºé•¿åº¦
                )
            except Exception as e:
                logger.warning(f"âš ï¸ è°ƒç”¨LLMå‹ç¼©å¤±è´¥: {e}ï¼Œè·³è¿‡å‹ç¼©")
                compressed = None
            
            if compressed and len(compressed.strip()) > 0:
                # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
                if len(compressed) > max_compressed_length:
                    compressed = compressed[:max_compressed_length] + "..."
                logger.info(f"ğŸ“¦ [LLMå‹ç¼©] å¯¹è¯å†å²å·²å‹ç¼©: {total_length} å­—ç¬¦ â†’ {len(compressed)} å­—ç¬¦")
                return compressed
            else:
                logger.warning("âš ï¸ LLMå¯¹è¯å†å²å‹ç¼©å¤±è´¥ï¼Œè¿”å›None")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LLMå‹ç¼©å¤±è´¥: {e}", exc_info=True)
            return None
    
    else:
        logger.warning(f"âš ï¸ æœªçŸ¥çš„å‹ç¼©æ–¹æ³•: {compression_method}ï¼Œä½¿ç”¨è§„åˆ™å‹ç¼©")
        return compress_conversation_history_rule_based(
            conversation_history=conversation_history,
            current_query=current_query,
            max_messages=max_messages,
            max_compressed_length=max_compressed_length,
            min_total_length=min_total_length
        )


def extract_keywords(
    query: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None,
    understanding: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> Optional[dict]:
    """
    ä½¿ç”¨æ™ºè°±æ¸…è¨€æ¨¡å‹æå–æœç´¢å…³é”®è¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        api_key: æ™ºè°±æ¸…è¨€APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        model: æ™ºè°±æ¸…è¨€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        understanding: é—®é¢˜ç†è§£ç»“æœï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›ï¼Œå°†åŸºäºç†è§£ç»“æœç”Ÿæˆæ›´å…¨é¢çš„å…³é”®è¯
    
    Returns:
        åŒ…å«zh_keyså’Œen_keysçš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        from datetime import datetime
        
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
        zhipu_api_key = api_key if api_key is not None else ZHIPU_API_KEY
        zhipu_model = model if model is not None else ZHIPU_MODEL
        
        # è·å–å½“å‰æ—¥æœŸï¼Œå»æ‰æœˆä»½å’Œæ—¥æœŸçš„å‰å¯¼é›¶
        now = datetime.now()
        current_date = f"{now.year}å¹´{now.month}æœˆ{now.day}æ—¥"
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä½¿ç”¨å‹ç¼©æŠ€æœ¯ï¼‰
        context_info = ""
        if conversation_history:
            # å°è¯•å‹ç¼©å¯¹è¯å†å²ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼Œå› ä¸ºextract_keywordså¯èƒ½è¢«ç‹¬ç«‹è°ƒç”¨ï¼‰
            # ä¼˜å…ˆä½¿ç”¨è§„åˆ™å‹ç¼©ï¼ˆå¿«é€Ÿï¼Œæ— éœ€APIè°ƒç”¨ï¼‰
            compressed_context = compress_conversation_history(
                conversation_history=conversation_history,
                current_query=query,
                max_messages=4,  # å¦‚æœå†å²â‰¤4æ¡æ¶ˆæ¯ï¼Œä¸å‹ç¼©ï¼ˆé»˜è®¤å€¼ï¼Œå¯åœ¨è°ƒç”¨æ—¶è¦†ç›–ï¼‰
                max_compressed_length=500,  # å‹ç¼©åæœ€å¤š500å­—ç¬¦
                min_total_length=1000,  # é»˜è®¤å­—ç¬¦é˜ˆå€¼ï¼ˆå¯åœ¨è°ƒç”¨æ—¶è¦†ç›–ï¼‰
                compression_method="rule_based",  # ä½¿ç”¨è§„åˆ™å‹ç¼©ï¼ˆå¿«é€Ÿï¼Œæ— éœ€APIï¼‰
                api_key=api_key,
                model=model
            )
            
            # å¦‚æœè§„åˆ™å‹ç¼©å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½æˆªæ–­
            if not compressed_context:
                compressed_context = compress_conversation_history(
                    conversation_history=conversation_history,
                    current_query=query,
                    max_messages=4,
                    max_compressed_length=500,
                    min_total_length=1000,
                    compression_method="smart_truncate"
                )
            
            if compressed_context:
                # ä½¿ç”¨å‹ç¼©åçš„ä¸Šä¸‹æ–‡
                context_info = f"""

**å¯¹è¯ä¸Šä¸‹æ–‡æ‘˜è¦**ï¼ˆé‡è¦ï¼è¯·ç»“åˆä¸Šä¸‹æ–‡æå–å…³é”®è¯ï¼‰ï¼š
{compressed_context}

**æ³¨æ„**ï¼šå¦‚æœå½“å‰é—®é¢˜æ˜¯ç®€çŸ­çš„ä¸å®Œæ•´è¡¨è¿°ï¼ˆå¦‚"ç”¨è¡¨æ ¼å¯¹æ¯”"ã€"è¯¦ç»†è¯´æ˜"ã€"è¿˜æœ‰å“ªäº›"ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ä¸­çš„æ ¸å¿ƒæ¦‚å¿µæå–å…³é”®è¯ã€‚
ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"ç”¨è¡¨æ ¼å¯¹æ¯”"ï¼Œä½†ä¸Šä¸‹æ–‡ä¸­æåˆ°è¿‡"å¼€æºå¤§æ¨¡å‹"ï¼Œåº”è¯¥æå–"å¼€æºå¤§æ¨¡å‹ è¡¨æ ¼ å¯¹æ¯” æ¯”è¾ƒ"ç­‰å…³é”®è¯ã€‚
"""
            else:
                # å¦‚æœå‹ç¼©å¤±è´¥æˆ–ä¸éœ€è¦å‹ç¼©ï¼Œä½¿ç”¨ç®€å•çš„æˆªæ–­æ–¹å¼
                recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
                context_parts = []
                for msg in recent_history:
                    role = msg.get("role", "unknown")
                    content = msg.get("content", "")
                    if role == "user" and content and content != query:
                        context_parts.append(f"- ç”¨æˆ·ä¹‹å‰æåˆ°: {content}")
                    elif role == "assistant" and content:
                        content_preview = content[:100] + "..." if len(content) > 100 else content
                        context_parts.append(f"- AIä¹‹å‰å›ç­”: {content_preview}")
                
                if context_parts:
                    context_info = f"""

**å¯¹è¯ä¸Šä¸‹æ–‡**ï¼ˆé‡è¦ï¼è¯·ç»“åˆä¸Šä¸‹æ–‡æå–å…³é”®è¯ï¼‰ï¼š
{chr(10).join(context_parts)}

**æ³¨æ„**ï¼šå¦‚æœå½“å‰é—®é¢˜æ˜¯ç®€çŸ­çš„ä¸å®Œæ•´è¡¨è¿°ï¼ˆå¦‚"ç”¨è¡¨æ ¼å¯¹æ¯”"ã€"è¯¦ç»†è¯´æ˜"ã€"è¿˜æœ‰å“ªäº›"ç­‰ï¼‰ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ä¸­çš„æ ¸å¿ƒæ¦‚å¿µæå–å…³é”®è¯ã€‚
ä¾‹å¦‚ï¼šç”¨æˆ·è¯´"ç”¨è¡¨æ ¼å¯¹æ¯”"ï¼Œä½†ä¸Šä¸‹æ–‡ä¸­æåˆ°è¿‡"å¼€æºå¤§æ¨¡å‹"ï¼Œåº”è¯¥æå–"å¼€æºå¤§æ¨¡å‹ è¡¨æ ¼ å¯¹æ¯” æ¯”è¾ƒ"ç­‰å…³é”®è¯ã€‚
"""
        
        # æ„å»ºPromptï¼ˆæ ¹æ®æ˜¯å¦æœ‰ç†è§£ç»“æœé€‰æ‹©ä¸åŒçš„promptï¼‰
        if understanding:
            # æ·±åº¦æ¨¡å¼ï¼šåŸºäºé—®é¢˜ç†è§£ç”Ÿæˆæ›´å…¨é¢çš„å…³é”®è¯
            prompt = f"""ä»Šå¤©æ˜¯{current_date}ã€‚ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢å…³é”®è¯æå–ä¸“å®¶ã€‚åŸºäºå¯¹ç”¨æˆ·é—®é¢˜çš„æ·±åº¦ç†è§£ï¼Œæå–å…¨é¢çš„æœç´¢å…³é”®è¯ã€‚

ç”¨æˆ·å½“å‰é—®é¢˜ï¼š{query}{context_info}

é—®é¢˜ç†è§£ï¼š
{understanding}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. åŸºäºé—®é¢˜ç†è§£ï¼Œæå–æ¶µç›–ç”¨æˆ·æ‰€æœ‰æ½œåœ¨éœ€æ±‚çš„æœç´¢å…³é”®è¯
2. å…³é”®è¯åº”è¯¥è¦†ç›–é—®é¢˜çš„å¤šä¸ªç»´åº¦ï¼š
   - **æ ¸å¿ƒæ¦‚å¿µ**ï¼šé—®é¢˜çš„ä¸»ä½“ã€ç›¸å…³æœ¯è¯­ã€åŒä¹‰è¯
   - **å…³é”®å±æ€§**ï¼šç‰¹å¾ã€å‚æ•°ã€è§„æ ¼ã€å“ç‰Œã€ç‰ˆæœ¬ç­‰
   - **è¯„ä¼°ç»´åº¦**ï¼šè¯„æµ‹ã€å¯¹æ¯”ã€ä¼˜ç¼ºç‚¹ã€æ’åã€è¯„ä»·ç­‰
   - **å®ç”¨ä¿¡æ¯**ï¼šåº”ç”¨ã€ä½¿ç”¨ã€è´­ä¹°ã€è·å–ã€æ•™ç¨‹ç­‰
   - **æ—¶é—´é™å®š**ï¼šå¦‚æœæ¶‰åŠ"æœ€æ–°"ã€"æœ€è¿‘"ï¼Œå¿…é¡»åŠ ä¸Š{now.year}å¹´æˆ–{now.month}æœˆ

3. **è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰**ï¼š
   - zh_keys: ä¸­æ–‡å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œ15-20ä¸ª
   - en_keys: è‹±æ–‡å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œ15-20ä¸ª
   - è‹±æ–‡å…³é”®è¯è¦åŒ…å«å®Œæ•´æœ¯è¯­å’Œå¸¸ç”¨ç¼©å†™

**ç¤ºä¾‹**ï¼ˆä»…ä¾›å‚è€ƒï¼Œéœ€æ ¹æ®å®é™…é—®é¢˜çµæ´»è°ƒæ•´ï¼‰ï¼š
1. æŠ€æœ¯é—®é¢˜ï¼š"Pythonå’ŒJavaå“ªä¸ªå¥½ï¼Ÿ"
{{
  "zh_keys": "Python Java ç¼–ç¨‹è¯­è¨€å¯¹æ¯” ä¼˜ç¼ºç‚¹ å­¦ä¹ éš¾åº¦ åº”ç”¨é¢†åŸŸ æ€§èƒ½å¯¹æ¯” å°±ä¸šå‰æ™¯ å¼€å‘æ•ˆç‡ ç”Ÿæ€ç³»ç»Ÿ é€‚ç”¨åœºæ™¯ {now.year}å¹´",
  "en_keys": "Python Java programming language comparison pros cons learning curve application domain performance job market development efficiency ecosystem use case {now.year}"
}}

2. æ¶ˆè´¹é—®é¢˜ï¼š"iPhone 15å€¼å¾—ä¹°å—ï¼Ÿ"
{{
  "zh_keys": "iPhone 15 è‹¹æœæ‰‹æœº å€¼å¾—è´­ä¹° æ€§èƒ½è¯„æµ‹ ä»·æ ¼ ä¼˜ç¼ºç‚¹ ç”¨æˆ·è¯„ä»· å¯¹æ¯”iPhone14 å‚æ•°é…ç½® è´­ä¹°å»ºè®® {now.year}å¹´",
  "en_keys": "iPhone 15 Apple smartphone worth buying performance review price pros cons user review comparison iPhone14 specs buying guide {now.year}"
}}

ç°åœ¨ï¼Œè¯·ä¸ºç”¨æˆ·é—®é¢˜æå–å…³é”®è¯ï¼š"""
        else:
            # å¿«é€Ÿæ¨¡å¼ï¼šåŸæœ‰çš„ç®€å•å…³é”®è¯æå–ï¼ˆå¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œä¹ŸåŠ ä¸Šï¼‰
            prompt_base = f"""ä»Šå¤©æ˜¯{current_date}ã€‚ä¸ºäº†ç»™ç”¨æˆ·çš„å›ç­”ä¿æŒå‡†ç¡®ï¼Œä½ éœ€è¦ä½¿ç”¨æœç´¢å¼•æ“ã€‚ä½¿ç”¨jsonæ ¼å¼è¿”å›å…³é”®è¯ï¼Œå±æ€§ä¸ºzh_keys,en_keysã€‚æ¯ä¸ªå±æ€§åªéœ€è¦ä¸€è¡Œï¼Œå…³é”®è¯ç”¨ç©ºæ ¼åˆ†éš”ã€‚ä»…éœ€è¿”å›é‡è¦å…³é”®è¯ï¼Œæ¯è¡Œä¸è¶…è¿‡10ä¸ªã€‚å¯¹äºè‹±è¯­å…³é”®è¯ï¼Œé™¤äº†å®Œæ•´ç¿»è¯‘ï¼Œè¿˜å¯ä»¥åŠ ä¸Šç›¸å…³ç¼©å†™ã€‚å¦‚æœè¯­å¥ä¸­åŒ…å«"æœ€è¿‘"ï¼Œ"æœ€æ–°"ç­‰è¯è¯­ï¼Œæ ¹æ®éœ€è¦åŠ ä¸Šå¹´ä»½æˆ–è€…æœˆä»½ï¼Œå¹´ä»½å’Œæœˆä»½ä¸èƒ½è¿åœ¨ä¸€èµ·ã€‚"""
            
            if context_info:
                prompt = f"""{prompt_base}

**å¯¹è¯ä¸Šä¸‹æ–‡**ï¼ˆé‡è¦ï¼è¯·ç»“åˆä¸Šä¸‹æ–‡æå–å…³é”®è¯ï¼‰ï¼š
{context_info}

**æ³¨æ„**ï¼šå¦‚æœå½“å‰é—®é¢˜æ˜¯ç®€çŸ­çš„ä¸å®Œæ•´è¡¨è¿°ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ä¸­çš„æ ¸å¿ƒæ¦‚å¿µæå–å…³é”®è¯ã€‚

ç°åœ¨ä»ä¸‹é¢è¿™å¥è¯ä¸­æå–ç”¨äºæœç´¢å¼•æ“çš„å…³é”®è¯ï¼š{query}"""
            else:
                prompt = f"""{prompt_base}ä»ä¸‹é¢è¿™å¥è¯ä¸­æå–ç”¨äºæœç´¢å¼•æ“çš„å…³é”®è¯ï¼š{query}"""
        
        headers = {
            "Authorization": f"Bearer {zhipu_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": zhipu_model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 1,
            "max_tokens": 65536,
            "stream": False,
            "thinking": {"type": "disabled"},
            "do_sample": True,
            "top_p": 0.95,
            "tool_stream": False,
            "response_format": {"type": "json_object"}
        }
        
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        max_retries = 3
        retry_delay = 2  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    ZHIPU_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=30  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°30ç§’ï¼Œä¸call_zhipu_llmä¸€è‡´
                )
                
                response.raise_for_status()
                result = response.json()
                
                # è§£æè¿”å›çš„JSON
                choices = result.get("choices", [])
                if not choices:
                    logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºchoices")
                    return None
                
                message = choices[0].get("message", {})
                content = message.get("content", "").strip()
                
                if not content:
                    logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºå†…å®¹")
                    return None
                
                # è§£æJSONå­—ç¬¦ä¸²ï¼ˆcontentä¸­åŒ…å«JSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼‰
                import json
                try:
                    keywords_dict = json.loads(content)
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"åŸå§‹å†…å®¹: {content[:200]}")  # è®°å½•å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
                    return None
                
                zh_keys = keywords_dict.get("zh_keys", "").strip()
                en_keys = keywords_dict.get("en_keys", "").strip()
                
                if zh_keys or en_keys:
                    logger.info(f"âœ… å…³é”®è¯æå–æˆåŠŸ: zh_keys={zh_keys}, en_keys={en_keys}")
                    return {
                        "zh_keys": zh_keys,
                        "en_keys": en_keys
                    }
                else:
                    logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºå…³é”®è¯")
                    return None
                    
            except (requests.exceptions.ReadTimeout, requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                    logger.warning(f"âš ï¸ å…³é”®è¯æå–è¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {e}")
                    raise
            except Exception as e:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
                raise
            
    except Exception as e:
        logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def call_zhipu_llm(
    prompt: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    response_format: Optional[dict] = None
) -> Optional[str]:
    """
    é€šç”¨çš„æ™ºè°±æ¸…è¨€ LLM è°ƒç”¨å‡½æ•°
    
    Args:
        prompt: æç¤ºè¯
        api_key: æ™ºè°±æ¸…è¨€APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        model: æ™ºè°±æ¸…è¨€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        response_format: å“åº”æ ¼å¼ï¼ˆå¦‚ {"type": "json_object"}ï¼‰
    
    Returns:
        LLMè¿”å›çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        zhipu_api_key = api_key if api_key is not None else ZHIPU_API_KEY
        zhipu_model = model if model is not None else ZHIPU_MODEL
        
        headers = {
            "Authorization": f"Bearer {zhipu_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": zhipu_model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False,
            "thinking": {"type": "disabled"},
            "do_sample": True,
            "top_p": 0.95,
            "tool_stream": False
        }
        
        # å¦‚æœæŒ‡å®šäº†å“åº”æ ¼å¼ï¼Œæ·»åŠ åˆ°payload
        if response_format:
            payload["response_format"] = response_format
        
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        max_retries = 3
        retry_delay = 2  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    ZHIPU_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=30
                )
                
                response.raise_for_status()
                result = response.json()
                
                # è§£æè¿”å›çš„JSON
                choices = result.get("choices", [])
                if not choices:
                    logger.warning("âš ï¸ æ™ºè°±æ¸…è¨€APIè¿”å›ç©ºchoices")
                    return None
                
                message = choices[0].get("message", {})
                content = message.get("content", "").strip()
                
                if not content:
                    logger.warning("âš ï¸ æ™ºè°±æ¸…è¨€APIè¿”å›ç©ºå†…å®¹")
                    return None
                
                return content
                
            except (requests.exceptions.ReadTimeout, requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                    logger.warning(f"âš ï¸ æ™ºè°±æ¸…è¨€APIè¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ æ™ºè°±æ¸…è¨€è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {e}")
                    raise
            except Exception as e:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                logger.error(f"âŒ æ™ºè°±æ¸…è¨€è°ƒç”¨å¤±è´¥: {e}")
                raise
            
    except Exception as e:
        logger.error(f"âŒ æ™ºè°±æ¸…è¨€è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def translate_text(query: str, source: str = "zh", target: str = "en") -> Optional[str]:
    """
    è°ƒç”¨ç¿»è¯‘APIç¿»è¯‘æ–‡æœ¬
    
    Args:
        query: è¦ç¿»è¯‘çš„æ–‡æœ¬
        source: æºè¯­è¨€ (zh/en)
        target: ç›®æ ‡è¯­è¨€ (zh/en)
    
    Returns:
        ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": TRANSLATE_API_KEY
        }
        
        data = {
            "q": query,
            "source": source,
            "target": target
        }
        
        response = requests.post(
            TRANSLATE_API_URL,
            headers=headers,
            json=data,
            timeout=10
        )
        
        response.raise_for_status()
        result = response.json()
        
        translated_text = result.get("translatedText", "")
        if translated_text:
            logger.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {query} -> {translated_text}")
            return translated_text
        else:
            logger.warning(f"âš ï¸ ç¿»è¯‘APIè¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        return None


def convert_to_markdown(text: str) -> str:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    lines = text.split('\n')
    result = []
    
    for line in lines:
        line = line.strip()
        
        if not line:
            result.append('\n')
            continue
        
        # å¤„ç†å¼•ç”¨æ ‡è®° [citation:X]
        if '[citation:' in line:
            line = re.sub(r'\[citation:(\d+)\]', r'[\1]', line)
        
        # å¤„ç†æ ‡é¢˜
        if line.startswith('#'):
            header_text = line.strip('#').strip()
            # å¤„ç†åŠ ç²—æ–‡æœ¬
            if '**' in header_text:
                header_text = re.sub(r'\*\*(.*?)\*\*', r'**\1**', header_text)
            result.append(f"{line}\n")
        
        # å¤„ç†åˆ—è¡¨é¡¹
        elif line.strip().startswith('- '):
            bullet_text = line.strip()[2:]
            result.append(f"- {bullet_text}\n")
        
        # å¤„ç†åˆ†éš”çº¿
        elif line.strip() == '---':
            result.append("---\n")
        
        # å¤„ç†æ™®é€šæ–‡æœ¬
        else:
            result.append(f"{line}\n")
    
    return ''.join(result)


def search_searxng(
    query: str,
    num_results: int,
    ip_address: str = "http://localhost:9080",
    language: str = "zh",
    time_range: str = "",
    deduplicate_by_url: bool = True
) -> List[SearchDocument]:
    """
    ä½¿ç”¨SearXNGæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: éœ€è¦çš„ç»“æœæ•°é‡
        ip_address: SearXNGæœåŠ¡åœ°å€
        language: æœç´¢è¯­è¨€ (zh/en)
        time_range: æ—¶é—´èŒƒå›´ (day/week/month/year/"")
    
    Returns:
        æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5"
    }
    
    # æ„å»ºè¯·æ±‚URL
    params = {
        "q": query,
        "format": "json",
        "language": language,
    }
    
    # æ·»åŠ æ—¶é—´èŒƒå›´å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
    if time_range:
        params["time_range"] = time_range
    
    # æ„å»ºåŸºç¡€URL
    base_url = ip_address.rstrip('/')
    if not base_url.startswith('http'):
        base_url = f"http://{base_url}"
    
    res = []
    seen_urls = set() if deduplicate_by_url else None
    pageno = 1
    
    while len(res) < num_results:
        params["pageno"] = pageno
        query_string = urllib.parse.urlencode(params)
        url = f"{base_url}/search?{query_string}"
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            response_dict = response.json()
            
        except JSONDecodeError as e:
            logger.error(f"âŒ SearXNG JSONè§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
            raise ValueError("JSONDecodeError: è¯·ç¡®ä¿SearXNGå®ä¾‹å¯ä»¥è¿”å›JSONæ ¼å¼æ•°æ®")
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ SearXNGè¯·æ±‚å¤±è´¥: {e}")
            raise
        
        result_dicts = response_dict.get("results", [])
        if not result_dicts:
            logger.debug(f"ç¬¬{pageno}é¡µæ— æ›´å¤šç»“æœ")
            break
        
        for result in result_dicts:
            # æå–å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
            content = result.get("content", "") or result.get("snippet", "")
            result_url = result.get("url", "")
            
            # å»é‡ï¼šå¦‚æœå¯ç”¨äº†å»é‡ä¸”URLå·²å­˜åœ¨ï¼Œè·³è¿‡
            if deduplicate_by_url and seen_urls is not None:
                if result_url in seen_urls:
                    continue
                seen_urls.add(result_url)
            
            if content:
                doc = SearchDocument(
                    title=result.get("title", ""),
                    url=result_url,
                    snippet=result.get("snippet", ""),
                    content=content,
                    score=result.get("score", 0.0)
                )
                res.append(doc)
                
                if len(res) >= num_results:
                    break
        
        # å¦‚æœæ²¡æœ‰æ›´å¤šç»“æœï¼Œåœæ­¢åˆ†é¡µ
        if len(result_dicts) < 20:  # é€šå¸¸æ¯é¡µ20ä¸ªç»“æœ
            break
        
        pageno += 1
    
    logger.info(f"âœ… SearXNGæœç´¢å®Œæˆ: è·å¾—{len(res)}ä¸ªç»“æœ")
    return res


async def search_duckduckgo(
    query: str,
    max_results: int = 20,
    language: str = "zh",
    time_range: Optional[str] = None
) -> List[SearchDocument]:
    """
    ä½¿ç”¨ DuckDuckGo API ç›´æ¥æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§ç»“æœæ•°é‡
        language: æœç´¢è¯­è¨€ (zh/en)
        time_range: æ—¶é—´èŒƒå›´ ('d'=å¤©, 'w'=å‘¨, 'm'=æœˆ, 'y'=å¹´, None=ä¸é™)
    
    Returns:
        æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
    """
    try:
        # å°è¯•å¯¼å…¥ ddgs
        try:
            from ddgs import DDGS
        except ImportError:
            from duckduckgo_search import DDGS
        
        # å‡†å¤‡æœç´¢å‚æ•°
        search_params = {
            "query": query,
            "max_results": max_results,
            "safesearch": "moderate"
        }
        
        # æ ¹æ®è¯­è¨€è®¾ç½®åœ°åŒºå‚æ•°
        if language == "zh":
            search_params["region"] = "cn-zh"
        else:
            search_params["region"] = "us-en"
        
        # è®¾ç½®æ—¶é—´èŒƒå›´
        if time_range:
            # å°† searxng çš„æ—¶é—´èŒƒå›´æ ¼å¼è½¬æ¢ä¸º duckduckgo æ ¼å¼
            time_map = {
                "day": "d",
                "week": "w",
                "month": "m",
                "year": "y"
            }
            ddg_time = time_map.get(time_range.lower())
            if ddg_time:
                search_params["timelimit"] = ddg_time
        
        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œæœç´¢ï¼ˆé¿å…é˜»å¡ï¼‰
        def _run_search():
            with DDGS() as ddgs:
                return list(ddgs.text(**search_params))
        
        results = await asyncio.to_thread(_run_search)
        
        # è½¬æ¢ä¸º SearchDocument æ ¼å¼
        documents = []
        for result in results:
            doc = SearchDocument(
                title=result.get("title", ""),
                url=result.get("href", ""),
                snippet=result.get("body", ""),
                content=result.get("body", ""),  # DuckDuckGo è¿”å›çš„æ˜¯ body
                score=0.0  # DuckDuckGo ä¸æä¾›åˆ†æ•°
            )
            if doc.url and (doc.title or doc.snippet):
                documents.append(doc)
        
        logger.info(f"âœ… DuckDuckGoæœç´¢å®Œæˆ: æŸ¥è¯¢='{query}', è¯­è¨€={language}, è·å¾—{len(documents)}ä¸ªç»“æœ")
        return documents
        
    except ImportError:
        logger.warning("âš ï¸ DuckDuckGoæœç´¢åŒ…æœªå®‰è£…ï¼Œè·³è¿‡DuckDuckGoæœç´¢ã€‚è¯·è¿è¡Œ: pip install ddgs")
        return []
    except Exception as e:
        logger.error(f"âŒ DuckDuckGoæœç´¢å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []


class FaissRetriever:
    """FAISSå‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, embedding_model, num_candidates: int = 40, sim_threshold: float = 0.45) -> None:
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹ï¼ˆSentenceTransformerå®ä¾‹ï¼‰
            num_candidates: å€™é€‰æ–‡æ¡£æ•°é‡
            sim_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.embedding_model = embedding_model
        self.num_candidates = num_candidates
        self.sim_threshold = sim_threshold
        self.embeddings_dim = embedding_model.get_sentence_embedding_dimension()
        self.reset_state()
        logger.info(f"ğŸ“¦ FAISSæ£€ç´¢å™¨åˆå§‹åŒ–: dim={self.embeddings_dim}, candidates={num_candidates}, threshold={sim_threshold}")
    
    def reset_state(self) -> None:
        """é‡ç½®çŠ¶æ€"""
        self.index = faiss.IndexFlatIP(self.embeddings_dim)  # ä½¿ç”¨å†…ç§¯ï¼ˆcosineç›¸ä¼¼åº¦ï¼‰
        self.documents = []
    
    def encode_doc(self, doc: str | List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æ¡£ä¸ºå‘é‡"""
        return self.embedding_model.encode(doc, normalize_embeddings=True)
    
    def add_documents(self, documents: List[SearchDocument]) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ æ²¡æœ‰æ–‡æ¡£æ·»åŠ åˆ°æ£€ç´¢å™¨")
            return
        
        self.reset_state()
        self.documents = documents
        
        # æå–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
        doc_texts = [doc.content if doc.content else doc.snippet for doc in documents]
        
        # ç¼–ç æ–‡æ¡£
        doc_embeddings = self.encode_doc(doc_texts)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(doc_embeddings)
        logger.debug(f"ğŸ“š æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£åˆ°FAISSç´¢å¼•")
    
    def filter_by_sim(self, distances: np.ndarray, indices: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ
        
        Args:
            distances: ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„
            indices: ç´¢å¼•æ•°ç»„
        
        Returns:
            è¿‡æ»¤åçš„ç´¢å¼•æ•°ç»„
        """
        cutoff_idx = -1
        for idx, sim in enumerate(distances):
            if sim >= self.sim_threshold:
                cutoff_idx = idx
            else:
                break
        
        if cutoff_idx == -1:
            return np.array([])
        
        return indices[:cutoff_idx + 1]
    
    def get_relevant_documents(self, query: str) -> List[SearchDocument]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.documents:
            logger.warning("âš ï¸ æ£€ç´¢å™¨ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode_doc(query)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1),
            min(self.num_candidates, len(self.documents))
        )
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°åˆ°æ–‡æ¡£
        for idx, sim in enumerate(distances[0]):
            doc_idx = indices[0][idx]
            if doc_idx < len(self.documents):
                self.documents[doc_idx].score = float(sim)
        
        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        top_indices = self.filter_by_sim(distances[0], indices[0])
        
        if len(top_indices) == 0:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>{self.sim_threshold}ï¼‰")
            return []
        
        relevant_docs = [self.documents[int(idx)] for idx in top_indices]
        
        logger.info(f"ğŸ¯ æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
        
        # è®°å½•å‰å‡ ä¸ªç»“æœ
        for idx, doc in enumerate(relevant_docs[:5]):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (sim: {doc.score:.3f})")
        
        return relevant_docs

    def get_relevant_documents_multi_query(self, queries: List[str]) -> List[SearchDocument]:
        """
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢ï¼Œç„¶ååˆå¹¶ç»“æœï¼ˆä¿ç•™æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        
        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š["ä¸­æ–‡æŸ¥è¯¢", "English keywords"]ï¼‰
        
        Returns:
            åˆå¹¶åçš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰
        """
        if not self.documents:
            logger.warning("âš ï¸ æ£€ç´¢å™¨ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            return []
        
        if not queries:
            return []
        
        # å­˜å‚¨æ–‡æ¡£URLåˆ°æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°çš„æ˜ å°„
        doc_scores = {}  # {url: max_score}
        doc_map = {}  # {url: SearchDocument}
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢
        for query_idx, query in enumerate(queries):
            if not query or not query.strip():
                continue
                
            logger.debug(f"[å¤šæŸ¥è¯¢æ£€ç´¢] æŸ¥è¯¢ {query_idx + 1}/{len(queries)}: {query[:50]}...")
            
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.encode_doc(query)
            
            # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
            distances, indices = self.index.search(
                query_embedding.reshape(1, -1),
                min(self.num_candidates, len(self.documents))
            )
            
            # å¤„ç†æ¯ä¸ªç»“æœï¼Œä¿ç•™æœ€é«˜åˆ†æ•°
            for idx, sim in enumerate(distances[0]):
                doc_idx = int(indices[0][idx])
                if doc_idx >= len(self.documents):
                    continue
                
                doc = self.documents[doc_idx]
                sim_score = float(sim)
                
                # åªè€ƒè™‘è¶…è¿‡é˜ˆå€¼çš„æ–‡æ¡£
                if sim_score >= self.sim_threshold:
                    # ä¿ç•™æ›´é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°
                    if doc.url not in doc_scores or sim_score > doc_scores[doc.url]:
                        doc_scores[doc.url] = sim_score
                        # åˆ›å»ºæ–‡æ¡£å‰¯æœ¬å¹¶æ›´æ–°åˆ†æ•°
                        doc_copy = SearchDocument(
                            title=doc.title,
                            url=doc.url,
                            snippet=doc.snippet,
                            content=doc.content,
                            score=sim_score
                        )
                        doc_map[doc.url] = doc_copy
        
        if not doc_map:
            logger.warning(f"âš ï¸ å¤šæŸ¥è¯¢æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        relevant_docs = list(doc_map.values())
        relevant_docs.sort(key=lambda x: x.score, reverse=True)
        
        logger.info(f"ğŸ¯ å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
        
        # è®°å½•å‰å‡ ä¸ªç»“æœ
        for idx, doc in enumerate(relevant_docs[:5]):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (sim: {doc.score:.3f})")
        
        return relevant_docs