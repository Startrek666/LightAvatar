"""
Momo Search Utils - æœç´¢å·¥å…·å‡½æ•°
"""
from dataclasses import dataclass
import urllib.parse
from json import JSONDecodeError
from typing import List, Optional
import re
import asyncio

import faiss
import numpy as np
import requests
from loguru import logger

# ç¿»è¯‘APIé…ç½®
TRANSLATE_API_URL = "https://api-utils.lemomate.com/translate"
TRANSLATE_API_KEY = "L5kGzmjwqXbk0ViD@"

# æ™ºè°±æ¸…è¨€å…³é”®è¯æå–APIé…ç½®
ZHIPU_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
ZHIPU_API_KEY = "6f29a799833a4a5daf5752973e9d0cc4.uoelH21xYFMkDknh"
ZHIPU_MODEL = "glm-4.5-flash"


@dataclass
class SearchDocument:
    """æœç´¢ç»“æœæ–‡æ¡£"""
    title: str = ""
    url: str = ""
    snippet: str = ""
    content: str = ""
    score: float = 0.0


def encode_url(url: str) -> str:
    """URLç¼–ç """
    return urllib.parse.quote(url)


def decode_url(url: str) -> str:
    """URLè§£ç """
    return urllib.parse.unquote(url)


def escape_markdown(text: str) -> str:
    """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""
    special_chars = r'_\*\[\]\(\)~`>#\+\-=\|\{\}\.\!'
    return re.sub(f'([{special_chars}])', r'\\\1', text)


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
    
    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡
    """
    if not text:
        return "en"
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s+', '', text))  # å»é™¤ç©ºæ ¼åçš„æ€»å­—ç¬¦æ•°
    
    if total_chars == 0:
        return "en"
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    if chinese_ratio > 0.3:
        return "zh"
    else:
        return "en"


def extract_keywords(
    query: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None
) -> Optional[dict]:
    """
    ä½¿ç”¨æ™ºè°±æ¸…è¨€æ¨¡å‹æå–æœç´¢å…³é”®è¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        api_key: æ™ºè°±æ¸…è¨€APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        model: æ™ºè°±æ¸…è¨€æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    Returns:
        åŒ…å«zh_keyså’Œen_keysçš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        from datetime import datetime
        
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
        zhipu_api_key = api_key if api_key is not None else ZHIPU_API_KEY
        zhipu_model = model if model is not None else ZHIPU_MODEL
        
        # è·å–å½“å‰æ—¥æœŸï¼Œå»æ‰æœˆä»½å’Œæ—¥æœŸçš„å‰å¯¼é›¶
        now = datetime.now()
        current_date = f"{now.year}å¹´{now.month}æœˆ{now.day}æ—¥"
        
        # æ„å»ºPrompt
        prompt = f"""ä»Šå¤©æ˜¯{current_date}ã€‚ä¸ºäº†ç»™ç”¨æˆ·çš„å›ç­”ä¿æŒå‡†ç¡®ï¼Œä½ éœ€è¦ä½¿ç”¨æœç´¢å¼•æ“ã€‚ä½¿ç”¨jsonæ ¼å¼è¿”å›å…³é”®è¯ï¼Œå±æ€§ä¸ºzh_keys,en_keysã€‚æ¯ä¸ªå±æ€§åªéœ€è¦ä¸€è¡Œï¼Œå…³é”®è¯ç”¨ç©ºæ ¼åˆ†éš”ã€‚ä»…éœ€è¿”å›é‡è¦å…³é”®è¯ï¼Œæ¯è¡Œä¸è¶…è¿‡10ä¸ªã€‚å¯¹äºè‹±è¯­å…³é”®è¯ï¼Œé™¤äº†å®Œæ•´ç¿»è¯‘ï¼Œè¿˜å¯ä»¥åŠ ä¸Šç›¸å…³ç¼©å†™ã€‚å¦‚æœè¯­å¥ä¸­åŒ…å«"æœ€è¿‘"ï¼Œ"æœ€æ–°"ç­‰è¯è¯­ï¼Œæ ¹æ®éœ€è¦åŠ ä¸Šå¹´ä»½æˆ–è€…æœˆä»½ï¼Œå¹´ä»½å’Œæœˆä»½ä¸èƒ½è¿åœ¨ä¸€èµ·ã€‚ä»ä¸‹é¢è¿™å¥è¯ä¸­æå–ç”¨äºæœç´¢å¼•æ“çš„å…³é”®è¯ï¼š{query}"""
        
        headers = {
            "Authorization": f"Bearer {zhipu_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": zhipu_model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 1,
            "max_tokens": 65536,
            "stream": False,
            "thinking": {"type": "disabled"},
            "do_sample": True,
            "top_p": 0.95,
            "tool_stream": False,
            "response_format": {"type": "json_object"}
        }
        
        response = requests.post(
            ZHIPU_API_URL,
            json=payload,
            headers=headers,
            timeout=15
        )
        
        response.raise_for_status()
        result = response.json()
        
        # è§£æè¿”å›çš„JSON
        choices = result.get("choices", [])
        if not choices:
            logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºchoices")
            return None
        
        message = choices[0].get("message", {})
        content = message.get("content", "").strip()
        
        if not content:
            logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºå†…å®¹")
            return None
        
        # è§£æJSONå­—ç¬¦ä¸²ï¼ˆcontentä¸­åŒ…å«JSONæ ¼å¼çš„å­—ç¬¦ä¸²ï¼‰
        import json
        try:
            keywords_dict = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹å†…å®¹: {content[:200]}")  # è®°å½•å‰200ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
            return None
        
        zh_keys = keywords_dict.get("zh_keys", "").strip()
        en_keys = keywords_dict.get("en_keys", "").strip()
        
        if zh_keys or en_keys:
            logger.info(f"âœ… å…³é”®è¯æå–æˆåŠŸ: zh_keys={zh_keys}, en_keys={en_keys}")
            return {
                "zh_keys": zh_keys,
                "en_keys": en_keys
            }
        else:
            logger.warning("âš ï¸ å…³é”®è¯æå–APIè¿”å›ç©ºå…³é”®è¯")
            return None
            
    except Exception as e:
        logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def translate_text(query: str, source: str = "zh", target: str = "en") -> Optional[str]:
    """
    è°ƒç”¨ç¿»è¯‘APIç¿»è¯‘æ–‡æœ¬
    
    Args:
        query: è¦ç¿»è¯‘çš„æ–‡æœ¬
        source: æºè¯­è¨€ (zh/en)
        target: ç›®æ ‡è¯­è¨€ (zh/en)
    
    Returns:
        ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": TRANSLATE_API_KEY
        }
        
        data = {
            "q": query,
            "source": source,
            "target": target
        }
        
        response = requests.post(
            TRANSLATE_API_URL,
            headers=headers,
            json=data,
            timeout=10
        )
        
        response.raise_for_status()
        result = response.json()
        
        translated_text = result.get("translatedText", "")
        if translated_text:
            logger.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {query} -> {translated_text}")
            return translated_text
        else:
            logger.warning(f"âš ï¸ ç¿»è¯‘APIè¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        return None


def convert_to_markdown(text: str) -> str:
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    lines = text.split('\n')
    result = []
    
    for line in lines:
        line = line.strip()
        
        if not line:
            result.append('\n')
            continue
        
        # å¤„ç†å¼•ç”¨æ ‡è®° [citation:X]
        if '[citation:' in line:
            line = re.sub(r'\[citation:(\d+)\]', r'[\1]', line)
        
        # å¤„ç†æ ‡é¢˜
        if line.startswith('#'):
            header_text = line.strip('#').strip()
            # å¤„ç†åŠ ç²—æ–‡æœ¬
            if '**' in header_text:
                header_text = re.sub(r'\*\*(.*?)\*\*', r'**\1**', header_text)
            result.append(f"{line}\n")
        
        # å¤„ç†åˆ—è¡¨é¡¹
        elif line.strip().startswith('- '):
            bullet_text = line.strip()[2:]
            result.append(f"- {bullet_text}\n")
        
        # å¤„ç†åˆ†éš”çº¿
        elif line.strip() == '---':
            result.append("---\n")
        
        # å¤„ç†æ™®é€šæ–‡æœ¬
        else:
            result.append(f"{line}\n")
    
    return ''.join(result)


def search_searxng(
    query: str,
    num_results: int,
    ip_address: str = "http://localhost:9080",
    language: str = "zh",
    time_range: str = "",
    deduplicate_by_url: bool = True
) -> List[SearchDocument]:
    """
    ä½¿ç”¨SearXNGæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: éœ€è¦çš„ç»“æœæ•°é‡
        ip_address: SearXNGæœåŠ¡åœ°å€
        language: æœç´¢è¯­è¨€ (zh/en)
        time_range: æ—¶é—´èŒƒå›´ (day/week/month/year/"")
    
    Returns:
        æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5"
    }
    
    # æ„å»ºè¯·æ±‚URL
    params = {
        "q": query,
        "format": "json",
        "language": language,
    }
    
    # æ·»åŠ æ—¶é—´èŒƒå›´å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
    if time_range:
        params["time_range"] = time_range
    
    # æ„å»ºåŸºç¡€URL
    base_url = ip_address.rstrip('/')
    if not base_url.startswith('http'):
        base_url = f"http://{base_url}"
    
    res = []
    seen_urls = set() if deduplicate_by_url else None
    pageno = 1
    
    while len(res) < num_results:
        params["pageno"] = pageno
        query_string = urllib.parse.urlencode(params)
        url = f"{base_url}/search?{query_string}"
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            response_dict = response.json()
            
        except JSONDecodeError as e:
            logger.error(f"âŒ SearXNG JSONè§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
            raise ValueError("JSONDecodeError: è¯·ç¡®ä¿SearXNGå®ä¾‹å¯ä»¥è¿”å›JSONæ ¼å¼æ•°æ®")
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ SearXNGè¯·æ±‚å¤±è´¥: {e}")
            raise
        
        result_dicts = response_dict.get("results", [])
        if not result_dicts:
            logger.debug(f"ç¬¬{pageno}é¡µæ— æ›´å¤šç»“æœ")
            break
        
        for result in result_dicts:
            # æå–å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
            content = result.get("content", "") or result.get("snippet", "")
            result_url = result.get("url", "")
            
            # å»é‡ï¼šå¦‚æœå¯ç”¨äº†å»é‡ä¸”URLå·²å­˜åœ¨ï¼Œè·³è¿‡
            if deduplicate_by_url and seen_urls is not None:
                if result_url in seen_urls:
                    continue
                seen_urls.add(result_url)
            
            if content:
                doc = SearchDocument(
                    title=result.get("title", ""),
                    url=result_url,
                    snippet=result.get("snippet", ""),
                    content=content,
                    score=result.get("score", 0.0)
                )
                res.append(doc)
                
                if len(res) >= num_results:
                    break
        
        # å¦‚æœæ²¡æœ‰æ›´å¤šç»“æœï¼Œåœæ­¢åˆ†é¡µ
        if len(result_dicts) < 20:  # é€šå¸¸æ¯é¡µ20ä¸ªç»“æœ
            break
        
        pageno += 1
    
    logger.info(f"âœ… SearXNGæœç´¢å®Œæˆ: è·å¾—{len(res)}ä¸ªç»“æœ")
    return res


async def search_duckduckgo(
    query: str,
    max_results: int = 20,
    language: str = "zh",
    time_range: Optional[str] = None
) -> List[SearchDocument]:
    """
    ä½¿ç”¨ DuckDuckGo API ç›´æ¥æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§ç»“æœæ•°é‡
        language: æœç´¢è¯­è¨€ (zh/en)
        time_range: æ—¶é—´èŒƒå›´ ('d'=å¤©, 'w'=å‘¨, 'm'=æœˆ, 'y'=å¹´, None=ä¸é™)
    
    Returns:
        æœç´¢ç»“æœæ–‡æ¡£åˆ—è¡¨
    """
    try:
        # å°è¯•å¯¼å…¥ ddgs
        try:
            from ddgs import DDGS
        except ImportError:
            from duckduckgo_search import DDGS
        
        # å‡†å¤‡æœç´¢å‚æ•°
        search_params = {
            "query": query,
            "max_results": max_results,
            "safesearch": "moderate"
        }
        
        # æ ¹æ®è¯­è¨€è®¾ç½®åœ°åŒºå‚æ•°
        if language == "zh":
            search_params["region"] = "cn-zh"
        else:
            search_params["region"] = "us-en"
        
        # è®¾ç½®æ—¶é—´èŒƒå›´
        if time_range:
            # å°† searxng çš„æ—¶é—´èŒƒå›´æ ¼å¼è½¬æ¢ä¸º duckduckgo æ ¼å¼
            time_map = {
                "day": "d",
                "week": "w",
                "month": "m",
                "year": "y"
            }
            ddg_time = time_map.get(time_range.lower())
            if ddg_time:
                search_params["timelimit"] = ddg_time
        
        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œæœç´¢ï¼ˆé¿å…é˜»å¡ï¼‰
        def _run_search():
            with DDGS() as ddgs:
                return list(ddgs.text(**search_params))
        
        results = await asyncio.to_thread(_run_search)
        
        # è½¬æ¢ä¸º SearchDocument æ ¼å¼
        documents = []
        for result in results:
            doc = SearchDocument(
                title=result.get("title", ""),
                url=result.get("href", ""),
                snippet=result.get("body", ""),
                content=result.get("body", ""),  # DuckDuckGo è¿”å›çš„æ˜¯ body
                score=0.0  # DuckDuckGo ä¸æä¾›åˆ†æ•°
            )
            if doc.url and (doc.title or doc.snippet):
                documents.append(doc)
        
        logger.info(f"âœ… DuckDuckGoæœç´¢å®Œæˆ: æŸ¥è¯¢='{query}', è¯­è¨€={language}, è·å¾—{len(documents)}ä¸ªç»“æœ")
        return documents
        
    except ImportError:
        logger.warning("âš ï¸ DuckDuckGoæœç´¢åŒ…æœªå®‰è£…ï¼Œè·³è¿‡DuckDuckGoæœç´¢ã€‚è¯·è¿è¡Œ: pip install ddgs")
        return []
    except Exception as e:
        logger.error(f"âŒ DuckDuckGoæœç´¢å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []


class FaissRetriever:
    """FAISSå‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, embedding_model, num_candidates: int = 40, sim_threshold: float = 0.45) -> None:
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹ï¼ˆSentenceTransformerå®ä¾‹ï¼‰
            num_candidates: å€™é€‰æ–‡æ¡£æ•°é‡
            sim_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.embedding_model = embedding_model
        self.num_candidates = num_candidates
        self.sim_threshold = sim_threshold
        self.embeddings_dim = embedding_model.get_sentence_embedding_dimension()
        self.reset_state()
        logger.info(f"ğŸ“¦ FAISSæ£€ç´¢å™¨åˆå§‹åŒ–: dim={self.embeddings_dim}, candidates={num_candidates}, threshold={sim_threshold}")
    
    def reset_state(self) -> None:
        """é‡ç½®çŠ¶æ€"""
        self.index = faiss.IndexFlatIP(self.embeddings_dim)  # ä½¿ç”¨å†…ç§¯ï¼ˆcosineç›¸ä¼¼åº¦ï¼‰
        self.documents = []
    
    def encode_doc(self, doc: str | List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æ¡£ä¸ºå‘é‡"""
        return self.embedding_model.encode(doc, normalize_embeddings=True)
    
    def add_documents(self, documents: List[SearchDocument]) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ æ²¡æœ‰æ–‡æ¡£æ·»åŠ åˆ°æ£€ç´¢å™¨")
            return
        
        self.reset_state()
        self.documents = documents
        
        # æå–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨contentï¼Œå¦åˆ™ä½¿ç”¨snippetï¼‰
        doc_texts = [doc.content if doc.content else doc.snippet for doc in documents]
        
        # ç¼–ç æ–‡æ¡£
        doc_embeddings = self.encode_doc(doc_texts)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(doc_embeddings)
        logger.debug(f"ğŸ“š æ·»åŠ {len(documents)}ä¸ªæ–‡æ¡£åˆ°FAISSç´¢å¼•")
    
    def filter_by_sim(self, distances: np.ndarray, indices: np.ndarray) -> np.ndarray:
        """
        æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ
        
        Args:
            distances: ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„
            indices: ç´¢å¼•æ•°ç»„
        
        Returns:
            è¿‡æ»¤åçš„ç´¢å¼•æ•°ç»„
        """
        cutoff_idx = -1
        for idx, sim in enumerate(distances):
            if sim >= self.sim_threshold:
                cutoff_idx = idx
            else:
                break
        
        if cutoff_idx == -1:
            return np.array([])
        
        return indices[:cutoff_idx + 1]
    
    def get_relevant_documents(self, query: str) -> List[SearchDocument]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.documents:
            logger.warning("âš ï¸ æ£€ç´¢å™¨ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encode_doc(query)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1),
            min(self.num_candidates, len(self.documents))
        )
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°åˆ°æ–‡æ¡£
        for idx, sim in enumerate(distances[0]):
            doc_idx = indices[0][idx]
            if doc_idx < len(self.documents):
                self.documents[doc_idx].score = float(sim)
        
        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        top_indices = self.filter_by_sim(distances[0], indices[0])
        
        if len(top_indices) == 0:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>{self.sim_threshold}ï¼‰")
            return []
        
        relevant_docs = [self.documents[int(idx)] for idx in top_indices]
        
        logger.info(f"ğŸ¯ æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
        
        # è®°å½•å‰å‡ ä¸ªç»“æœ
        for idx, doc in enumerate(relevant_docs[:5]):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (sim: {doc.score:.3f})")
        
        return relevant_docs
    
    def get_relevant_documents_multi_query(self, queries: List[str]) -> List[SearchDocument]:
        """
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢ï¼Œç„¶ååˆå¹¶ç»“æœï¼ˆä¿ç•™æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        
        Args:
            queries: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼š["ä¸­æ–‡æŸ¥è¯¢", "English keywords"]ï¼‰
        
        Returns:
            åˆå¹¶åçš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰
        """
        if not self.documents:
            logger.warning("âš ï¸ æ£€ç´¢å™¨ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£")
            return []
        
        if not queries:
            return []
        
        # å­˜å‚¨æ–‡æ¡£URLåˆ°æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°çš„æ˜ å°„
        doc_scores = {}  # {url: max_score}
        doc_map = {}  # {url: SearchDocument}
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢
        for query_idx, query in enumerate(queries):
            if not query or not query.strip():
                continue
                
            logger.debug(f"[å¤šæŸ¥è¯¢æ£€ç´¢] æŸ¥è¯¢ {query_idx + 1}/{len(queries)}: {query[:50]}...")
            
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.encode_doc(query)
            
            # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
            distances, indices = self.index.search(
                query_embedding.reshape(1, -1),
                min(self.num_candidates, len(self.documents))
            )
            
            # å¤„ç†æ¯ä¸ªç»“æœï¼Œä¿ç•™æœ€é«˜åˆ†æ•°
            for idx, sim in enumerate(distances[0]):
                doc_idx = int(indices[0][idx])
                if doc_idx >= len(self.documents):
                    continue
                
                doc = self.documents[doc_idx]
                sim_score = float(sim)
                
                # åªè€ƒè™‘è¶…è¿‡é˜ˆå€¼çš„æ–‡æ¡£
                if sim_score >= self.sim_threshold:
                    # ä¿ç•™æ›´é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°
                    if doc.url not in doc_scores or sim_score > doc_scores[doc.url]:
                        doc_scores[doc.url] = sim_score
                        # åˆ›å»ºæ–‡æ¡£å‰¯æœ¬å¹¶æ›´æ–°åˆ†æ•°
                        doc_copy = SearchDocument(
                            title=doc.title,
                            url=doc.url,
                            snippet=doc.snippet,
                            content=doc.content,
                            score=sim_score
                        )
                        doc_map[doc.url] = doc_copy
        
        if not doc_map:
            logger.warning(f"âš ï¸ å¤šæŸ¥è¯¢æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
            return []
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        relevant_docs = list(doc_map.values())
        relevant_docs.sort(key=lambda x: x.score, reverse=True)
        
        logger.info(f"ğŸ¯ å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, æ‰¾åˆ°{len(relevant_docs)}ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆé˜ˆå€¼>={self.sim_threshold}ï¼‰")
        
        # è®°å½•å‰å‡ ä¸ªç»“æœ
        for idx, doc in enumerate(relevant_docs[:5]):
            logger.debug(f"  {idx+1}. {doc.title[:50]}... (sim: {doc.score:.3f})")
        
        return relevant_docs